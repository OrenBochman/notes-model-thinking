[
  {
    "objectID": "w11.html",
    "href": "w11.html",
    "title": "Lesson 11 - Lyapunov Functions",
    "section": "",
    "text": "Lyapunov functions map models into outcomes. We can take a model or a system and ask whether there is a Lyapunov function that describes that model or system. If that is possible, then that system goes to equilibrium. If we can’t construct a Lyapunov function, then the type of outcome can be of any class. e.g., in physics you might have a velocity function over time, and if the velocity changes then it goes down with at least some minimum amount, and there is a minimum velocity, e.g. zero. Those conditions mean that the system has to stop at some point.\nThis can be formalised as follows. There is a Lyapunov function \\(F(x)\\), and the system has to stop and go to equilibrium at some point meaning that \\(x_{t+1} = x_t\\), if the following conditions hold:\n1. it has a maximum value (or a minimum value if it is going down);\n2. there is a k &gt; 0 such that, if \\(x_{t+1} \\ne x_t\\) then \\(F(x_t+1) &gt; F(x_t) + k\\) (or \\(F(x_{t+1}) &lt; F(x_t) - k\\) if it is going down).\nThe change must be of some minimum amount k because of Zeno’s Arrow paradox that states if you go halfway the remaining route every day, you will never arrive at your destination as you travel smaller distances every day. If you know the minimum amount k then you know the maximum number of steps. e.g., if k is a quarter of the total distance then the maximum number of steps is 4. The tricky part is construction the Lyapunov function F(x). Sometimes it is easy, sometimes it is hard, and sometimes it is impossible to do."
  },
  {
    "objectID": "w11.html#sec-lyapunov-functions",
    "href": "w11.html#sec-lyapunov-functions",
    "title": "Lesson 11 - Lyapunov Functions",
    "section": "",
    "text": "Lyapunov functions map models into outcomes. We can take a model or a system and ask whether there is a Lyapunov function that describes that model or system. If that is possible, then that system goes to equilibrium. If we can’t construct a Lyapunov function, then the type of outcome can be of any class. e.g., in physics you might have a velocity function over time, and if the velocity changes then it goes down with at least some minimum amount, and there is a minimum velocity, e.g. zero. Those conditions mean that the system has to stop at some point.\nThis can be formalised as follows. There is a Lyapunov function \\(F(x)\\), and the system has to stop and go to equilibrium at some point meaning that \\(x_{t+1} = x_t\\), if the following conditions hold:\n1. it has a maximum value (or a minimum value if it is going down);\n2. there is a k &gt; 0 such that, if \\(x_{t+1} \\ne x_t\\) then \\(F(x_t+1) &gt; F(x_t) + k\\) (or \\(F(x_{t+1}) &lt; F(x_t) - k\\) if it is going down).\nThe change must be of some minimum amount k because of Zeno’s Arrow paradox that states if you go halfway the remaining route every day, you will never arrive at your destination as you travel smaller distances every day. If you know the minimum amount k then you know the maximum number of steps. e.g., if k is a quarter of the total distance then the maximum number of steps is 4. The tricky part is construction the Lyapunov function F(x). Sometimes it is easy, sometimes it is hard, and sometimes it is impossible to do."
  },
  {
    "objectID": "w11.html#sec-the-organization-of-cities",
    "href": "w11.html#sec-the-organization-of-cities",
    "title": "Lesson 11 - Lyapunov Functions",
    "section": "The organization of cities",
    "text": "The organization of cities\nIn any major city there is an amazing order. Restaurants have the right number of people in them, so do coffee shops. There are not huge lines behind dry cleaners. The interesting thing is that there is no central planner and the city self-organises in some way so that the right number of people are at the right places and there are no vacancies or crowds at particular points. What makes the city organise in this way?\nSuppose that there are five locations everyone has to go to each week, which are the cleaners C, the grocery G, the deli D, the book store B and the fish market F, and there are five days to do them, which are Monday Mo, Tuesday Tu, Wednesday We, Thursday Th and Friday Fr. Assume that individuals choose their order of visiting those places randomly. A route that someone might take during the week is: (Mo, Tu, We, Th, Fr) =&gt; (C, G, B, D, F). Suppose there are five people that choose some random order to visit these locations. Assume that the behaviour of these people is to switch two locations as to avoid crowds.\nIf people follow these rules, then we can use the Lyapunov function on the process, and show that it goes to equilibrium. Assume those five people picked the following routes:\n\n(C, G, D, B, F),\n(G, C, D, B, F),\n(C, D, G, F, B),\n(C, B, F, G, D),\n(C, F, D, B, G).\n\nEach week one person switches locations to avoid other people, and takes the most efficient step, meaning that he or she makes the switch that avoids the most people. After one week person 1 might then switch C and F, meaning that the next week he is going to the fish market on Monday and to the cleaners on Friday to avoid crowds at the cleaners.\nFinding a Lyapunov function might be a form of trial and error. e.g., you may try the total number of people at each location per week. That doesn’t work out because the total number of people will always be 5. Another option is the number of people that the five people meet each week. In the first week person 1 meets 3 + 0 + 2 + 2 + 1 = 8 people. If this person switches the cleaners and the fish market, he will meet 4 people next week because the others don’t switch. Consequently, those others don’t meet person 1 at the cleaners too, so the total number of meetings in that week drops by 8.\nThis is a Lyapunov function because there is a minimum of 0 meetings during a week and people keep switching until there are no options to reduce the number of meetings. When that happens, the system enters into equilibrium. The value of k is 2 because if a person avoids meeting another person, this other person also meets one less person. This explains why cities are self-organizing because people develop routines to avoid crowds. This model is simplistic, because people move, businesses start and stop, and more people may decide to change their route simultaneously. That is going to keep a city churning and somewhat complex."
  },
  {
    "objectID": "w11.html#sec-exchange-economies-and-externalities",
    "href": "w11.html#sec-exchange-economies-and-externalities",
    "title": "Lesson 11 - Lyapunov Functions",
    "section": "Exchange economies and externalities",
    "text": "Exchange economies and externalities\nAn exchange market has a Lyapunov function and therefore goes to equilibrium. There are other sorts of markets that don’t go to equilibrium. It is sometimes possible to see why is that the case. What prevents us from constructing a really simple Lyapunov function and showing that the system goes to equilibrium is related to Chris Langton’s lambda used in the one-dimensional cellular automata models.\nAn exchange market consists of a situation where people just bring stuff, e.g. fish, baskets or money, and trade these things. The question is whether that system goes to an equilibrium, or are people just going to keep on trading things throughout the day? The assumptions for the model of the exchange market are:\n\neach person brings in an amount of stuff;\n\npeople only trade if this increases their happiness at least with some fixed amount k.\n\nThis fixed amount of k represents effort or transaction costs to make the trade. That is important for the Lyapunov function to work. A possible Lyapunov function could be the total happiness of the people. There is a maximum amount of happiness to be derived from a fixed amount of stuff. There is also some fixed amount k of transaction costs such that, if the process doesn’t stop, happiness goes up by at least k.\nIf North Korea and Iraq exchange nuclear weapons for oil, then both countries would probably be more happy as Iraq has nuclear weapons and North Korea has oil. The United States, China, and many other countries, who were not part of the transaction, probably are not so pleased, so total happiness may not have increased with this trade. If total happiness went down, that may mean that other people then have to make other trades as they try and make total happiness go up.\nIn that case we don’t know for sure whether the system is ever going to stop because we can’t put a Lyapunov function on the process. It may happen in political coalitions or firms merging. When party A merges with Party B, then party C may be upset, and total happiness may not be going up. The same is true with political alliances between countries. They could make other countries less secure. And that could mean that there is no Lyapunov function.\nWhen two people decide to date, they are both happier. Or when two people break up, presumably they are both happier. But that could affect other people who are friends of those people, who maybe wanted to date one of those people, and it’s not clear. Maybe dating has a Lyapunov function, maybe happiness is a Lyapunov function for dating, maybe it’s not. It depends on the size of the externalities.\nThat can be related to Langton’s lambda parameter from the simple cellular automata model. The cellular automata model tells that systems where behavior isn’t influenced by others, tend to go to equilibrium, while systems where behavior is influenced by others, tend to be complex or random. Similarly, we can apply a Lyapunov function where someone’s actions don’t materially affect others, or if they do, they make them happier, meaning that there are no negative externalities."
  },
  {
    "objectID": "w11.html#sec-time-to-convergence-and-optimality",
    "href": "w11.html#sec-time-to-convergence-and-optimality",
    "title": "Lesson 11 - Lyapunov Functions",
    "section": "Time to convergence and optimality",
    "text": "Time to convergence and optimality\nThere are two questions to Lyapunov functions:\n\nHow long does the process take to go to an equilibrium?\n\nDoes the process always stop at the minimum or the maximum?\n\nHow long does the process take to go to an equilibrium? If the process stops, it is in equilibrium. If it doesn’t stop, then its value according to increases by at least k. Since there’s a maximum, that means that at some point the process has to stop. Suppose that we start out with F(x1) = 100, and k = 2, and the maximum is 200 then the number of periods has to be equal or less than 50.\n\n\n\n\nTrade\n\nDoes the process always stop at the minimum or the maximum? Generally a process can get stuck someplace less than the max. That can be explained in two ways. First, it is possible using a model. The rugged landscape model has multiple peaks. A Lyapunov function can be seen as taking steps up at least some distance each period, but it doesn’t necessarily mean that this is leading to the highest peak.\nA second way is an actual example rather than an abstract model like a rugged landscape. Assume there are three persons, numbered 1, 2, and 3. The graph shows their preferences and they all own the item in the middle column of their corresponding row. Person 1 owns a banana prefers apples to bananas and bananas to coconuts.\nAssume that there is an exchange market. Person 1 likes the apple of person 3, but if he offers the banana to person 3, she refuses. Person 2 likes the banana of person 1 but can’t trade the coconut for it. Similarly, person 3 cannot trade his apple for the coconut of person 2. None of them can make a pair wise trade and be better off."
  },
  {
    "objectID": "w11.html#sec-lyapunov-fun-and-deep",
    "href": "w11.html#sec-lyapunov-fun-and-deep",
    "title": "Lesson 11 - Lyapunov Functions",
    "section": "Lyapunov: fun and deep",
    "text": "Lyapunov: fun and deep\nLyapunov functions are one technique for determining whether or not a system goes to equilibrium. But can we always tell, possibly with the help of other techniques? The question is does the system go to equilibrium or does it not, and can we even tell? We are going to do this in a fun way with some examples and then we will go a little bit deeper. We will see why some processes are very hard to figure out.\nThe fun example is called chairs and offices. A firm is moving to new offices with different types of chairs. An employee, who followed this course, suggests to distribute furniture by randomly assigning each person a chair and then let people trade. The boss thinks that it is not a good idea. But the employee says that at some point the process will stop because the process consists of an exchange market and a Lyapunov function of how happy people are with their chairs. With every trade happiness goes up. People will stop trading if they are satisfied, because it takes time and effort to trade.\nThen the boss also wants to do this for offices and randomly assign people offices and then let them trade. Then the employee said that is a terrible idea. The student says, the office is different because there are externalities. If a person moves because of the habits of another person, e.g. playing loud music, people in the new location may move because of this person’s habits, e.g. wandering around. And so, there will be no increase in total happiness after a move, and the system may not go to equilibrium. Finding this out requires a lot of knowledge about people’s preferences.\nThat leads to the deep question of when can you decide? And can you always decide? The answer is it depends on the problem. In some cases you can figure out some other way to show or prove that the process goes to equilibrium. In some other cases you can come up with a sophisticated Lyapunov function to show that the system goes to equilibrium. But other problems, even those that seem incredibly simple, turn out to be very hard to solve.\ne.g., the Collatz problem or HOTPO (half or three plus one), is that you pick a number, and then do the following. If it is an even number, you divide it by two, if it is an odd number, you multiply it by three and add one. You stop if you reach one. The question is, does this process ever stop? For 5 you get: 16, 8, 4, 2, 1 (stop). For 7 you get: 22, 11, 34, 17, 52, 26, 13, 40, 20, 10, 5 (stops because 5 stops). For 27 and many other numbers the Collatz problem hasn’t been solved yet. So for some numbers we can tell and for some numbers we can’t. For the chair problem we can tell, but for the office problem we can’t.\n\n\n\nLyapunov functions\n\n\n\n\n\nMarkov processes"
  },
  {
    "objectID": "w11.html#sec-lyapunov-or-markov",
    "href": "w11.html#sec-lyapunov-or-markov",
    "title": "Lesson 11 - Lyapunov Functions",
    "section": "Lyapunov or Markov",
    "text": "Lyapunov or Markov\nThere are some fundamental differences between the equilibria with Lyapunov functions and equilibria with Markov processes. Both Markov processes and Lyapunov functions give conditions under which we can determine that a system is going to equilibrium.\nA Lyapunov function F has a maximum value. If the process isn’t in equilibrium then it goes up by at least some fixed amount k. This has to stop at some point. A Markov process has a finite number of states, and if the probability of moving between those states stays fixed over time, and it is possible to get from any one state to any other, and it is not a simple cycle, the Markov convergence theorem states that the system goes to a unique equilibrium distribution that doesn’t depend on the initial state.\nThis is a stochastic equilibrium so the system is still churning. In a Markov process, history doesn’t matter. A Lyapunov function could depend a lot on the initial conditions. There could be many equilibria depending on where you start and where you go. It’s also not a stochastic equilibrium. It is a fixed point.\nIf you can construct a Lyapunov function, then the system goes to equilibrium. If you can’t, that doesn’t mean it doesn’t. If you can write down a Lyapunov function, you can figure out how long it’s going to take to reach equilibrium. A Lyapunov equilibrium needs not be unique or efficient. The reason a Lyapunov system won’t go equilibrium is because of externalities pointing in the other direction.\nThat is the same lesson we learned from the Langton’s lambda in the simple cellular automata model. When one person’s action or one cell’s action depends on the actions of others, the system’s likely to churn. When what I do is unaffected by other people, then the system is likely to go to equilibrium.\n\nReferences\nNote: this page is based on the following source:\n\n[@page2017modelthinking] MOOC, Course material & Transcripts.\nTA Notes by [@fisher2017modelthinking].\nStudent notes by in [@kleinikink2016naturalmoney] and [@groh2017model]."
  },
  {
    "objectID": "w12.html",
    "href": "w12.html",
    "title": "Lesson 12 - Coordination and Culture",
    "section": "",
    "text": "Coordination is doing the same somebody else is doing. Culture refers to differences between groups of interacting people. In order to have these differences between groups of people, there have to be similarities within those groups. That is where coordination matters. Cultural behavior can be suboptimal in the sense of efficiency. Often it doesn’t make sense from the outsider’s perspective, but viewed from within that culture, it makes sense.\nThe pure coordination game that enables us to understand why people do the same thing, and why sometimes they do the same thing that is not optimal. In the context of this game, an important question is does the whole system coordinate if people are trying to coordinate? To understand that, the Lyapunov function can be used on a single coordination game on one behavior.\nIt is also possible to have a many-action coordination game on multiple behaviors. This can be used to explain cultural differences. In particular, Robert Axelrod made a model that looks at how culture emerges, and why you might get thick boundaries between different cultures. There is also inconsistency within cultures. The coordination consistency model deals with both coordination between people and consistency within a person with the use of Markov processes."
  },
  {
    "objectID": "w12.html#sec-coordination-and-culture",
    "href": "w12.html#sec-coordination-and-culture",
    "title": "Lesson 12 - Coordination and Culture",
    "section": "",
    "text": "Coordination is doing the same somebody else is doing. Culture refers to differences between groups of interacting people. In order to have these differences between groups of people, there have to be similarities within those groups. That is where coordination matters. Cultural behavior can be suboptimal in the sense of efficiency. Often it doesn’t make sense from the outsider’s perspective, but viewed from within that culture, it makes sense.\nThe pure coordination game that enables us to understand why people do the same thing, and why sometimes they do the same thing that is not optimal. In the context of this game, an important question is does the whole system coordinate if people are trying to coordinate? To understand that, the Lyapunov function can be used on a single coordination game on one behavior.\nIt is also possible to have a many-action coordination game on multiple behaviors. This can be used to explain cultural differences. In particular, Robert Axelrod made a model that looks at how culture emerges, and why you might get thick boundaries between different cultures. There is also inconsistency within cultures. The coordination consistency model deals with both coordination between people and consistency within a person with the use of Markov processes."
  },
  {
    "objectID": "w12.html#sec-what-is-culture-and-why-do-we-care",
    "href": "w12.html#sec-what-is-culture-and-why-do-we-care",
    "title": "Lesson 12 - Coordination and Culture",
    "section": "What is culture and why do we care?",
    "text": "What is culture and why do we care?\nThere are many definitions of culture. Culture is such a complex thing that no simple definition can capture it. Still, culture mattes a lot for the success of countries. Tylor, who is the father of modern cultural anthropology, said in 1871 that culture is the complex whole which includes knowledge, belief, art, law, morals and customs. Boas extended this in 1911 by calling culture the totality of mental and physical reactions and activities that characterize behavioral responses to environment, others and to the self.\nBoas was the first to get the idea that there should be some consistency across these mental and physical reactions to the world. This consistency could be partly brought on with the environment or it could be socially constructed. These dry academic definitions don’t show exactly what culture is. Writers have also written definitions of culture, e.g. Calvin Trilling wrote in 1955:\n\nWhen we look at people in the degree of abstraction which the idea of culture implies, we cannot but be touched and impressed by what we see, we cannot help but be awed by something mysterious at work, some creative power that seems to transcend any particular act or habit or quality that may be observed. To make a coherent life, to confront the terrors of the outer and the inner world, to establish the ritual and art, the pieties and duties which make possible the life of the group and the individual - these are culture, and to contemplate these various enterprises which constitute a culture is inevitably moving.\n\nTo make a coherent life refers to the idea of consistency. An essential reason for the existence of culture is to made possible the life of the group and the individual. In order for groups and individuals to function, they have to have some similarities within. They have to agree or coordinate on certain sets of behaviors, morals, laws, customs, and understandings, in order to confront the inner and outer world. The interesting thing about cultures is that people do this in different ways.\n\n\n\n\nCulture dimensions\n\nWhat could these differences be? And how extreme can these differences be? One way of figuring this out is doing experiments. The ultimatum game is an experiment that was done in different societies. Player 1 is given $10. Then player 1 is told to offer a split to player 2. The split might vary, e.g. $5 apiece, $2 for player 2 or even $0.01 for player 2. Player 2 can either accept or reject. If player 2 accepts the money, then they split it. If he rejects, they both get nothing.\nPlayer 1 has to figure out what’s the minimum amount he can offer player 2 to get it accepted. People from different cultures didn’t play the game in the same way. One group were the Lamalera, who are Indonesian whale hunters that hunt collectively. Another group, the Machigenga, who are an Amazonian group that hunted individually and didn’t even use names, was much more selfish. On average the Lamalera offered $5.70, while the Machigenga offered $2.60.\n\n\n\n\nCulture of individuals in Sweden\n\n\n\n\nCulture of individuals in Zimbabwe\n\n\nRon Inglehart, who spent decades surveying people all over the world doing something called the World Value Survey, constructed a graph by ranking different cultures on two dimensions, which are survival versus self-expression and traditional versus secular-rational. If you divide countries in these two ways, then the whole world makes sense by geography. e.g., protestant Europe is in the self-expressive secular realm while Islamic and African countries are more in the religious and survival-based area.\nThis map might suggest that all people from Sweden or Zimbabwe score the same. That is not true. Each country has a population and within this population there are differences. The point on the graph is just an average. The individual data points are clouds. But there is very little overlap between the scores of individuals in Sweden and Zimbabwe, so there is a difference between people from Sweden and Zimbabwe, but also differences within the Swedish people and within the people from Zimbabwe.\nGeert Hofstede, who focused on the business world, uses five dimensions to make sense of cultures, which are:\n\nPower distance or how much inequality is tolerated,\nUncertainty avoidance versus risk taking,\nIndividualism versus collectivism,\nMasculinity versus femininity, and\nConfucianism versus dynamism or how forward looking are you?\n\nThe US scores are: power distance 32 (a low value signaling a lot of inequality tolerance), uncertainty avoidance 40 (low), individualism 90 (very high), and masculinity 60 (high). The scores of France are: power distance 61 (a high value signaling little inequality tolerance), uncertainty avoidance 80 (very high), individualism 63 (high), and masculinity 32 (low). Those dimensions are useful because they capture differences. But those dimensions don’t capture everything. e.g., South Korea and El Salvador score practically the same on Hofstede’s dimension, but they are completely different countries.\nWhy should we care about culture? This is because the economy, political systems and society work through social exchanges. According to Kenneth Arrow:\n\nVirtually every commercial transaction has within itself an element of trust, certainly any transaction conducted over a period of time. It can be plausibly argued that much of the economic backwardness in the world can be explained by the lack of mutual confidence.\n\n\n\n\n\nWealth versus trust\n\nIn cultures there are different levels of trust, and these different levels of trust have huge implications for how well political, economic, social, and religious institutions perform in terms of meeting the needs of the citizens. Bob Solow, who invented the growth model, wanted these notions of trust and social capital to be measurable in some way. That is one of the reasons why we have models like those of Inglehart and Hofstede.\nHow do you measure something like trust? This can be done using questions like do you claim government benefits you are not entitled to? Do you avoid paying a fare on public transport? Do you cheat on your taxes? Do you keep money if you found it? Do you fail to report damage you’ve done accidentally to a parked vehicle? These sort of questions indicate how trusting are people within a society.\nYou can also ask the question, do you think people in general can be trusted? There are massive differences across countries. In Sweden 70% of people answer this question positively. In Italy, it is 33%, and in Turkey it is only 10%. That is going to affect how well the economy is going to work.\nIt does matter a lot, because in general, rich countries have a higher level of trust. There is a correlation between trust and wealth, but that doesn’t imply that trust causes higher levels of wealth even though that seems plausible. It might also be that higher levels of wealth make people more trustworthy."
  },
  {
    "objectID": "w12.html#sec-pure-coordination-game",
    "href": "w12.html#sec-pure-coordination-game",
    "title": "Lesson 12 - Coordination and Culture",
    "section": "Pure coordination game",
    "text": "Pure coordination game\nTo make sense of cultural differences, a pure coordination game can be used. Some examples help to understand the pure coordination game. The ketchup question is do you store your ketchup in the fridge or the cupboard? Most Americans store their ketchup in the fridge, but people from England tend to store it in the cupboard. Where you store your ketchup is a cultural decision. You store it in the same place other people store their ketchup. This is an example of a coordination game. It is a situation where you do the same thing other people do, otherwise they can’t find the ketchup.\n\n\n\n\nDagen H logo\n\nOther examples are electrical outlet plugs. Different countries have different plug configurations. Everybody in the country wants to coordinate on the same plug type, but different countries may coordinate on different types of plugs. The ramifications of failure to coordinate can be rather severe.\nOn the third of September in 1967 (Dagen H) at 4:00 AM, Sweden switched the side of the road they drove on from left to right. People from Sweden drove to other countries and caused accidents. It is also easier to make cars with steering wheels on the same side. England and some former British colonies still drive on the left. Those countries are mostly islands. On an island, you don’t have to worry about crossing a border and causing an accident. That is a coordination game.\n\n\n\n\nPure coordination game\n\nThis can be turned into a formal game. A game is a set of players with actions and payoffs. In the pure coordination game there are two players, and each player has one action. e.g., they can put the ketchup in the fridge or they can put it in the cupboard. If they both put their ketchup in the fridge, both get a payoff of one. If they both put their ketchup in the cupboard, both also get a payoff of one. But if one of them one puts it in the cupboard, and the other one puts it in the fridge, then each player gets zero because neither one can find the ketchup.\n\n\n\n\nN-person coordination game\n\nThe game can be expanded to \\(N\\) persons that have to decide about something like driving left or right or whether the ketchup should be stored in the fridge or in the cupboard. Assume that they play this game against their two neighbors and switch their behavior if both neighbors behave differently. Because people want to coordinate their behavior to match other people they change their behavior to create the similarities within. After three steps you also see differences between groups emerging.\nDoes this game actually lead to similarities? Does this process stop or does it keep on churning? The answer can be found using the Lyapunov functions. If the process can be represented by a function that has a maximum, and it goes up by at least some fixed amount each time if it moves, then the process is going to stop at some point.\nIf this pure coordination game is played asymmetrically, that means having only one person moving at a time, which is unlike the example in the graph, then you can show that it satisfies the Lyapunov function of the number of coordinations. e.g., if someone switches to blue, the number of coordination goes up by two. There is also a maximum to the number of coordinations, meaning that everyone has the same color, so the process has to stop. The process doesn’t have to stop with everybody being the same as you can get a boundary between the colors.\n\n\n\n\nInefficient coordination\n\nThere is a difference between coordination and the standing ovation problem. In the coordination model, e.g. choosing the side of the road to drive on, there is a measurable difference in payoffs so no-one would choose not to coordinate. In the standing ovation model it is more a psychological effect. It is no problem to differ from other people.\nInefficient coordination refers to behavior that doesn’t seem optimal. To make sense why this happens, you can use the inefficient coordination game. It may apply to implementing the metric system \\(M\\) versus the obscure English system of measurements \\(D\\). If both parties agree on \\(M\\), they get a payoff of 2. If both agree on \\(D\\), they get a payoff of 1. If they differ, they get a payoff of 0. If both parties were free to choose they would agree on \\(M\\), but in England people are stuck with \\(D\\).\nHow do you get inefficiency? The metric system wasn’t around when the English measurements were made up. Situations can shift. This can be explained with the shake-bow game. Greeting people is a coordination game. It is important that we greet people the same way. If one shakes and the other bows, eyes get poked out. If both shake hand, they get a payoff of 1. If one shakes and one bows, we both get 0. If both bow, bot get a payoff of \\(a\\).\nThe question is: which is better, bowing \\(a &gt; 1\\) or shaking \\(a &lt; 1\\)? Shaking could be better than bowing because people can get a sense of the other’s general health by feeling. However, recently the value of \\(a\\) has increased because people now travel all over the world in airplanes with germs over the seats and touching these seats spreads germs all over the hands. And then bowing is better than shaking because the world has changed. As a result, countries that are shaking could get stuck in a suboptimal way of greeting, just like England is stuck in a suboptimal system of measurements."
  },
  {
    "objectID": "w12.html#sec-emergence-of-culture",
    "href": "w12.html#sec-emergence-of-culture",
    "title": "Lesson 12 - Coordination and Culture",
    "section": "Emergence of culture",
    "text": "Emergence of culture\nTo understand why cultures differ, the pure coordination game is extended to multiple games. Culture is the result of lots of coordination games. Here are some example questions. Do people wear shoes in your house? Do you cross the street when the “don’t walk” sign is flashing? Do you read the newspaper at the breakfast table or are you talking to people? Do you hug your friends when you see them? And do you interrupt someone who’s talking?\nEach of these situations is a pure coordination game. If everybody else is wearing shoes in the house, you don’t want to take your shoes off cause you will get dirt all over your feet. If you are walking with friends, and they don’t cross the street when the sign is flashing and you do, then they will be standing on one side of the street and you will be standing on the other. If you read the newspaper at the breakfast table and the other person is hoping to engage in conversation, he is going to stare at your newspaper. If you’re in a culture where people don’t interrupt and somebody interrupts you, this disrupts the flow of the conversation.\nPeople that give the same answers to these questions are culturally similar. If there are 20 coordination games with two possible answers to each, then you have 220 or more than 1,000,000 possible cultures. There are of course more coordination games, and many of them have more than two answers, so there are many more possible cultures. Why do cultures differ? The paradoxical answer is they differ because everyone is trying to coordinate. People are trying to coordinate within a culture most likely on issues that are different than those in others cultures.\n\n\n\n\nAxelrod’s definitions\n\nRobert Axelrod’s culture model [@Axelrod1997DisseminationofCulture] explains how cultures emerge, and why there are boundaries between cultures. Axelrod defines a set of features which are coordination games like whether you read the newspaper at the table or talk. Traits define what action you take on that feature, e.g. talk. There could be multiple options. A person is a vector of traits on features. For instance, person A wears shoes in the house, stores ketchup in the fridge, and reads the newspaper at the table.\n\n\n\n\nSocial space\n\nAxelrod then puts people in social space that could be represented by a grid of boxes that represent people. The last assumption Axelrod makes, is that people interact with their four direct neighbors in the grid with a probability that is equal to the similarity between them. This interaction works like a coordination game.\nIf you don’t agree on much with your neighbor then you don’t interact and don’t change your behavior. If you agree on everything then you interact for sure, but then you don’t have to change anything. If you agree for 70% of the traits, then you randomly pick a feature, and then match the trait of that person. It could be you already match that person on that trait, but if you don’t then you will switch and match.\n\n\n\n\nAxelrod’s game\n\nThis can be worked out using a model. Suppose there are five features, ten values, four neighbors (North, South, East, West), and the similarity is the percentage of traits that a person agrees with the another person. When two people meet, one of them is a leader while the other is a follower. e.g., the leader may have the characteristics 53211 and the follower 51331.\nThe probability these two people are going to interact is 40% because they agree on two traits out of five. If they decide to interact, then then they decide play a coordination game on one of the traits, e.g. the second trait. The follower then matches this trait to the leader so that his characteristics become 53311.\n\n\n\n\nStart of game\n\n\n\n\nEnd of game\n\n\nThis can be simulated using NetLogo. In this example there are five features and twelve traits. In the centre of each square is a person. The thickness of the boundary represents the number of treats those two people agree on and thus the likelihood they will interact. If that line is dark, there is little agreement, and if it’s lighter, there is more agreement and a higher probability of interaction.\nOver time the number of regions are falling and the cultures are becoming more similar. The largest region is also growing but there is still some cultural heterogeneity. And if we let it run till it stops, you get a few separate cultures and there are thick boundaries around them. They have to have these thick boundaries because if they didn’t, people would interact with their neighbors across those boundaries and become similar.\nAxelrod’s model shows that similar regions emerge, so that you get multiple cultures, but the boundaries between those cultures are thick. Axelrod then concludes that if our neighbors are like us, we tend to interact with them, and if they are not, we tend not to. This produces distinct cultures with thick boundaries or vast differences between these cultures. Thick boundaries emerge because of the fact that if there weren’t thick, then people would interact and become more similar so that the boundary would disappear. Axelrod’s model shows how distinct cultures on multi dimensions can emerge with self-reinforcing boundaries."
  },
  {
    "objectID": "w12.html#sec-coordination-and-consistency",
    "href": "w12.html#sec-coordination-and-consistency",
    "title": "Lesson 12 - Coordination and Culture",
    "section": "Coordination and consistency",
    "text": "Coordination and consistency\nAxelrod’s model leaves out the notion of consistency within a culture. Jenna Bednar developed a model of culture based on coordination games that includes coherence and consistency as well as the heterogeneity within cultures. In Ron Inglehart’s World Value Survey, there are a lot of consistencies across countries, e.g. Protestant Europe looks the same, Catholic Europe looks the same and the Islamic countries look the same. On the other hand, everybody hasn’t the same characteristics. e.g., people in Sweden differ. And the same is true for Zimbabwe.\nTo explain why there is consistency and heterogeneity within a culture we assume that:\n\nthe values, actions, and behaviors that people coordinate on have some meaning;\n\npeople desire some consistency, e.g. to avoid cognitive dissonance;\n\nthere is some innovation and errors.\n\nThe coordination rule is that two people, that each have a vector of actions, beliefs or attitudes, meet, and the follower randomly picks a dimension, and coordinates on this dimension, e.g. he puts the ketchup where his friend puts the ketchup. Consistency would be this that a person looks at himself, and attribute values of the vector have meaning. e.g., a person with characteristics 5144 might think, I’m 5 on the first and 1 on the second, and that doesn’t make sense, so I switch and become 5 on the second. And so the characteristics of this person become 5544.\ne.g., you come from a family that is pretty reserved, so you don’t even hug your parents very often, and have hugging behavior 1. Then you go to college and all your friends hug each other. Now you have hugging behavior 5 with your friends. When you go home you realise you have been hugging someone you just met a month ago to say good-bye so you start hugging your mother and change your behavior to 5. If you hug your friends, you might as well hug your mother. In this way you become more consistent in terms of how you behave.\n\n\n\n\nInnovation spreading\n\nIf we assume that people try to coordinate with other people, and try to be consistent, it could be expected to lead to consistent coordinated behavior. It happened but the process took an incredibly long time to converge. When some tiny errors or innovations were introduced, then there were big spreads in the characteristics of individuals like those in Sweden and Zimbabwe. Only a small number of people have to try new things, to get that level of heterogeneity within a community.\nThat was a surprising result. The question is why? Suppose the model converges to everybody being 5 on everything and somebody decides to innovate a bit and change to 6 on one thing. We might think that this should go away because people try to be consistent with themselves or meet somebody else so that it gets corrected to 5.\nAnother possibility is that during an interaction somebody else copies the 6 or that people themselves switch other attributes to 6 in order to be more consistent. In this way the six can spread within a person (horizontally in the graph) and across people (vertically in the graph). If more of such innovations or errors are made, e.g. switching to 7, these can also spread. This means the process isn’t going to converge very fast. There is a lot of spread because errors propagate in many directions. Most traits are going to be fives, but there will be sixes and sevens everywhere.\nIt is possible to mathematically understand why small amounts of error can cause a big spread. The simplest model consists of two agents, two games, and two actions. e.g. two people are deciding whether to hug or bow with their family, or hug or bow with their friends. Now it is possible to write down all possible states of the world. The columns represent the games. The rows represent the persons.\n\n\n\n\nStates of the world\n\nThe following could happen:\n\nboth do the same thing on both games, e.g. action red;\n\none person is taking one action on both games, but the other person is taking the other action one game, which is called off by one;\n\nthey could be consistent but not coordinated, e.g. one person is green on both, and the other person is red on both;\n\nthey could be coordinated but not consistent, e.g. they could both play the red action on one game and the green action on the other game, but they would have a lot of cognitive dissonance because neither one is consistent;\n\nthey could be not coordinated and not consistent, which could be a total mess.\n\n\n\n\n\nConsistency dynamic\n\nIf this system is without an error, the dynamics can be described. e.g., in the state where people are off by one, the consistency dynamic is as follows:\n\nperson 2 could look at himself, check for consistency, and conclude that he is consistent, and nothing would change;\n\nperson 1 could look at herself, check for consistency, and conclude that she is not consistent, and switch both actions to red. Then both would be consistent, but neither would be coordinated;\n\nperson 2 could look at herself, check for consistency, and conclude that she is not consistent, and switch both actions to green. If person 1 finds herself to be inconsistent, then there is a 50% chance for (2) to happen as well as a 50% chance for (3) to happen.\n\n\n\n\n\nTransition map\n\nIt is also possible to write down the coordination dynamic for all the different states including the probability of moving from those states to other states. e.g., if there is a total mess (top), and one of the persons looks at himself or herself and decides to become consistent, then they move to the off by one state (centre). Alternatively, suppose that two people may decide to coordinate. Then again, they move to the off by one state. No matter what happens, with probability they move from the total mess state to the off by one state.\nIn the off by one state (centre), a lot could happen. They could stay in the off by one state, e.g. when person 2 looks at himself and decides that he is consistent, or if these two people decide to coordinate on action 1. Alternatively, it would be possible to move over to the state consistent but not coordinated (right) or coordinated but not consistent (bottom). Finally, they could move to the coordinated and consistent state (left). This is the only state that is stable because they will not move out.\n\n\n\n\nMarkov process\n\nThis is like a Markov process, except that they can’t get from any state to any other state because they can get stuck in the equilibrium coordinated and consistent state. If there is innovation or error \\(ε\\) then they can move back to the off by one state and then it is a Markov process.\nIn this way it becomes possible to write a matrix we with the horizontal axis representing the states at time \\(t\\) and the vertical axis representing the states at time \\(t+1\\). The cells represent the odds of moving from one state to the other. This model shows that small innovation rates lead to substantial heterogeneity. This was a big surprise.\nThere are differences between cultures, so people from France behave differently than people from Mexico. There are also see similarities within, meaning that people within an interacting group become similar. Their behavior isn’t identical and there is a lot of within-group heterogeneity, e.g. in Sweden or Zimbabwe. Some of that behavior is interesting in the sense that it doesn’t appear to be optimal from the outside. e.g., people might do 5 to become consistent, even though it is not optimal.\nThere are three ways people can coordinate on a wrong action:\n\nby idiosyncratic coordination on the wrong thing.\npayoffs can change over time, e.g. with bowing versus shaking hands.\nto maintain consistency, people could choose a behavior that is not optimal in one domain.\n\nCulture can be seen as multiple coordination games, where people are trying be consistent. To get good outcomes, it’s important to coordinate behavior, e.g. to greet people the same way. There is still a lot of within-group heterogeneity that could be explained by assuming that there is a small amount of error and experimentation as errors propagate in two directions. This could be shown using a Markov model.\nPure coordination behavior creates a Lyapunov function so this process converges quickly.\nIf we include consistency and error then we no longer have a Lyapunov function , but we could use a Markov model.\nThe Lyapunov model and the Markov model can be used to explain other models like the culture model.\nEven though culture is complex, simple models allow us to understand some basic properties of cultures, such as difference between cultures, consistency within cultures, and heterogeneity within cultures.\n\nReferences\nNote: this page is based on the following source:\n\n[@page2017modelthinking] MOOC, Course material & Transcripts.\nTA Notes by [@fisher2017modelthinking].\nStudent notes by in [@kleinikink2016naturalmoney] and [@groh2017model]."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "notes-model-thinking",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "w01.html",
    "href": "w01.html",
    "title": "Lesson 1 - Why Model?",
    "section": "",
    "text": "Perhaps not the subject of this course, but sometimes I wonder\nIf you are still here, have a look at this model Zoo.\nSo what may seem like a good idea the first time you think 🤔 about it may not look so hot once you look at it through the lens of a model?"
  },
  {
    "objectID": "w01.html#why-model",
    "href": "w01.html#why-model",
    "title": "Lesson 1 - Why Model?",
    "section": "Why model?",
    "text": "Why model?\n\n\n\n\nGeorge E. P. Box — Statistician, Walmart Model, possible sweet tooth.\n\n\nEssentially, all models are wrong, but some are useful. — George E. P. Box [@box1976] 1\n1 :wondering: Both the course and, even more, so his book, [@page2018model], are punctuated with quotations that repeatedly interrupt the flow, gaining nothing in the process when the author might have explained some of the more puzzling assertions he made. Let’s keep these under reign, emphasizing the few that abbreviate and aid in communicating a salient point.\nThis obligatory quote on models is the most famous, and here is the next:\n\nIt is difficult to make predictions, especially about the future. — Yogi Berra"
  },
  {
    "objectID": "w01.html#reasons-to-use-models",
    "href": "w01.html#reasons-to-use-models",
    "title": "Lesson 1 - Why Model?",
    "section": "Reasons to use models:",
    "text": "Reasons to use models:\n👎 I don’t buy the claims that knowing how to use a model is a requirement to be an intelligent citizen of the world. 2 Dr. Page makes a better claim about knowing many models and which one makes more sense in a given context - but that’s a different ballpark.\n2 👎 While knowing how to model can be helpful, I’m pretty confident it is not required. Newsflash 📰 The UN isn’t giving any world citizen cards even if you pass this course 😆3 💭 thinking can get complicated as models grow, there can be many features, many steps so we should remember to keep asking “How does this model make my thinking clearer?” to keep us on the right track!👍 Using models helps us think 🤔 more clearly3. When models explicate assumptions, we can use them to divide and conquer big problems into more manageable, clearly labeled parts (like endogenous variable, exogenous variable, effect size, p-value, statistical-significance, practical-significance) and keep us focused 📸 by prioritizing 🚨 the effects while omitting insignificant minutia.\n👍 Becoming a better decision-maker! Experts relying on intuition and rules of thumb tend to exaggerate the importance of edge cases and frequently fall victim to their bias. For these reasons, a simple linear model calibrated to actual data often makes better predictions.\n👍 Models help us to use and understand, decide, plan, and design. The primary use of modeling in industry within production systems is where they replace human decision-makers to make recommendations or decisions.\n\n\n\n\nPhillips with his MONIAC computer\n\n👎 Fertility Model can have applications in other fields. Simple models tend to be the most fertile. Some examples are:\n\nModel everything first using a straight line\nModel anything using a normal distribution\nColoumbs’ law has the same form as Newton’s universal law of Gravitation,\nWhen economic models are simpler, their applications broaden.\nBenoit Mandelbrot said he saw the same graphs he was studying long-term interaction in time series of Nile flooding data in a researcher working on US grain supply.\nPhysical analogies for differential equations e.g. MONIAC shown on the right that Bill Phillips used to model the uk economy with water.\nMarkov chains have been used to identify authorship of the Federalist Papers.\nKeep the first term of a Tylor expansion, AKA first-degree approximation.\nKeep the first terms of a Fourier expansion\nKeep the biggest term of a linear model, AKA “the big coefficient” 4\nEquilibrium in game theory\n\n4 rareSo, we get two dimensions of fertility.\n\nsame law for different phenomena. 5\nsame math, different applications. 6\n\n5 rare6 Obvious"
  },
  {
    "objectID": "w01.html#sec-intelligent-citizens-of-the-world",
    "href": "w01.html#sec-intelligent-citizens-of-the-world",
    "title": "Lesson 1 - Why Model?",
    "section": "Intelligent citizens of the world",
    "text": "Intelligent citizens of the world\n\n“πόλλ’ οἶδ’ ἀλώπηξ, ἀλλ’ ἐχῖνος ἓν μέγα” “a fox knows many things, but a Hedgehog knows one big thing” — Erasmus’s Adagia from 1500\n\nModels work because they are abstractions that simplify.\nModels have become essential to the lingua franca 7 of academia, business, and even politics. Companies regularly place models in their production systems, automating decisions and improving internal processes.\n7 new languageDr Page alludes to the Great Books Movement, which by all accounts was a failure not only tried to sell out copyright books to its clients but also claimed to inform the intelligentsia with a collection of ideas they should be familiar with. One such idea is “tie yourself to the mast”, alluding to Odysseus’ request from the Odyssey where he wished to the siren’s song. This idea recurs with the British painter Turner, who wanted to observe a storm.\nDr. Page points out that proverbs that encode these ideas sometimes come in contradictory pairs, e.g.:\n\n\n\n“Two heads are better than one”\n\n\n\n“Too many cooks spoil the broth.”\n\n\n\n\n\n\n“he who hesitates is lost.”\n\n\n\n“A stitch in time saves nine.”\n\n\n\nDr. Page succinctly states, “Models tie us to a mast of logic.” When we run a model, we can avoid being swayed by our biases and usually also get an estimate of how well this outcome fits with observed reality.\nMany sciences rely on models. Game theory models are foundational in understanding behavior in economics, political science, and even linguistics. Networks are used in sociology, ecology, and biology.\nAnother reason to use models is that they are better at predicting than making a decision or an estimate without one. The following section makes the point that using multiple models is even better.\n\n\n\n\n\n\nPerformance By Tetlock\n\n\nThis graph, taken from [@tetlock2015superforecasting] by Psychologist Philip Tetlock‘s research into the accuracy of political prediction by experts, compares the analysts’ accuracy called calibration on the horizontal axis, with how conservative their predictions are on average.\nLet us review the groups we can see in the graph:\n\n🧢 Undergrads make the worst analysts.\n🦔 Hedgehogs are the analysts whom Tetlock characterizes as domain experts who know a lot in one area but effectively use a single 🔮 large model. Hedgehogs tend to be poor at predicting future outcomes. This is the type of analyst 📺 TV panels tend to feature — wrong but consistently wrong ❌ for decades.\n🦊 Foxes are the analysts whom Tetlock describes as utilizing 🔮 many 🔮 loose 🔮 models 🔮. They tend to be better than the Hedgehogs and can become good predictors once they 🏫 learn to 📏 calibrate. 8\n🤖 Formal models, Even simple linear ones, tend to outperform the foxes. 9\n\n8 Tatelocks describes this as learning to update their evaluation dozens or hundreds of times as they collect and evaluate new evidence. These updates are usually getting smaller as time goes by.9 Formal models are mathematical abstractions. An essential point omitted in the lesson is that they tested these formal models based on the data provided by the above hedgehogs and foxes.👎 Models make us humble. Using a model can dramatically change our initial picture of the situation. Models that work for a long time can stop working overnight, and in many cases, their users don’t know why or how to fix them.\n\nExample 1 (Real Estate Crash of 2008) Most people who used a simple linear model of house prices failed to forecast the housing crash of 2008. 10\n10 💭 so Models may make us complacentScott stated that analysts who predicted the crash could have made a killing on the stock market. This is the plot of the The Big Short. But this is overselling the model and ignoring the real complexities.\n\n👿 almost everyone in the know knew there was a Real Estate bubble\nBubbles are very profitable 💵: till they explode 🤯.\n🔮 Modelling when 📅 a bubble will explode is a notoriously difficult task 😵.\nMaking a 💵 big 💵 profit 💵 requires making a bet 🙄 by buying 🩳 short positions in the market 🛍️.\nAs the bubble grows, you need to 💰 pay 💵 to keep your 🩳 short position.\nYou need to raise a lot of 💰 money from 🏌️ investors to make a profit from a crash, you need to keep the 🩳 short position contract until the bubble bursts or you lose your pants 🤢.\nThe system is 🏴‍☠️ rigged—even when the bubble burst you will have a hard time 🦈 cashing in. The investment banks 🏦 and insurgence companies 🏛️ refused to devalue their derivatives even when the primary assets had crashed until they had dumped their toxic 🤑 portfolios and safeguarded their positions 🤷🏽.\nSome investment banks 🏦 went under 🤦‍♂️ before they honored their contracts.\n\nA final point is that their 🔮 models are not easy to find; they were probably not published.\n\nThe point is therefore that using multiple 🧠 mental 🔮 models is good approach and that if they are formal models then we are going to be better off. In many complex scenarios, we will consider only people using multiple models are better than a random guess.\n\n\n\n\nBruce Bueno de Mesquita\n\n\n\n\nNot all forecasters are humble but Good forecasters use models tempered by personal judgement. Bruce Bueno de Mesquita, International Conflict predictor. Recent books include @de2011dictator; @de2009predictioneer; and @de2005logic. Models give guidance that blends experience and decision-making.\nThe course is concerned primarily with agent based models. However, these are often rule based models, which in certain cases be formalized using game theory which may allow a more mathematical approach.\n\n\n\n\nGame theory model"
  },
  {
    "objectID": "w01.html#thinking-more-clearly",
    "href": "w01.html#thinking-more-clearly",
    "title": "Lesson 1 - Why Model?",
    "section": "Thinking 🤔 more clearly",
    "text": "Thinking 🤔 more clearly\n\nA problem well-stated is a problem half-solved — Charles Kettering, head of research at GM\n\nModeling following the reductionist approach in science. The let us think 🤔 more precisely about how things work and in the process we get to throw out any parts that are less important.\nUsing models consists of the following steps:\n\nName the parts.\n\nIf we build a real estate regression model for pricing, you need to collect data on apartments, such as location, historical prices, size, rooms, floor, view, directions, and neighborhood.\nIf we build a movie recommendation system, we need to identify the movies, their rating, and the people who watched and ranked them.\nIf we need to model a marketing website, we need to know the customers, their customer journey, their interaction with ads, landing pages, product and information pages, shopping carts, checkout forms, inventories, and social media, as well as minor events and particularly ZMOT. All these and more are available from web/app analytics products and from advertising platforms.\n\nIdentify the relationships between the parts. In a regression, you get the coefficients that set the size of the effect that changing the explanatory variables has on the response variable.\n\nin game theory model we consider the relationships between action by player 1, subsequent actions by player 2 and thier ultimate payoffs.\n\nWork through the logic. :::\n\n\nSay we want to estimate the length of a rope needed to tie around the Earth at one meter above the surface. Let’s further assume the Earth’s circumference is 40,000 km. Since\n\\[C = πD\\]\nwe get\n\\[\n\\begin{aligned}\nC  &= \\pi(D_\\text{Earth}  + 2m) \\\\&= πD_\\text{Earth}  + (π * 2m) \\\\&= 40,000 km + 6.2m\n\\end{aligned}\n\\]\n\nInductively explore adding one case at a time until you arrive at a general rule\nModels of 🦋 dynamical systems ⛈️ have different classes 11 of outcomes, these are:\n11 each may take different forms\n🧷 Equilibrium (characterized by some steady state pattern)\n🌀 Cycle (characterized by some periodicity)\n🤡 Random (characterized by Probability distribution)\n🧭 Complex 12 (characterized by Strange attractors)\n\n12 ChaoticModels can help us to predict which of these outcomes will materialize.\nFor instance, the supply and demand for oil might increase in a relatively consistent pattern. The cost of oil is influenced by various factors, including reserves, market participants, political dynamics, etc., making its price complex and difficult to forecast, yet not entirely unpredictable.\nIdentify logical boundaries. Models help us determine what context is relevant and at what point it is no longer applicable.\nFacilitate communication. Taking politics as an example: by evaluating how liberal or conservative both a candidate and a voter are, we can place them in a model to see which candidate aligns more closely with the voter’s preferences. This can be used to choose the best talking point for a call to that voter soliciting a campgn contribution."
  },
  {
    "objectID": "w01.html#sec-using-and-understanding-data",
    "href": "w01.html#sec-using-and-understanding-data",
    "title": "Lesson 1 - Why Model?",
    "section": "Using and understanding data",
    "text": "Using and understanding data\nAnother model type is the explicative model, which can be used to understand and understand data.\nUnderstand patterns. e.g., fluctuations in GDP growth can be explained by a business cycle model.\nExtrapolate. e.g., if the price of a house in a neighborhood is a linear function of the number of \\(m^2\\), and you know the number of \\(m^2\\), then you could use the function to predict the price of the house via the point value on the function graph;\nProduce bounds models that capture uncertainty like an estimate of inflation ten years hence should produce a range rather than one specific number; this range can be used as a lower and upper bound in any downstream tasks\nRetrodict 13. We can use models with the data to predict the past. If we have missing data in our historical records, we can put our model to work to interpolate the missing past. This is also called retrodiction or imputation. Another way we use past data is for model cross-validation, where we omit part of the data from model training and then evaluate the model on the held-out data.\n13 predict the pastPredict other things. If you build a model that is good at predicting the unemployment rate, you are well on the way to predicting the inflation rate, which we call a downstream task. In astronomy, early models of the solar system predicted an unknown planet; guided by this, astronomers pointed the telescopes into the night sky and soon discovered Neptune.\nInformed data collection. When we build a a model to predict school performance to help improve education policy, we begin by naming the parts, in these case, we might choose the following explanatory variables: teacher quality, the parents’ highest level of education, absences, the school’s budget, size of the classroom, etc. We fit the model and we can quickly see if we need more data. Thus the model informs us determine which and how much data we need to collect.\nEstimate hidden parameters14. We can use data to tell us more about the model and then use the model to tell us more about the world.\n14 latent variablesthe SIR model is an established contagion model describing the spread of diseases. SIR is an acronym for the compartments of Susceptible, Infected, and Recovered individuals. Once we observe the data on how many people are in each compartment, we can predict how an epidemic will progress over time.\nCalibrate. After constructing a model, one should calibrate it to the data so that its predictions are as close as possible to our view of the real world."
  },
  {
    "objectID": "w01.html#sec-models-for-decisions-strategy-and-design",
    "href": "w01.html#sec-models-for-decisions-strategy-and-design",
    "title": "Lesson 1 - Why Model?",
    "section": "Using models for Decisions, Strategy, and Design",
    "text": "Using models for Decisions, Strategy, and Design\nModeling helps us make better decisions. Models let us formulate a strategy, an optimal response to any contingency. They are also useful when we want to improve a design."
  },
  {
    "objectID": "w01.html#some-use-for-models",
    "href": "w01.html#some-use-for-models",
    "title": "Lesson 1 - Why Model?",
    "section": "Some use for models",
    "text": "Some use for models\n\nDecision Aids\n\n\n\n\n\n\n\nFigure 1: A Diagrammatic Depiction of Co-Risk Feedbacks\n\n\nWhen probabilistic distributions are weighted with the cost of each possible alternative, managers get a view of the risk associated with different decisions. c.f. co-risks chart, which suggests why the US government let Lehman Brothers fail, but bailed out AIG. The economy eventually recovered and it appears the US government made a sound decision.\n\nExample 2 (Company bailout) We can use the co-risk graph to understand the network of relationships between financial institutions like Morgan Stanley, Bear Sterns, AIG, etc. in, in terms of how their economic success or failure are correlated to one another. Notice the hubs in red, Bear Sterns, and AIG. The layman’s term for this is Too big to Fail.\nThe US government, facing the prospect of cascading failure of it largest financial institutions, has to decide which banks to bail out and which ones to allow to fail. The graph which shows the dependence between the top financial institutions indicating how likely one would fail would cascade to the other.\nIf AIG were to fail, this would be the worst-case scenario. If we sum up its incoming arrows, we get about 3000 units of systemic risk influence. Now, if we consider Lehman Brothers, it has about 190 units of systemic risk. This type of chart makes it clear that failing to bail out AIG would risk all the big banks going bankrupt, while letting Lehman Brothers fail would impact only a small part of the market\np.s. Lehman Brothers is no more; AIG paid the bailout in full and ahead of schedule. 15\n15 Q. How do we make a model like this?\n\nExample 3 (Monty hall problem)  \n\n\n\n\nMonty Hall\n\n\n\n\nMonty Hall problem\n\n\n\n\nMonty Hall solution\n\n\n\n\nThe Monty Hall problem is named after Monty Hall, the host of the show Lets Make a Deal that aired during the 1970’s.\n\n\nSuppose you’re on a game show, and you’re given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what’s behind the doors, opens another door, say No. 3, which has a goat. He then says to you, “Do you want to pick door No. 2?” Is it to your advantage to switch your choice? – [@vos1990ask]\n\nWhen Savant wrote in her column, the player should switch. She received thousands of letters from reader, almost all disagreeing with her.\nI have added a decision tree explaining her solution although Wikipedia from which I sourced the images provides greater depth of analysis as well as alternative ways to get to the solution.\nThe main point is that without a model, this kind of problem can be notoriously hard to solve for most people, including Ph.D.\n\n\n\nComparative Statics\n\n\n\n\nSupply and demand\n\nPredictive modeling lets us evaluate statistics statistic on our data and compare outcomes of different interventions.\n\nExample 4 (shifting equilibrium in supply curve) e.g., in the graph \\(S\\) is a supply curve for some good, while \\(D1\\) and \\(D2\\) are demand curves. When demand changes from \\(D1\\) to \\(D2\\), both quantity demanded goes up and the price goes up. Economic Models help us to understand world might behave if equilibrium conditions change and we get this by making a projection in a simple model.\n\n\n\nCounterfactuals\nLet us play out different scenarios. Even though history only runs once with sufficiently robust causal models, we can test hypotheses spanning many possible worlds and conduct a what-if analysis.\n\nExample 5 e.g., in April of 2009, the Federal Government decided to implement a recovery plan. Many economists tested their models of the economy to compare unemployment rate forecasts with or without the recovery plan. Most reported an improvement.\n\nWe will never be sure if our predictions of a slowdown would have happened without the recovery plan, but if we have modeled the parts correctly, there is a good chance that the model will capture the better part of the recovery plan. Counterfactuals often lack sufficient data to yield precise outcomes, but their outcomes can help determine the general patterns for figuring out whether a policy is good or not.\n\n\nIdentify & Rank High Leverage Points\n\n\n\n\nNetwork Analysis source\n\nHigh leverage points are rows/items in a dataset that have an unusually large bearing on the outcomes of a system. Rise or fall in those values can shift the predictions of the model in a material way.\n\nExample 6 (Contagion model for economic failure of the EU) A contagion model of economic failure shows that if Great Britain’s economy fails, this will be a cascading failure triggering the Netherlands, Switzerland, Ireland, and Belgium to fail. This will cause more solid economies of Germany and Sweden to fail, and after that France’s economy will also fail.\nThis model shows how interlinked EU financial system are. It also shows that London. UK’s financial center is a High leverage point of the EU.\n\n\nExample 7 (Impact of the Carbon Cycle on Global Warming) The carbon cycle is one of the models that is used to understand climate change. Total amounts of carbon are fixed, but they can be in the air or on Earth. Down on the Earth is much better as it isn’t contributing as much to global warming. If we consider an intervention, we may first pause and wonder where in this cycle there are high leverage points. If surface radiation turns out to be the dominant coefficient, we may choose to define policy in terms of this and the other leading coefficients.\n\n\n\nExperimental design\n\nExample 8 (Auctions) Suppose we want to to auction off the bandwidth for mobile communication to raise as much money as possible. We can check which auction design works best by simulating simple models of agents bidding in a mock auction. Each agent will have their private information on how much they are willing to pay and for what and we can see which design nets us the most.\nAuction theory is perhaps the most prominent application of mechanism design, where the goal is to allocate items or rights (like public good like bandwidth, a license or a patent) to those who value them the most. Different auction formats (Sealed bid, Second Price, English auction, Dutch auction, etc.) are designed to reveal participants’ true valuations and maximize revenue or efficiency.\n\n\n\nInstitutional design\nMechanism design is a field in economics and game theory that focuses on designing economic mechanisms or institutions that achieve desired objectives, given individuals’ private information and strategic behavior. It’s essentially the reverse of traditional economic analysis: instead of predicting outcomes based on existing rules and behaviors, mechanism design starts with the desired outcomes and works backward to create laws or mechanisms that lead to them. Here’s a more detailed look at its key concepts and principles:\n\nExample 9 (Voting Systems) Voting mechanisms are designed to aggregate individual preferences into a collective decision. The challenge is to design a system that reflects the true preferences of the electorate, considering issues like strategic voting.\n\n\nExample 10 (Public Goods Provision) Mechanisms for public goods (like parks or national defense) aim to ensure they are provided at appropriate levels despite free-rider problems and individuals’ incentives to under-report their valuation of these goods.\n\n\n\n\n\nStanley Reiter diagram\n\nThe Stanley Reiter diagram, named after the economist Stanley Reiter, (one of the Doctoral advisers of Dr. Page), is a graphical tool used in mechanism design, a branch of economic theory. This diagram helps illustrate the fundamental concept of a mechanism in an economic context, particularly focusing on how private information from different agents (participants) can be aggregated and processed to make collective decisions or allocate resources efficiently.\n\nThe box with \\(\\Theta\\) represents the type space, AKA the environment, e.g., the set of technologies or people’s preferences.\n\\(X\\) is the (desired) outcome.\n\\(f(\\theta)\\) is called a social choice function, which maps a type profile to an outcome.\nIn this Game of mechanism design agents send messages \\(M\\) in a game environment \\(g\\)\nThe $ (M,g,)$ can be designed to implement some social choice function \\(f(\\theta )\\)\n\n\nComponents of the Stanley Reiter Diagram\nThe diagram typically consists of the following main components:\n\nAgents: These are the participants in the mechanism. Each agent has private information, such as preferences, valuations, or types, that is relevant to the decision-making process.\nMessage Space: For each agent, there is a message space that represents all possible messages or reports that the agent can send within the mechanism. These messages are how agents communicate their private information.\nOutcome Space: This is the set of all possible outcomes that can result from the mechanism. Outcomes could be allocations of goods, pricing decisions, policy choices, etc., depending on the context of the mechanism.\nSocial Choice Function (SCF): The SCF is a theoretical concept representing the ideal decision rule that maps the true state of the world (the agents’ private information) directly to outcomes. However, because the mechanism designer does not have direct access to the agents’ private information, the SCF cannot be implemented directly.\nMechanism \\(M\\): The mechanism itself is represented in the diagram by two main components: the message profile (a combination of messages sent by all agents) and the outcome function. The outcome function takes the message profile as input and determines an outcome based on the rules of the mechanism.\n\n\n\nHow the Diagram Works\n\nEach agent sends a message from their message space. These messages contain or represent their private information.\nAll the individual messages form a message profile.\nThe outcome function of the mechanism processes this message profile to determine an outcome from the outcome space.\n\n\n\nPurpose and Application\nThe Stanley Reiter diagram serves several purposes:\n\nVisualizing Mechanism Design: It provides a visual framework to understand how mechanisms can be designed to achieve specific objectives, such as truthfulness, efficiency, or fairness, by processing private information.\nIllustrating the Role of Incentives: The diagram helps in analyzing how different mechanisms can create incentives for agents to report their private information truthfully or behave in a manner that leads to desirable outcomes.\nDesigning and Comparing Mechanisms: Laying out the components of different mechanisms allows for a comparative analysis of their effectiveness in achieving various economic and social goals.\n\nDespite its abstract nature, the Stanley Reiter diagram is a powerful tool for understanding the process of mechanism design, highlighting the interaction between private information, strategic behavior, and collective decision-making in economic systems.\n\n\n\nTo Choose among policies\nThis is the domain of decision models. These can be simple like a classification model or complex like a recommendation system."
  },
  {
    "objectID": "w01.html#reducing-green-house-emissions",
    "href": "w01.html#reducing-green-house-emissions",
    "title": "Lesson 1 - Why Model?",
    "section": "Reducing Green House emissions",
    "text": "Reducing Green House emissions\nSay we need to choose a market for pollution permits or a carbon emission trading system. However, if you review the Wikipedia article, you will see many different options. What we can do is build a (simple) model and use it to tell which one is going to work better. This might be more objective since there are lobbying and external interests at play in such a decision."
  },
  {
    "objectID": "w01.html#reducing-green-house-emissions-1",
    "href": "w01.html#reducing-green-house-emissions-1",
    "title": "Lesson 1 - Why Model?",
    "section": "Reducing Green House emissions",
    "text": "Reducing Green House emissions\nWe need to decide about measures to reduce temperature and mitigate pollution. Green spaces is generaly an improvement. But after adding such an area people could move next to that and build houses all around it, which just leads to even greater sprawl. Perhaps planting trees along the streets is a better alternative?"
  },
  {
    "objectID": "w02.html",
    "href": "w02.html",
    "title": "Lesson 2 - Segregation and Peer Effects",
    "section": "",
    "text": "In this lesson we get started by looking\n\n\n\n\n\n\nTLDR: Schelling👍🏽, Granovettor,👍🏽 Standing Ovation 👍🏽\n\n\n\nThis is a great lesson in which we cover lots of material:\n\nSchelling’s model, in which segregation emerges from localized homophily. This simple and fertile model spawned generations of related models, such as the following.\nEmergent properties of local models and Schelling’s book “Micro Motives and Macro Behaviour”\nHow to Measure segregation.\nGranovettor’s threshold model: which explains why riots are hard to forecasts, and\nThe standing ovation model\nthe Identification Problem\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Segregation &lt;br&gt;Peer Effects &lt;br&gt; Threshold))\n    Homophily\n        (Schelling's Segregation)\n            {{phase changes}}\n              ((integration))\n              ((segregation))\n              ((nomads))\n            {{Cascades}}\n              {{Genesis}}\n              {{Exsedos}}\n            )metrics(\n              {{index of dissimilarity}}\n              {{gini index}}\n            )Mechanisms(\n              {{Sorting}}\n            {{Social Roles}}\n              {{unhappy}}\n              {{happy}}\n    Peer Effects     \n        (Granovetter collective behavior)\n            {{Effects}}\n              {{Radicalization}}\n              {{Rioting}}\n              {{Black-Swans}}\n            {{Social Roles}}\n              {{Radicals}}\n              {{Fans}}\n            )Mechanism(\n              {{Contagation}}\n        (Standing Ovation)\n          {{Social Roles}}\n              {{Celebrity}}\n              {{Academic}}\n          )Mechanism(\n              {{Percolation}}\n              {{BandWagon}}\n            \n\n\n\n\n\n\n\n\nSome questions about segregation and segregation models:\n\nHow many time steps are needed for segregating in a 50x50 city? 1\nDoes convergence time change with city dimensions? 2\nWhich segregation level can we expect from an intolerance level of 1/3? 3\nWhat happens to the convergence time if we increase the intolerance? And to the final segregation level? 4\n\n1 in @Mauro2022segregation2 in @Mauro2022segregation3 in @Mauro2022segregation4 in @Mauro2022segregation\nLegend\n\nRed is White,\nBlue is Black,\nGreen is Asian,\nOrange is Hispanic,\nGray is Other,\neach dot is 25 people\n\n\n\n\nRace & ethnicity: Huston by Erica Fischer CC-BY-SA\n\n\n\n\n\nRace & ethnicity: Chicago by Erica Fischer CC-BY-SA\n\n\n\n\n\nRace & ethnicity: New York by Erica Fischer CC-BY-SA"
  },
  {
    "objectID": "w02.html#sec-segregation_and_peer_effects",
    "href": "w02.html#sec-segregation_and_peer_effects",
    "title": "Lesson 2 - Segregation and Peer Effects",
    "section": "",
    "text": "In this lesson we get started by looking\n\n\n\n\n\n\nTLDR: Schelling👍🏽, Granovettor,👍🏽 Standing Ovation 👍🏽\n\n\n\nThis is a great lesson in which we cover lots of material:\n\nSchelling’s model, in which segregation emerges from localized homophily. This simple and fertile model spawned generations of related models, such as the following.\nEmergent properties of local models and Schelling’s book “Micro Motives and Macro Behaviour”\nHow to Measure segregation.\nGranovettor’s threshold model: which explains why riots are hard to forecasts, and\nThe standing ovation model\nthe Identification Problem\n\n\n\n\n\n\n\n\n\nmindmap\n  root((Segregation &lt;br&gt;Peer Effects &lt;br&gt; Threshold))\n    Homophily\n        (Schelling's Segregation)\n            {{phase changes}}\n              ((integration))\n              ((segregation))\n              ((nomads))\n            {{Cascades}}\n              {{Genesis}}\n              {{Exsedos}}\n            )metrics(\n              {{index of dissimilarity}}\n              {{gini index}}\n            )Mechanisms(\n              {{Sorting}}\n            {{Social Roles}}\n              {{unhappy}}\n              {{happy}}\n    Peer Effects     \n        (Granovetter collective behavior)\n            {{Effects}}\n              {{Radicalization}}\n              {{Rioting}}\n              {{Black-Swans}}\n            {{Social Roles}}\n              {{Radicals}}\n              {{Fans}}\n            )Mechanism(\n              {{Contagation}}\n        (Standing Ovation)\n          {{Social Roles}}\n              {{Celebrity}}\n              {{Academic}}\n          )Mechanism(\n              {{Percolation}}\n              {{BandWagon}}\n            \n\n\n\n\n\n\n\n\nSome questions about segregation and segregation models:\n\nHow many time steps are needed for segregating in a 50x50 city? 1\nDoes convergence time change with city dimensions? 2\nWhich segregation level can we expect from an intolerance level of 1/3? 3\nWhat happens to the convergence time if we increase the intolerance? And to the final segregation level? 4\n\n1 in @Mauro2022segregation2 in @Mauro2022segregation3 in @Mauro2022segregation4 in @Mauro2022segregation\nLegend\n\nRed is White,\nBlue is Black,\nGreen is Asian,\nOrange is Hispanic,\nGray is Other,\neach dot is 25 people\n\n\n\n\nRace & ethnicity: Huston by Erica Fischer CC-BY-SA\n\n\n\n\n\nRace & ethnicity: Chicago by Erica Fischer CC-BY-SA\n\n\n\n\n\nRace & ethnicity: New York by Erica Fischer CC-BY-SA"
  },
  {
    "objectID": "w02.html#sorting_and_peer_effects",
    "href": "w02.html#sorting_and_peer_effects",
    "title": "Lesson 2 - Segregation and Peer Effects",
    "section": "Sorting and peer effects introduction",
    "text": "Sorting and peer effects introduction\nWhy do individuals who study, reside, or spend time together often exhibit similarities in appearance, thought processes, and behaviors?\nAny number of mechanisms may lead to the creation of homogeneous groups. We will study primarily the first two of the following mechanisms:\n\nSorting effect\n\npreferential exodus into neighborhoods with similar neighbors increases homogeneity.\n\nPeer effect\n\nSocial pressure from peers causes people to conform or to leave, increasing homogeneity.\n\nBandwagon effect:\n\na follow-the-leader effect first noticed in politics. c.f. bandwagon effect** in the many context innovation, marketing, economics ,\n\n\n\n\n\n\nwhen the underdog is just a kitten\n\n\nUnder Dog effect\n\nSimilar to a bandwagon effect but where the group arises around a weaker candidate and is sometimes observed in politics,\n\n\n\n\n\n\nIn contagion, we can see several mechanisms for spreading of illness and romours\n\n\nCritical-mass/density effects\n\nIn contagion model epidemiology. We see that a critical mass or density is enough for a seed of sickness to spread to most of the population.\n\n\n\n\n\n\nAdverse selection can create a market for lemons\n\n\nAdverse Selection\n\nWhen most sellers lack incentives to participate in a market, we end up with a homogenous market full of inferior products. This was investigated in the seminal paper [@Akerlof1970Lemons] called the “Market for Lemons,” where the author discusses adverse selection in economics\n\n\nIf you look at the map of Detroit, you see that people of different ethnicity tend to live together in segregated communities. Birds of a feather flock together, and people who look like each other tend to live together. Since people can not change their skin color, the mechanism for this is Sorting.\nAlternatively, people could adapt their behavior to match that of others around them. e.g., kids who hang around with kids that are smoking may take an occasional cigarette, and start smoking in this way. Alternatively, if you hang out with people who are serious about their health and don’t smoke, then you might decide to quit smoking. These are examples of peer effects.\nBoth sorting and peer effects create uniformity within groups. Models have been developed to better understand these mechanisms. Their results have revealed some unexpected insights. In many cases where uniformity is observed, the mechanism that creates the effect isn’t clear. We can’t tell if we are dealing with sorting, peer effects, a combination, or something else. In other words, a model is not an alternative to research. The models we will consider include:\n\nSchelling’s segregation model tries to explain how segregation is driven by sorting.\nGranovettor’s Threshold model, which is centered around how a minority’s willingness to participate in some collective behavior can define the behavior of the whole group.\nThe standing ovation model explains how uniformity arises through a peer effect.\nHow we can distinguish between sorting and peer effects is called the identification problem.\n\nThere are different types of models:\n\nFormal Model These are primarily mathematical and logical specifications. The idea behind formalism in mathematics is that most theories, when specified with great precision, lead to greater insights and elimination of paradoxical results. However, a formal model is usually the result of a deep analysis of informal models.\nGame theoretical Models are models involving agents that rationally, following a detailed strategy which maximizes their expected rewards. In this sense their actions can be called optimal. However many real world optimization problems defy an optimal solution.\nAgent based models use agents to represent individuals, organizations or even countries. The agents follow simple rules that govern their behavior and are implemented using framework like NetLogo or Mesa.\n\nABMs are easy to code, particularly when we know an ABM framework. ABMs can be iteratively refined or simplified. ABMs can be made interactive - which facilitates quick exploratory analysis. ABMs are amenable to automated analysis. Which might reveal optimal strategies that let us proceed to a Game Theoretical model Alternatively, we might use them to formalize the model by deriving some equations that describe the emergent macroscopic behavior. The agents may also be given the capacity to learn more optimal behavior using RL (reinforcement learning). ABM may be tested for their capacity to match real-world phenomena. ABM models that are a good fit can also be calibrated to match the behavior in observed data more closely.\nFor example, there are many Formal Contagion models that use PDEs to model epidemics. While SIRX models can just as easily handle billions of patients as thousands. However, they cannot be readily queried to check the effectiveness of different mitigation methods for a disease. ABMs, Although limited to 100,000 agents, were used to explore mitigations in various scenarios like nursing homes, primary schools, secondary schools, and even prisons. They were also modified to match specific cases and calibrated using a few real test cases.\nThus, ABMs can give us a high value for our effort and fit into a rapid response."
  },
  {
    "objectID": "w02.html#sec-schellings_segregation_model",
    "href": "w02.html#sec-schellings_segregation_model",
    "title": "Lesson 2 - Segregation and Peer Effects",
    "section": "Schelling’s Segregation Model",
    "text": "Schelling’s Segregation Model\n\n\n\n\n\nSchelling’s Book Micro Motives & Macro Behavior\n\n\n\n\n\nNew York - coded ethnically\n\n\n\nRed is White,\nBlue is Black,\nGreen is Asian,\nOrange is Hispanic,\nYellow is Other, each dot is 25 residents.\n\n\n\n\nNew York - economically coded\n\n\nAn examination of New York’s demographic distribution, represented on a color-coded map where red dots signify upper-class agents, blue dots denote lower-class agents, and moderately blue dots indicate middle-class agents, reveals that the city is ethnically and economically segregated.\n\n\n\nThe plot of ‘Far From Heaven’ touches on peer-effects leading to segregation\n\n\n\n\nSchelling board\n\n\n\n\nSchelling Netlogo - start\n\n\n\n\nSchelling Netlogo end\n\n\n\n\n\nIn research papers [@schelling1969models], [@schelling1971dynamic], and later in [@schelling1978micromotives], his famous yet erudite opus Nobel Laureate in Economist and noted game theorist Thomas Schelling outlines how he applied his unique brand of game theory to explore how ethnic and income-based segregation arises in a city. Schelling worked out his model by hand using a game board for a city and some pennies for his agents. He soon discovered that encoding homophily as an incentive for agents to relocate rapidly segregated his city.\nToday we can simulate using Agent Based Modeling (ABM) packages like Netlogo or Mesa to implement Schelling’s model. with different thresholds.\n\n\n\nTable 1: outcomes for different thresholds\n\n\n\n\n\n\n\n\n\n\n\nthreshold\ninitial unhappiness\nfinal unhappiness\nfinal similarity\n\n\n\n\n15%\n\n\n\n\n\n40%\n28%\n0%\n80%\n\n\n52%\n54%\n0%\n94%\n\n\n80%\n90%\n–\n50%\n\n\n\n\n\n\n\n\nwe can see that a slight preference of homogeneity leads to almost full segregation in this model. From watching the simulation, we can see that most cascades are concentrated at the boundary of segregated zones and that unhappiness will often be minimized by reducing the boundary to a single line from one edge of the city to another.\n\nInsights\nThe insights of simulating Schelling’s model are:\n\nSegregation and other emergent macro-level effects may not reflect the behavior and preferences of individuals at the micro level. We see that individuals who are happy so long as they live with just a minimal proportion of their ethnic group within their vicinity will ultimately find themselves in a state of pronounced segregation.\nSegregation actually breaks down if intolerance gets too high enough.\n\nIn this course, we will return to the idea of tipping points. But lacking a clear definition makes the discussions rather pedestrian at times.\nI like to think of a tipping point as a critical point in systems dynamics at which there is a discontinuity in macroscopic behavior. Phase changes are best understood in physical systems, where we often find that they have higher-dimensional geometry since the phase change depends on multiple factors.\nWhat Scott calls “a tip” is more straightforward to understand if we call it “a cascade,” which I define as “a chain reaction or sequence of events arising from a single cause.” Not all models exhibit cascades, but when they do, they are a significant part of how the micro is transformed into the macro.\nUnfortunately, the simulation I looked at in NetLogo and Mesa doesn’t show how to keep track of cascades and visualize them in real time. The tipping points in this model are the thresholds described above at which we will see large-scale segregation emerge, and the second one is where it stops, and we see permanent churn emerge. What are described below are cascades.\nSchelling’s model has a cascade phenomenon. For example, if one person moves, this may trigger another person to move, and so on.\n\nThere is an Exodus cascade, which means that a person will leave if a similar person in the neighborhood leaves. The trigger for this cascade for \\(X\\) is due to a like-me neighbor leaving and thus decrementing the numerator of the ‘threshold’ equation from \\(N/D\\), which is still happy, to \\((N-1)/D\\), which is no longer happy, thus triggering \\(X\\)’s move. These exodus cascades will start with the most intolerant neighbor of \\(X\\), but others with lower thresholds may follow dropping.\nThere is also a Genesis cascade, in which a person at \\(X\\) could become unhappy if a “different” type of person moves into one of the vacant apartments in his neighborhood, \\(Y\\). This will happen if \\(X\\)’s threshold is triggered by a denominator increase, i.e., \\(X\\) was happy at \\(N/D &lt; T\\), but at \\(N/(D+1) &gt;= T\\), \\(X\\) is no longer happy. Genesis events may cascade if more individuals like \\(Y\\) move into the mixed neighborhood, occupying additional vacant apartments nearby. They can trigger an exodus cascade, creating more vacant lots and clearing the way for further genesis.\n\nIn most major urban centers with individuals of diverse ethnic backgrounds, there exists a pronounced segregation along racial lines despite the prevalent desire among the populace to reside in ethnically mixed communities.\nNote:\n\nThe threshold is held fixed in this model, but in reality, it is a distribution.\nThe rich are a minority & the poor are a majority in most cities.\nThe model is biased in the sense that an agent only get unhappy due to lack of self same neighbors. We can just as easily let them get unhappy if there is no diversity by adding a threshold for that. This should allow people to move nearer to the segregated zone or perhaps into an integrated area. This might make the boundaries expand or even setup zones of higher and lower homogeneity.\nif people with lower thresholds decide to move towards the boundary from both sides. If we gave houses a price element that reflected the supply and demand for different preferences, we could see"
  },
  {
    "objectID": "w02.html#sec-measuring_segregation",
    "href": "w02.html#sec-measuring_segregation",
    "title": "Lesson 2 - Segregation and Peer Effects",
    "section": "Measuring Segregation",
    "text": "Measuring Segregation\n\n\n\n\nMeasuring segregation\n\nIn this lesson, we set our sights on the Index of Dissimilarity as a measure of segregation between two demographic groups living in the same metropolitan region. Its advantage is it’s easy to calculate. Its main shortcoming is that it cannot handle multiple groups, nor can it deal with a continuous variable like wealth. In this section, we consider wealth a dichotomous variable: people are either rich or poor.\nWhat I wanted from this section was to learn the logic for creating such statistics/metrics by myself. I guess that this is not so easy as it seems. However there are a number of other approaches for evaluating segregation and wealth in equality.\nMetrics follows\nThe metric we developed for evaluating segregation called The index of dissimilarity was introduced in [@duncan1955methodological].\nLet’s posit a scenario in which a mix of upper and lower class agent live in 24 residential tracts, each inhabited by 10 individuals. Within this configuration, 12 luxury tracts are priced exclusively for rich individuals, 6 tracts of public residential facilities cater only the poor while the remaining 6 tracts share a demographic mix of rich and poor in equal proportion.\nThe aggregate count of the upper class is \\[\n12\\times 10 + 6\\times 5 = 150\n\\]\nWhereas the total number of the lower class amounts to \\[\n6\\times 10 + 6\\times 5 = 90\n\\]\nLet\n\n\\(b\\) be the quantity of a particular group within a tract,\n\\(B\\) be the aggregate quantity of that group (150),\n\\(y\\) be the quantity of the contrasting group within that tract,\n\\(Y\\) be the total number of the contrasting group (90),\n\nthen the difference between the proportions \\(\\frac{b}{B}\\) and \\(\\frac{y}{Y}\\), or\n\\[\n\\left | \\frac{b}{B} - \\frac{y}{Y} \\right | \\qquad\n\\tag{1}\\]\nServes as an indicator of the extent to which the distribution in the tract deviates from a state of diversity.\nUtilizing this metric, a perfectly homogenous tract would possess\n\\(|\\frac{b}{B} - \\frac{y}{Y}| = \\frac{5}{150} - \\frac{3}{90} | = 0.\\)\nFor the blue tracts: \\[ \\left |\\frac{b}{B} - \\frac{y}{Y} \\right | = \\left | \\frac{10}{150} - \\frac{0}{90} \\right | = \\frac{1}{15}\\] For the yellow tracts: \\[\\left | \\frac{b}{B} - \\frac{y}{Y} \\right | = \\left | \\frac{0}{150} - \\frac{10}{90} \\right | = \\frac{1}{9}\\]\nFor the green tracts: \\[ \\left |\\frac{b}{B} - \\frac{y}{Y} \\right | = \\left | \\frac{5}{150} - \\frac{5}{90} \\right | = \\frac{1}{45}\\]\n\nThe Index of Dissimilarity\n\\[\n(\\frac{1}{2}\\sum |\\frac{b}{B} - \\frac{y}{Y}|) \\qquad\n\\tag{2}\\]\nWe sum the individual differences and then divide them by 2 to keep the range of the index between 0 and 1.\n\n0 indicates heterogeneity.\n1 indicates homogeneity.\n\nFor the example above, you have\n\\[\\frac{1}{2} \\left (\\frac{12}{15} + \\frac{6}{9} + \\frac{6}{45} \\right ) =\\frac{72}{90} = 0.8\\]\nWhich is highly segregated.\n\n\nA Generalized Index of Dissimilarity\n\\[\nD_G = \\frac{1}{2} \\frac{\\sum_j \\sum_i \\left | N_{ij}-E_{ij}\\right |}{\\sum NP_j(1-P_j)} \\qquad\n\\tag{3}\\]\nWhere:\n\n\\(E_{ij} = \\frac{N_i N_j}{N}\\)\n\\(P_j = \\frac{N_j}{N}\\)\n\n\n\nMoran’s I\nMoran’s I is a measure of spatial autocorrelation. The statistic for a spatial variable \\(x_h\\) over the occupied cells follows Equation 4: \\[\n\\text{Moran's I} = \\frac{M\\sum_i\\sum_jw_{ij}(x_i-\\bar x)(x_j-\\bar x)}{\\sum_i\\sum_jw_{ij}\\sum_i(x_i-\\bar x)^2 \\qquad}\n\\tag{4}\\]\nWhere:\n\n\\(x_h\\) is defined as \\(x_h = 0\\) if \\(h\\) is occupied by red agents and \\(x_h = 1\\) if \\(h\\) is occupied by yellow agents.\n\\(x_i, x_j\\) denote the values of \\(x_h\\) in cells i, j;\n\\(M = Round((1-d)\\times N^2)\\) is the overall number of occupied cells,\n\\(\\bar x\\) is the mean value of \\(x_h\\) over the occupied cells,\n\\(w_{ij} = \\begin{cases} 1 & \\text{if } j \\in U(i) \\\\ 0 & \\text{otherwise}\\end{cases}\\)\n\nInterpretation of the statistic\n\nA value of Moran’s I close to 0 represents an integrated region.\nA value close to 1 represents a fully segregated region.\n\n\n\nInteraction or Exposure Index (B)\nThe Interaction Index, also known as the Exposure Index, is a measure of potential contact or interaction between different groups in a geographic area. It measures the probability that a member of one group will come into contact with a member of another group.\nThe formula for the Interaction Index (B) between group X and group Y is:\n\\[\nB(X, Y) = \\sum_{i=1}^{n} \\frac{x_i}{X} \\frac{y_i}{T_i} \\qquad\n\\tag{5}\\]\nWhere:\n\n\\(n\\) is the total number of tracts.\n\\(x_i\\) is the population of group \\(X\\) in tract $\\(i\\).\n\\(X\\) is the total population of group \\(X\\) in all tracts.\n\\(y_i\\) is the population of group Y in tracts \\(i\\).\n\\(T_i\\) is the total population in area \\(i\\).\nThe sum is taken over all areas \\(i\\).\n\nInterpretation:\n\nThe Interaction Index has values ranging from 0 to 1.\nA value of 0 indicates complete segregation.\nA value of 1 indicates complete integration.\n\n\n\nSegregation in Multiple Categories: Entropy Index\nBoth the Index of Dissimilarity and Interaction Index can only measure the segregation of two groups compared to each other.\nThe Entropy Index is a measure of diversity that measures the spatial distribution of multiple groups simultaneously. It may be used to quantify segregation in a population across multiple groups. It is calculated as follows:\n\\[\n\\text{{Entropy Index}} = -\\sum_{i=1}^{n} p_{ij} \\log(p_{ij}) \\qquad\n\\tag{6}\\]\nIn this formula:\n\n\\(n\\) is the total number of groups.\n\\(p_{ij}= \\frac{n_{ij}}{n_i}\\) is the ratio of group \\(i\\) in cell \\(j\\)\n\\(n_{ij}\\) = number of population of \\(j\\)th ethnicity in cell \\(i\\)\n\\(n_{i}\\) =total population in cell \\(i\\)\nThe sum is taken over all groups \\(i\\) in each cell \\(j\\).\nThe logarithm is typically base 2 or base e (natural logarithm), depending on the context.\n\nInterpretation: - The Entropy Index ranges from 0 (no diversity, all members belong to the same group) to log(n) (maximum diversity, members are evenly distributed across all groups). - In the context of segregation, a lower Entropy Index indicates higher segregation (less diversity within areas), while a higher Entropy Index indicates lower segregation (more diversity within areas)."
  },
  {
    "objectID": "w02.html#segregation-in-the-wild.",
    "href": "w02.html#segregation-in-the-wild.",
    "title": "Lesson 2 - Segregation and Peer Effects",
    "section": "Segregation in the Wild.",
    "text": "Segregation in the Wild.\nThe following table based on data from Most to Least Segregated Cities in 2020 According to 2020 Census Data shows segregation levels in major US cities\n\nimport pandas as pd\nfrom itables import show\ndf = pd.read_csv(\"data/seg_high_place.csv\")\ndf['City'] = df['City'].str.slice(0,20)\nshow(df,maxBytes = 200 * 1024)\n\nSegregation in US cities with a population greater than 20k based on\nthe 2020 census {#5dd25ad3}\n\n    \n      \n      Rank\n      City\n      Divergence\n      Segregation Category\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nOne more point to consider is, given all the simplifying assumptions, how useful is the scheduling model for modeling reality?\nIn real life, residential real estate is usually bought through careful deliberation, and the key factors are price per square meter and the largest loan one can get. This seems to be a far cry from the Schelling model. This point is considered in great detail in [@Hatna2012Schelling], which looks at segregation in two mixed cities in Israel."
  },
  {
    "objectID": "w02.html#peer_effects",
    "href": "w02.html#peer_effects",
    "title": "Lesson 2 - Segregation and Peer Effects",
    "section": "Peer Effects",
    "text": "Peer Effects\n\n\n\n\npeer pressure"
  },
  {
    "objectID": "w02.html#sec-granovettors-model",
    "href": "w02.html#sec-granovettors-model",
    "title": "Lesson 2 - Segregation and Peer Effects",
    "section": "Granovettor’s Threshold Model",
    "text": "Granovettor’s Threshold Model\n\n\n\n\nGranovettor’s model can explain riots\n\nPeer effects can have a contagious mechanism a kin to the spread of diseases. Another aspect of peer effects is called “the tail is wagging the dog,” where a small minority sets the tone for the whole group. Situations where behavior in which the most extremist people at the tail end of the political distribution determine what will happen will always be challenging to predict, for instance, political uprisings like the fall of the Berlin Wall and more recently the Orange Revolution in Ukraine. In such cases, it is difficult to predict what will happen, and the Granovetter Model explains that it is a microcosm in which we can study why it is so challenging to predict such events.\nIn [@granovetter1983threshold; @granovetter1986threshold; @granovetter1988threshold] the authors discusses Granovettor’s Threshold Model which can be described by:\n\n\\(N\\) population size.\nAgent j has a participation threshold \\(T_j\\) and will only participate if \\(T_j\\) others individuals participate.\nAn Agent with a threshold of 0 will always participate.\nAn Agent with a threshold of 10 will wait till it sees 10 other agents participating.\n\nThus, the outcome varies depending on the distribution of thresholds.\nAssume that there are five individuals and that the behavior is participating in a riot.\n\n\n\nTable 2: ready for a riots\n\n\n\n\n\n# agents\nthreshold\n\n\n\n\n1\n0\n\n\n1\n1\n\n\n3\n2\n\n\n\n\n\n\nThen, we can expect that the first agent will start protesting because her threshold is 0. Then second the agent will join the protest since his threshold is 1. The other thee will now join the protest as their threshold is 2.\nWith\n\n\n\nTable 3: no one riots\n\n\n\n\n\n# agents\nthreshold\n\n\n\n\n3\n1\n\n\n2\n2\n\n\n\n\n\n\nWe can expect no one will riot even though the collective has a much lower threshold for participation.\nWith\n\n\n\nTable 4: another riots\n\n\n\n\n\n# agents\nthreshold\n\n\n\n\n1\n0\n\n\n1\n1\n\n\n1\n2\n\n\n1\n3\n\n\n1\n4\n\n\n1\n5\n\n\n\n\n\n\nEvery agent will, in turn, join the protest even though the collective isn’t as polarized as the previous one. We can see this is a case where the extremists set the tone.\nWe can conclude that in this model of collective action, participation is more likely when :\n\nThe thresholds are lower\nThere is a greater variation of thresholds.\n\nThe second point is why it is so hard to predict if a riot or a run on a bank will take place. First, you need to estimate the level of discontentment and also the distribution of thresholds."
  },
  {
    "objectID": "w02.html#sec-standing_ovation_model",
    "href": "w02.html#sec-standing_ovation_model",
    "title": "Lesson 2 - Segregation and Peer Effects",
    "section": "The Standing Ovation Model",
    "text": "The Standing Ovation Model\n\n\n\n\nStanding ovation\n\n\n\n\nStanding ovation\n\n\nGranovettor’s Threshold model can explain participation in risky activities where risks and rewards are tied to the number of peers participating.\n\nThe basic SOP can be stated as follows: A brilliant economics lecture ends, and the audience begins to applaud. The applause builds, and tentatively, a few audience members may or may not decide to stand. Does a standing ovation ensue, or does the enthusiasm fizzle? — [@miller2004standing]\n\nThe Formal Model:\n\nAgents are seated in a rectangular auditorium with \\(R\\) rows and \\(C\\) seats per row.\nAt the conclusion of a performance, each agent makes an evaluation of the performance’s quality.\nLet \\(q_{ij} ∈ Q_{ij} = [0, 1]\\) represent the quality signal received by the audience member seated in the \\(i\\)th row and \\(j\\)th seat.\n\nHigher values of \\(q_{ij}\\) represent greater perceived quality.\nFor the moment, values may be thought of as either private or common with idiosyncratic noise, or some convex combination of the two.\n\nEach audience member possesses an exogenous threshold level in addition to his or her quality evaluation.\n\nThe threshold level for the agent in the \\(i\\)th row and \\(j\\)th seat, \\(T_{ij}\\) , equals the minimal quality required for that agent to stand immediately.\nThus, if an audience member’s private value weakly exceeds the threshold, that is if \\(q_{ij} \\ge T_{ij}\\) , she stands immediately. If not, she remains seated.\n\nLet \\(s^t_{ij} \\in \\{0, 1\\}\\) denote whether or not the audience member is standing \\((s^t_{ij}=1)\\) or sitting \\((s^t_{ij}=0)\\) \\(t\\) time periods after the completion of the performance, and let \\(S^t\\) be the total number of audience members standing at time \\(t\\). Therefore,\n\n\\[\nS^0  = \\sum ^C_{j=1}\\sum ^R_{i=1} s^0_{ij}\n\\]\nequals the number of agents standing immediately. In interesting cases, only a fraction \\((0 &lt; S 0 &lt; R \\cdot C)\\) of the audience stands immediately. With a portion of the audience standing, those who remain seated must decide whether to stand, and those audience members standing must decide whether to remain standing or to sit. Both decisions rely on local information—possibly the number of neighbors standing or the percentage of audience members within sight who are standing—as well as the initial quality appraisal of the individual.\nFor example, someone seated surrounded entirely by people who are standing most likely will stand, unless she abhorred the performance. Similarly, unless she felt that the performance was stupendous, an isolated standing person may decide to sit if her neighbors do not join the standing ovation quickly.\nThe behavior of an audience member at a particular point in time can be represented by a heuristic that maps her information and quality appraisal into an action, either sit or stand. Periods are considered as discrete units. The continuous time case shall be addressed later in the discussion of random asynchronous and endogenous asynchronous updating. Let \\(K_{ij}\\) be the seat assignments visible to the audience member in the ith row and jth seat. Define a behavioral rule at time \\(t &gt; 0 \\text{ by } F^t : K^t × Q_{ij} \\to \\{0, 1\\} \\text{ for } t \\ge 1\\). Recall that in period 0, the decision to stand depends only on the agent’s threshold and \\(q_{ij}\\).\n\nThe authors discuss the math and internals of their models as form of noisy channel. Agents need to fuse a signal based on their private information with the information coming from their peers and arrive at a binary decision.\nThe standing ovation model by Scott and Miller is intentionally presented in the most vague form according to their later publication. This is rather annoying as without a more formal spec it isn’t possible to reproduce any results. Also most people don’t actually seem to implement it as explained in the lesson or the paper.\nI found the paper [@miller2004standing] rather tiresome and had to have a few goes at it. The authors’ exhortation exaltation on the grandeur of their model is matched by the vagueness of its definitions — basically there are any number of variants and extensions of the standing ovation models. Criticism aside I noticed some point of interest in the paper:\n\ncascades I was a momentarily shocked to see many references to information cascades in the paper, which I mentioned above. It is evident the authors are well versed on this subject well begging the question, why it this not covered in the course but rather named tips etc.\nANTs (Active Nonlinear Tests) are another interesting idea - i.e. using them to break the model. This seems to be a from of fuzz testing. At this point though I was still more interested in demonstrating that the model actually works.\nEventually, I noticed there was a section called the formal model which defined the model more precisely.\nThere is also a section on implementing an ABM version with a number of variations they had considered which I summarized below, indicating if I have implemented them.\n\nRather than go into great detail about the model, I coded it using Mesa. Coding this model required some customization of the framework and that makes more advanced but also more educational. Also I can say that the lesson explained the model rather poor and that only by implementing the model did I get to understand this model.\n\nAgents are initialized as either standing or not, as with no one is standing at the start the will be no standing ovation.\nAt each time step, agents decide to stand based on a Signal &gt; Threshold where the signal is based on the Quality and an error. \\(S=Q+\\epsilon &gt; T\\)\n\nSome variations they discuss are\n\nupdating: 5\n\nSynchronous Updating At the start of each discrete time period, all agents update in unison.\nAsynchronous-Random Updating Within each discrete time period, the agents are permuted into a random order and updated in that order.\nAsynchronous-Incentive-Based Updating Within each discrete time period, the agents update one at a time based on an explicit ordering rule that has agents who are least like the people that surround them move first\n\nThe following neighborhoods are suggested 6\n\nFive Neighbors (left, right, front left, front, front right)\nCones add to the above two more rows in front, each growing by one on each side.\n\nDo agents sit if they are all alone?\nTopology is the world\n\na square or\na torus to avoid edge effects.\na cylinder (square with left and tight wrapping)\n\nDifferent geometries\nWeighting 7\n\nof left right neighbors\nof rows in the funnel by inverse square distance (row_2: 1/4 row_3: 1/9)\n\nCan we code partners and group agents. 8\nHecklers & Stooges - these never or always stand. 9\n\nCan we estimate how many are needed to ensure/prevent a SO?\nCan we add a seat price for the above 2 at limit 10 im middle?\n\nWhat if every so often an agent “turns round” to view the seats behind them? This is the same as Moore neighborhood 10\ntrack information cascades.\n\neach agent should have list called cascade - if standing it is in it otherwise empty.\neach agent should keep track of the sim_step when it stands\nwhen triggered the agent copies the cascade list of any agent that stood up in the previous time step.\nit should grab the cascade id list from his the peer that triggered him store it in locally.\n\ngenerate an animation\n\nof the state using the image of each step\nanimate agents who by blinking their weighted neighborhood.\nanimate information cascades - cycle colors from white to black of recently changed agents.\nwe also annotate each location using the length of its cascade\n\n\ndo a run to test the paper’s suggested metrics\n\n5 @miller2004standing6 @miller2004standing7 @Cole2020SOM8 @Cole2020SOM9 @Cole2020SOM10 @Cole2020SOMThe model has the following elements.\n\n\\(Q \\in [0,100]\\) is the quality of the show\n\\(\\epsilon\\) is an error or diversity\n\\(S=Q+\\epsilon\\) is the signal\n\\(T\\) is the personal threshold\nTrigger\n\ninitially \\(Q&gt;T\\)\nlater \\(S&gt;T\\)\n\n\nThree metrics:\n\n\n\n\nimport mesa\n\nclass AudienceMember(mesa.Agent):\n    def __init__(self, unique_id, model, threshold=0.5):\n        super().__init__(unique_id, model)\n        self.standing = False\n        self.threshold = threshold  # The proportion of neighbors standing that will trigger this agent to stand\n\n\n    def step(self):\n        # Implement the logic for deciding whether to stand or not\n        neighbors = self.model.grid.get_neighbors(self.pos, moore=True, include_center=False)\n        if not self.standing:\n            standing_neighbors = sum(1 for neighbor in neighbors if neighbor.standing)\n            if standing_neighbors / len(neighbors) &gt;= self.threshold:\n                self.standing = True\n\n\nclass StandingOvationModel(mesa.Model):\n    \"\"\"\n    Model class for the Standing Ovation Model.\n    \"\"\"\n\n    def __init__(self, width, height, initial_standing=0.1, threshold=0.5,seed=None):\n\n        \"\"\"\n        Create a new Standing Ovation Model.\n\n        Args:\n\n        \"\"\"\n        super().__init__(seed=seed)\n        self.grid = mesa.space.SingleGrid(width, height, torus=False)\n        self.schedule = mesa.time.RandomActivation(self)\n        \n        self.datacollector = mesa.DataCollector(\n                model_reporters={\"Standing\": lambda m: sum([1 for a in m.schedule.agents if a.standing])},\n                agent_reporters={\n                   \"x\": lambda a: a.pos[0],\n                    \"y\": lambda a: a.pos[1],\n                    \"standing\": lambda a: a.standing,\n                })\n\n        # Create agents\n        for i in range(width*height):\n            \n            agent = AudienceMember(i, self, threshold=self.random.uniform(0.1,.7))\n            self.schedule.add(agent)\n\n            # Place agent\n            x = i % width  \n            y = i // width  \n            self.grid.place_agent(agent, (x, y))\n\n            # Randomly decide if the agent is standing at the beginning\n            if self.random.random() &lt; initial_standing:\n                agent.standing = True\n\n\n    def step(self):\n        \"\"\"\n        Run one step of the model.\n        \"\"\"\n        self.schedule.step()\n        self.datacollector.collect(self)\n\nSome code to visualize the state\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MultipleLocator, FormatStrFormatter\n\nimport numpy as np\n\ndef plot_grid(model,model_name,state_name):\n    \"\"\"\n    Plot the current state of the grid with a graph paper-like background.\n    \"\"\"\n    grid = np.zeros((model.grid.width, model.grid.height))\n    for a ,(x,y) in model.grid.coord_iter():        \n        if a is not None:\n            grid[x][y] = 1 if a.standing else 0\n\n    fig, ax = plt.subplots()\n    ax.imshow(grid, cmap='Greys', interpolation='nearest', extent=[0, model.grid.width, 0, model.grid.height])\n\n    # Set up the minor grid lines\n    ax.set_xticks(np.arange(0, model.grid.width, 1), minor=True)\n    ax.set_yticks(np.arange(0, model.grid.height, 1), minor=True)\n\n    # Set up the major grid lines (every fifth line)\n    ax.set_xticks(np.arange(0, model.grid.width, 5), minor=False)\n    ax.set_yticks(np.arange(0, model.grid.height, 5), minor=False)\n\n    # Customize the appearance of the grid lines\n    ax.grid(which=\"minor\", color=\"blue\", linestyle='-', linewidth=0.5)\n    ax.grid(which=\"major\", color=\"red\", linestyle='-', linewidth=2)\n\n    # Hide the major tick labels\n    ax.tick_params(which=\"major\", labelbottom=True, labelleft=True)\n\n    plt.title(f\"{model_name} - {state_name}\")\n    plt.show()\n\n\n# Create the model\nso_model = StandingOvationModel(20, 20, initial_standing=0.1, threshold=0.5)\n\n/home/oren/.local/lib/python3.10/site-packages/mesa/time.py:82: FutureWarning:\n\nThe AgentSet is experimental. It may be changed or removed in any and all future releases, including patch releases.\nWe would love to hear what you think about this new feature. If you have any thoughts, share them with us here: https://github.com/projectmesa/mesa/discussions/1919\n\n\n\nplot the initial state\n\nmodel_name = \"Standing Ovation\"\nstate_name = \"Start State\"\nplot_grid(so_model,model_name,state_name)\n\n\n\n\n\n\n\nFigure 1: Standing Ovation Model - Initial Grid State\n\n\n\n\n\nSimulate the standing ovation model for 15 steps.\n\n# Run the model for a certain number of steps\nfor i in range(15):\n    so_model.step()\n\nPlot the final state.\n\nmodel_name = \"Standing Ovation\"\nstate_name = \"Final State\"\nplot_grid(so_model,model_name,state_name)\n\n\n\n\n\n\n\nFigure 2: Standing Ovation Model - Final Grid State\n\n\n\n\n\nLet’s look at the growth of the ovation.\n\nso_model_df = so_model.datacollector.get_model_vars_dataframe()\nso_model_df.head()\n#so_model_df.Standing.plot()\n\n\n\n\n\n\n\n\n\n\nStanding\n\n\n\n\n0\n69\n\n\n1\n89\n\n\n2\n106\n\n\n3\n123\n\n\n4\n129\n\n\n\n\n\n\n\nFigure 3: Standing Ovation Model - Growth Analysis\n\n\n\n\nThe following results come from this model.\n\nHigher \\(Q\\) will result in bigger ovations.\nLower \\(T\\) will result in bigger ovations.\nA lower percentage of standing \\(X\\) will cause more standing ovations. If \\(X\\) is large, an agent won’t stand up despite many people standing up.\nIf \\(X\\) is small, people will jump on any bandwagon.\nFor \\(Q &lt; T\\), greater variation in \\(\\epsilon\\) will help the standing ovation spread by increasing a contagion-like effect - as we saw in the previous model.\n\n\n\n\n\nStanding ovation - Celebrity view\n\nStanding ovation is more likely to emerge when:\n\nIf Quality is higher.\nThresholds are lower,\nThere is a more substantial peer effect\nThere is a greater variation in thresholds or errors.\n\nThe first three are obvious. Scott suggested that the last one was discovered by implementing his standing ovation model. Once we know about the Granovettor model’s requirement for agents to have different thresholds to enable participation to spread, it is obvious.\nThe model simplifies reality by excluding factors like the shape of the theater and the locations where people are situated or by assuming people act independently when most people come in pairs or larger groups.\nScott later discusses social roles, which he calls celebrities and academics, that would emerge in a more sophisticated model with a spatial layout with a front and a back. In the first chapter of Micro Motives and Macro Behavior, I later noticed that Schelling analyzed a surprising sitting arrangement that emerged in one of his talks, eliminating many hypotheses.\n\n\n\n\nStanding ovation - Academic’s view\n\nAgents in the front may be assigned a “celebrity” role; they can influence everybody behind them but are not influenced by others. Agents in the back may be assigned an “academic” role. They can see their immediate neighborhood but are unseen by many others.\nScott claims that the standing ovation model is helpful in other scenarios, which is termed fertility for models. The standing ovation model may be useful in modeling collective action problems. However, he doesn’t make a very strong case."
  },
  {
    "objectID": "w02.html#sec-identification_problem",
    "href": "w02.html#sec-identification_problem",
    "title": "Lesson 2 - Segregation and Peer Effects",
    "section": "The Identification problem",
    "text": "The Identification problem\n\n\n\n\nthe big short\n\n\n\n\nconnected\n\n\nThe challenge of distinguishing whether individuals associate with one another due to sorting (as discussed by Schelling) or peer influence (illustrated by the standing ovation example) varies in complexity. In specific scenarios, like Racial segregation, the distinction is straightforward.\nHowever, the situation becomes murkier in other instances. Consider the phenomenon of happy people gravitating towards other happy individuals and the same pattern is observed among those who are unhappy. In these cases, both sorting and peer effects could be at play, and often, it’s challenging to pinpoint which is the driving force since the end result appears the same for both mechanisms. Therefore, simply observing the outcome won’t provide clarity. Sorting, characterized by the physical movement of individuals, necessitates dynamic data for accurate identification.\nAuthor Bill Bishop, discusses sorting in [@bishop2009big] the. For example, in recent years, many counties in the USA have become more polarized. Bishop argues that this occurs due to sorting. However, gerrymandering is a more likely root cause.\nNicholas A. Christakis and James H. Fowler, discusses peer effects in [@christakis2009connected].\nWith migration data, it’s simpler to establish whether like-mindedness is caused by sorting or peer effects.\n\nReferences\nNote: this page is based on the following source:\n\n[@page2017modelthinking] MOOC, Course material & Transcripts.\nTA Notes by [@fisher2017modelthinking].\nStudent notes by in [@kleinikink2016naturalmoney] and [@groh2017model]."
  },
  {
    "objectID": "w04.html",
    "href": "w04.html",
    "title": "Lesson 4 - Decision Models",
    "section": "",
    "text": "Decision models encode how people make decisions as well as how they should make decisions.\n\nPositive models predict the behavior of others.\nNormative models provide guidance on how to make better, more consistant decisions.\nMulti criteria models make decision using multiple input variables.\nSpatial models are multi-criteria decision-making models that assume that there are a two dimensions and an ideal point. They try to infer how far the query is from the ideal point for each feature.\nMulti-criteria models: These models allow for the consideration of multiple factors in the decision-making process.\nProbabilistic models: These models incorporate uncertainty into the decision-making process.\n\nDecision trees can be used to handle uncertainty with the added bonus that trees make it easy to evaluate the value of information they encode.\n\n\nMulti-criterion decision making (MDCM)can be based on qualitative or quantitative criteria.\n\nExample 1 (Qualitative house ranking model)  \n\nListing the criteria for evaluation.\nChecking each criterion for each house under consideration.\nTallying the checks per house\nSelecting the house with the highest number of criteria met.\n\n\n\nExample 2 (Quantitative house ranking model) If we now also add a weight column, we can then assign different relative strength for each criteria reflecting its merits in determining the outcome which will make the model more powerful. Since we can now calibrate the weights of the model to our data, it is now quantitative model."
  },
  {
    "objectID": "w04.html#sec-introduction-to-decision-making",
    "href": "w04.html#sec-introduction-to-decision-making",
    "title": "Lesson 4 - Decision Models",
    "section": "",
    "text": "Decision models encode how people make decisions as well as how they should make decisions.\n\nPositive models predict the behavior of others.\nNormative models provide guidance on how to make better, more consistant decisions.\nMulti criteria models make decision using multiple input variables.\nSpatial models are multi-criteria decision-making models that assume that there are a two dimensions and an ideal point. They try to infer how far the query is from the ideal point for each feature.\nMulti-criteria models: These models allow for the consideration of multiple factors in the decision-making process.\nProbabilistic models: These models incorporate uncertainty into the decision-making process.\n\nDecision trees can be used to handle uncertainty with the added bonus that trees make it easy to evaluate the value of information they encode.\n\n\nMulti-criterion decision making (MDCM)can be based on qualitative or quantitative criteria.\n\nExample 1 (Qualitative house ranking model)  \n\nListing the criteria for evaluation.\nChecking each criterion for each house under consideration.\nTallying the checks per house\nSelecting the house with the highest number of criteria met.\n\n\n\nExample 2 (Quantitative house ranking model) If we now also add a weight column, we can then assign different relative strength for each criteria reflecting its merits in determining the outcome which will make the model more powerful. Since we can now calibrate the weights of the model to our data, it is now quantitative model."
  },
  {
    "objectID": "w04.html#sec-spatial-choice-models",
    "href": "w04.html#sec-spatial-choice-models",
    "title": "Lesson 4 - Decision Models",
    "section": "Spatial Choice Models",
    "text": "Spatial Choice Models\nSpatial choice models assume the existence of an ideal point, representing an optimal balance of attributes. The model focuses on the difference between the available choices and the ideal point for each feature.\nThis model can be can be on one dimension, e.g. the size of a television screen, or it can be on multidimensional, e.g. when choosing between cars or political candidates, where multiple issues might be relevant.\nSpatial choice models can be used both normatively, to guide decision-making, and positively, to understand an individual’s preferences based on their choices.\nHamburgers are a favorite example in economics.\n\nExample 3 (The Ideal Cheeseburger) Imagine the ideal burger: 2 cheese slices, 2 patties, 2 tomatoes, 4 tbsp of ketchup, 4 tbsp of mayo and 4 pickles.\nThis is our ideal point in six-dimensional space. Now let’s compare\n\nThe Big Mac\n\n\nscores 2, 2, 0, 3, 4, 6.\nabsolute value of the difference from the ideal: 0, 0, 2, 1, 0, 2.\nadding up we get 5.\n\n\nThe Whopper\n\n\nscores 2, 1, 2, 3, 4, 4\nand so the absolute value of the difference is 0, 1, 0, 1, 0, 0\nadding up gives 2.\n\n\\(2&lt;5 \\implies\\) we will decide to get a Whopper\n\n\nProbability: The basics\nThere are several approaches to probability:\n\nClassical probabilities\nFrequentist\nBayesian/Subjective probabilities\n\nThe Frequentist approach is the most common followed by the Bayesian one. Probability is defined as the likelihood of some future event happening. There are three axioms of probability that are universally true:\n\nP1 \\(P(A) \\in [0,1]\\) if \\(P(A)=1\\) we call it the impossible event, and is \\(P(A)=1\\) we call it the certain event.\nP2 \\(i \\ne j \\implies x_j \\bigcap x_i = \\emptyset \\text{ and } P(\\bigcup x_i) = \\mathbb{1} \\sum_{\\omega \\in \\Omega} P(X=x_\\omega)=1\\) i.e. sum of all the probabilities of all mutually exclusive outcomes of X is 1\nP3 \\(\\forall A \\subset B \\implies P(B)&gt;P(A)\\)."
  },
  {
    "objectID": "w04.html#sec-decision-trees",
    "href": "w04.html#sec-decision-trees",
    "title": "Lesson 4 - Decision Models",
    "section": "Decision trees",
    "text": "Decision trees\n\nDecision trees facilitate complex decision-making under uncertainty.\nThey can improve decision-making,\nProvide insights into others’ choices,\nEnhance self-understanding.\n\n\n\n\n\n\n\nReality Check\n\n\n\nIn the real wold we rarely encounter this kind of decision tree. We do use decision trees and random forests and none of this great ideas are simple to carry out with these algorithms, however they too have other neat tricks.\n\n\n\nA decision under uncertainty.\n\nExample 4 (Booking trains)  \n\n\n\n\nTrains Tickets Decision tree\n\nFor example, consider planning a trip where there’s a 40% chance of missing a 3pm train. The 3pm ticket costs $200, while the 4pm train costs $400. Buying the $200 ticket carries a 40% risk of wasting that money and incurring a total cost of $600.\nA decision tree helps estimate the expected cost of each option. The $200 ticket has an expected cost of $360, making it the rational choice despite the risk.\n\n\n\nMaking Complex Decision\nNext, we will consider a more complex scenario of applying for a scholarship\n\nExample 5 (Scholarship)  \n\n\n\n\nScholarship decision tree\n\nA Scholarship is worth $5,000, but there is a limit of 200 applicants. To be eligible, applicants must write a two-page essay. Using these ten finalists are chose who must complete a ten-page essay. To make an informed choice we should know the probability of each events and it associated cost or benefit. Suppose we would have to would have to invest $20 for a two-page essay and $40 for a ten-page essay, should we make this investment ?\nTo solve the problem we need to decide is this a classification problem or a regression problem. Do we want to model the cost a number or the decision which is a binary categorical variable\n\ndraw the tree;\nwrite down payoffs and probabilities\nsolve backwards.\n\n\n\n\n\n\n\nTip\n\n\n\nAlternatively, if we have data on the time/cost of writing previous scholarship essays, we can use a decision tree model to learn the probabilities from the data.\n\n\nIn the final round, there’s a 10% chance of winning $4940 and a 90% chance of losing $60, resulting in a value of \\(0.9 \\times - 60 USD + 0.1 \\times 4940 USD = 440 USD\\) The chance of reaching the final and potentially winning $440 is 5%, worth $22. There’s a 95% chance of losing $20, worth -$19. Therefore, the value of writing the first essay is $3 compared to $0 for not participating.\n\nWe can also use decision trees to infer what other people think about probabilities.1.\n1 this is a useful ability for Bayesian games\n\nInsights into others’ incentives\n\nExample 6 (Investment opportunity)  \n\n\n\n\nInvestment decision tree\n\nA friend tells us about an investment she read about from @InvestorMom on subreddit r/wallstreetbets about a “short squeeze” on a gaming company called GameStop.\n\nIt could pay us $50,000 if we put in $2,000.\nShe has invested $2,000.\nShould we follow suit?\n\nThe first thing to ask is, “What would she think her likelihood of success”?\nWe can try to work this out using a decision tree.\nLet us suppose:\n\n\\(P(I=success)=p\\)\nthus by rule P2 \\(P(I=failure)=1-p\\) .\nHer decision suggests that she thinks \\(50p - 2(1-p) &gt; 0 \\ge 52p &gt; 2 \\ge p &gt; 1/26 \\approx 4%\\).\nWe can infer from her decision that she thinks that the chance of success is above 4%.\n\n\n\n\nEnhancing self-understanding\nWe can use these trees to infer payoffs and sometimes decision trees can even reveal our own underlying values.\n\nExample 7 (Visting the old folks at home)  \n\n\n\n\nVisiting Parents decision tree\n\nImagine having a standby ticket to visit your parents with a \\(\\frac{1}{3}\\) chance of getting on the flight.\nThe decision of whether to go to the airport or not can be analyzed using a decision tree to determine how much you value seeing your parents.\nLet V represent the value of seeing your parents and c represent the cost of going to the airport. Choosing not to go implies that\n\\[\n\\frac{1}{3} (V - c) -\\frac{2}{3} c &lt; 0 \\implies \\frac{1}{3}V &lt; c \\implies V &lt; 3c\n\\]\nThis means the value of seeing your parents is less than three times the cost of going to the airport.\nConversely, going to the airport suggests the value is greater than three times the cost."
  },
  {
    "objectID": "w04.html#value-of-information",
    "href": "w04.html#value-of-information",
    "title": "Lesson 4 - Decision Models",
    "section": "Value of information",
    "text": "Value of information\nIn models of decisions being made under uncertainty, we should consider how much would some additional information be worth to us? A benefit of using a formal model is that we can now figure this out by:\n\nEstimate the value without the information using the expected value of our bast option.\nEstimate the value with the value of the optimal choice with the information\ncalculate the difference.\n\nThis is disarmingly simple except we will usually need the information in part 2, before we know if we should buy it.\n\nExample 8 (Roulette wheel) In the United States a roulette wheel has 38 different options, the numbers 1 through 36 plus two other spots. If we forecast a particular option, the odds of winning are \\(\\frac{1}{38}\\). What is the value of information here?\n\nWithout information our expected winnings are \\(\\frac{100}/{38}\\) USD.\nWith information our expected winnings are \\(100\\) USD.\nThus this information is worth \\(100- \\frac{100}/{38}\\) USD\n\n\n\n\n\n\n\n👮 Goofy Example 💩💩💩\n\n\n\nThis is another goofy example:\n\nIf we could get a tip for the next number 🔮 it really means is the roulette is rigged 🤑 in the worst way possible.\nIf we win a few times we would be in big trouble 😨\nthe tip is worthless 🥴\nwe should get out ASAP. 😆\n\n\n\n\nThis idea is applicable in more complex scenarios.\n\nExample 9 (Buy or rent a car) Suppose we are thinking about buying a car. We are worried about buying today since we have heard there might be a cash-back program next month in which we could get $1,000 back. We figure there’s a 40% chance there will be a cash-back program.\nWe might could rent a car for $500 then wait if cash-back materialises.\nNow suppose we know someone at the auto company who can sell us this information, how much would it be worth to pay him?\nThis is fairly easy to estimate with a decision tree.\nThe choice is between renting or buying a car.\nWe can see from the figure that - the value of renting without information is $0.4 \\(\\$500 + 0.6 \\times -\\$500 = -\\$100\\). - the value of the buying, is $0. - buying is best without information.\n\n\n\n\nRent or buy Ex Ante\n\n\n\n\nRent or buy with information\n\n\nIf you have the information we can estimate our new expected payoffs:\n\\(0.4 \\times 500 \\text{ USD } + 0.6 \\times 0 \\text{ USD } = 200 \\text{ USD }\\)\nSince before our best course had 0 USD expected payoff, this is also the value of the information."
  },
  {
    "objectID": "w04.html#more-decision-models",
    "href": "w04.html#more-decision-models",
    "title": "Lesson 4 - Decision Models",
    "section": "More decision models",
    "text": "More decision models\nThe decision book list around 50 of different models for different decisions. Some scenarios include marketing, picking gifts. Some are well known others obscure. The text, however, is very compact which means that these models will require additional research if they seem useful. What I liked though is that every reviewer seems to have picked a different subset."
  },
  {
    "objectID": "w04.html#references",
    "href": "w04.html#references",
    "title": "Lesson 4 - Decision Models",
    "section": "References",
    "text": "References\nNote: this page is based on the following source:\n\n[@page2017modelthinking] MOOC, Course material & Transcripts.\nTA Notes by [@fisher2017modelthinking].\nStudent notes by in [@kleinikink2016naturalmoney] and [@groh2017model]."
  },
  {
    "objectID": "w05.html",
    "href": "w05.html",
    "title": "Lesson 5 - Thinking Electrons Modelling People",
    "section": "",
    "text": "Murray Gell-Mann\n\n\n“Imagine how hard physics would be if electrons could think.” — Murray Gell-Mann (1969 Nobel Laureate in Physics)\n\nPeople are diverse, tending to pursue their own best interests. The following frameworks embody different approaches to model human behavior.\n\nRational actor model: This approach is based on rational choice theory, drawing from game theory and Adams Smith’s view of economics and treating people as homogenous goal optimizers.\nBehavioral Economics: This approach is a pushback against the rational actor, using psychology to introduce diversity and limit rationality, thus offering a remedy to the rational actor model.\nRule-based: This approach simplifies the above ideas by assuming people follow the rules without deeper motives. It is advantageous to agent-based modeling.\n\nSocial Network Analysis (SNA) is a methodological approach to understanding social structures through networks and graph theory. It analyzes the relationships between individuals, groups, organizations, computers, URLs, and other connected information/knowledge entities.\nMaslow’s Hierarchy of Needs is a psychological theory that prioritizes human needs into a hierarchy, suggesting that people are motivated to fulfill basic needs before moving on to other, more advanced needs.\nBig Five Personality Traits (OCEAN): This model categorizes human personality into five broad dimensions: openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. It’s widely used in psychology to understand and predict human behavior and personality.\nSocial Role Theory suggests that individuals’ behavior is shaped by the roles they are expected to play in society, which are shaped by societal norms and structures.\nSymbolic Interactionism focuses on the meanings that people derive from social interactions and how these meanings shape individual behaviors and societal norms.\nToday, we focus on the first three, but many models can mix and match different aspects of each approach. Also, many other ways exist to model people who do not fall under the above approaches."
  },
  {
    "objectID": "w05.html#sec-thinking-electrons-modelling-people",
    "href": "w05.html#sec-thinking-electrons-modelling-people",
    "title": "Lesson 5 - Thinking Electrons Modelling People",
    "section": "",
    "text": "Murray Gell-Mann\n\n\n“Imagine how hard physics would be if electrons could think.” — Murray Gell-Mann (1969 Nobel Laureate in Physics)\n\nPeople are diverse, tending to pursue their own best interests. The following frameworks embody different approaches to model human behavior.\n\nRational actor model: This approach is based on rational choice theory, drawing from game theory and Adams Smith’s view of economics and treating people as homogenous goal optimizers.\nBehavioral Economics: This approach is a pushback against the rational actor, using psychology to introduce diversity and limit rationality, thus offering a remedy to the rational actor model.\nRule-based: This approach simplifies the above ideas by assuming people follow the rules without deeper motives. It is advantageous to agent-based modeling.\n\nSocial Network Analysis (SNA) is a methodological approach to understanding social structures through networks and graph theory. It analyzes the relationships between individuals, groups, organizations, computers, URLs, and other connected information/knowledge entities.\nMaslow’s Hierarchy of Needs is a psychological theory that prioritizes human needs into a hierarchy, suggesting that people are motivated to fulfill basic needs before moving on to other, more advanced needs.\nBig Five Personality Traits (OCEAN): This model categorizes human personality into five broad dimensions: openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism. It’s widely used in psychology to understand and predict human behavior and personality.\nSocial Role Theory suggests that individuals’ behavior is shaped by the roles they are expected to play in society, which are shaped by societal norms and structures.\nSymbolic Interactionism focuses on the meanings that people derive from social interactions and how these meanings shape individual behaviors and societal norms.\nToday, we focus on the first three, but many models can mix and match different aspects of each approach. Also, many other ways exist to model people who do not fall under the above approaches."
  },
  {
    "objectID": "w05.html#sec-rational-actor-model",
    "href": "w05.html#sec-rational-actor-model",
    "title": "Lesson 5 - Thinking Electrons Modelling People",
    "section": "Rational Actor model",
    "text": "Rational Actor model\nThis approach builds on the definition of making rational decisions. Embodies the assumptions of choice and game theory:\n\nAgents have an objective - their goal in the model.\nGiven the objective, agents try to optimize their outcomes.\nAgents are rational actors.\nThey are endowed with a utility function, giving them the capacity to numerically quantify and rank risks or benefits. They can impeccably rank any situation, decision, or collection of alternatives without errors or mistakes.\nCertain bounds may be placed on their rationality like:\n\nIncomplete information on payoffs or the game structure, strategy of other players\nUncertainty\nSelf-control bound leading to a capacity for errors (brinksmanship and the shaking hand)\nIterated games that allow agents to adapt and learn.\n\nThey will develop a strategy profile, which is a collection of optimal action at each juncture in a finite tree of uncertain eventualities arising from the actions of other agents. The weighted sum of utilities or profits are called payoffs at the leaf nodes of such a tree, the possible scores that rational actors people can optimize, and they help to keep track of the games’ score\n\nIt helps to distinguish between a Decision Model that depends on the agent’s actions and a Game that involves actions by other agents or the environment. It also helps to distinguish rationality from selfish behavior. Game theory first dealt with zero-sum games. These were non-cooperative scenarios like chess, which dealt in absolutes, imposing winner-takes-all rules. With the advent of Nash equilibrium, the field opened up to include a broader range of cooperative games with win-win scenarios that can model diverse team behaviors like coordination, signaling, evolution of cooperation, altruism, and social dilemmas. When there are additional players, the Core and Shapley values provide a rational way for players to form coalitions and even conspiracies in which they pool resources and allocate shared payoffs fairly.\nAgents know or learn to apply different solution concepts like Minimax, Backwards Induction, Forward Induction, Nash Equilibrium, and Bayesian Equilibrium, which are formal rules that determine how a game will unfold and based on the rules, constraints, and incentives of another agent. Furthermore, rational agents may refine their solutions to eliminate pathological cases.\n\n\n\n\nExtensive form Game\n\n\n\n\nDeriving the Normal form\n\n\nWe can represent games in Normal form and Extensive form.\n\nThe Extensive form of a game describes how a game unfolds in terms of action and response arranged using a tree.\n\nuncertainty can then be represented by assigning a probability tree with nodes for each player’s actions and the payoffs at the leaves.\nbound on the availability of information can be indicated using information sets. This also allows extensive form games to model simultaneous moves using a tree - by letting players move in turn but in ignorance of the other player’s moves.\nincomplete information—such as information about other players’ payoffs or the type of their opponents, adds an initial move by “nature,” setting these at random at the start but in a privileged information set.\n\nThe Normal form summarizes the opposing payoffs using a contingency table for all possible action combinations. However this is the form in which it is possible to work our different equilibrium solutions.\n\nWhen do humans act as rational agents?\n\nWhen stakes are high.\nWhen games are iterated, agents will learn and adapt more often.\nIn group decisions.1.\nIn simple settings.\n\n1 disfuntional Groupthink in the context of the Bay of Pigs (the failed invasion of Castro’s Cuba in 1961) nearly lead to ArmageddonWhat are the merits of the rational actor model?\n\nIt’s a benchmark to evaluate actual behavior\nLearning has a tendency to make people more rational\nMistakes can cancel out: The average will approximate the rationale if there is no bias in the mistakes.\n\n\n\n\n\nIt’s so much fun bashing the Rational Actor Model\n\nThe rational actor model is the basis of modern economic thinking. Its centrality makes it the favorite pinata for anyone discontent with some aspect of the model or economic outcomes. While some have even received recognition and prizes, most economic thinking is fixed on treating people as rational actors. Why has the behavioral school of economics made such little impact? Because its alternatives are cumbersome mathematically, intractable computationally, and, in many cases, likely to cancel out in aggregation."
  },
  {
    "objectID": "w05.html#behavioral-models",
    "href": "w05.html#behavioral-models",
    "title": "Lesson 5 - Thinking Electrons Modelling People",
    "section": "Behavioral models",
    "text": "Behavioral models\n\n\n\n\nthinking fast and slow\n\n\n\n\nnudge\n\n\nBehavioral models are critical of the rational actor assumption based on evidence of actual behavior, neuroscience, and psychology. Experiments reveal systematic deviations from optimal choices, and neuroscience sheds light on how our brain structure, information processing, and thinking patterns lead us astray from rational actor predictions.\nIn Thinking, Fast and Slow, Daniel Kahneman @kahneman2011thinking proposes two systems of thought: fast processes driven by emotion and quick cues and slow processes that are more deliberate and rational. Fast thinking makes us susceptible to biases that the rational actor model ignores.\nCass Sunstein and Richard Thaler [@thaler2012nudge] argue that these systematic errors in human judgment have significant implications for policy design.\nFour types of well-documented biases cause behavior to deviate from rational behavior:\n\nProspect theory is an agent’s bias to be risk averse for profits yet risk loving for losses.\nHyperbolic discounting is a view by agents that recent rewards are preferable to equivalent future rewards\nStatus quo bias is a preference to avoid change\nBase rate bias means our current thinking influences us.\n\nProspect theory explains seemingly irrational gambling behavior. For example, people might choose a guaranteed $400 over a 50% chance of winning $1,000 but prefer a 50% chance of losing $1,000 over a guaranteed $400 loss.\nKahneman came up with the following example.\nSuppose you have two options. You can get $400 for sure, or you get a 50% chance of winning $1,000 and a 50% chance of gaining $0. A lot of people would choose the $400 for sure. If the amounts get larger, people become more risk-averse in gains. However, when there is a choice between an inevitable loss of $400 or a loss of $1,000 at \\(p(loss) = 0.5\\) or a loss of $0, people are more willing to take the gamble. Both behaviors are not rational.\nHyperbolic discounting explains our preference for immediate gratification. We might choose $1,000 today over $1,005 tomorrow but prefer $1,005 in a year and a day over $1,000 in a year. This leads to the “chocolate cake” dilemma – we might plan to resist temptation in the future but succumb when faced with immediate gratification.\ne.g., we tend to prefer $1000 now to $1005 tomorrow, but we tend to prefer $1005 over a year and a day to $1000 over a year. Immediate gratification matters a lot to humans. This often has what is called the chocolate cake implication. People want to be healthy, so if you are offered a chocolate cake a week from now, you are more likely to decline the offer, but if the chocolate cake is put in front of you, you are more likely to eat it. This is because fast thinking prevails.\n\n\n\n\nCheck box to contribute\n\nThe status quo bias explains our resistance to change. For example, people are less likely to contribute to a pension fund if they have to actively opt in, compared to having to opt out.\ne.g., if people have to check a box to contribute to the pension fund, most of them won’t check the box, but if they have to check a box to not contribute to the pension fund, most of them still won’t check the box. This is because checking the box implies a change. In England, people have to check a box to donate organs, and 25% of people do so. In the rest of Europe, people have to check a box not to donate organs, and only 10% have.\nThe base rate bias demonstrates how our current thoughts influence our judgments. For example, if asked to estimate the manufacturing date and price of a box, the answers are often close together, as the initial number influences the subsequent estimate.\nFor example, when people are asked when a box is made and how much it costs, the answers are often close to each other. e.g., you may think that the box may is made in 1950 (50), and then you probably estimate price close to that number, for instance 52. This is because you were already thinking of a number, so if you have to think of another number, this number probably is close to the first number.\nThere are many more biases, and they are well documented. There are also criticisms. For instance, most of those biases are found in Western, Educated, Industrialised, Rich, Developed (WEIRD) countries. So, how many of them apply to other countries as well? Furthermore, people learn so that they may overcome their biases. Finally, it can be computationally difficult to account for all kinds of biases, so many models assume people are rational. One way to deal with this is to use simple rules. A more sophisticated way is to start assuming that people are rational, and then look for the biases that are relevant, and include them in the model.\n Types of rules"
  },
  {
    "objectID": "w05.html#sec-rule-based-models",
    "href": "w05.html#sec-rule-based-models",
    "title": "Lesson 5 - Thinking Electrons Modelling People",
    "section": "Rule based models",
    "text": "Rule based models\nRule-based models assume people simply follow predetermined rules. For example, the Schelling model posits that individuals will relocate if the percentage of similar people in their vicinity falls below a threshold.\nFor example, the Schelling model presumes that people will move as soon as the percentage of similar people falls below a certain threshold. There are four types of rules-based behaviors in two dimensions. There are fixed and adaptive rules in a decision context or a game context where the payoff depends on what other people do. In a game context, a rule is often called a strategy.\n\nFixed decision rules: Examples include random choice (used as a benchmark) and taking the most direct route, which may not be optimal.\n\nAn example of a fixed decision rule is random choice. Random choice, like optimal choice, can be used as a benchmark. You can compare optimal choice and random choice and see how the model behaves under those assumptions to get more understanding about what might happen and what could happen. Another example of a fixed rule is taking the most direct route, which is the route closest to the right direction. This may not be the shortest or the fastest route, so this rule may not be optimal.\n\n\n\n\nTit for tat Moore Machine\n\nFixed strategies: Examples include dividing resources evenly and tit for tat, where cooperation is reciprocated and defection is punished. An example of a fixed strategy is divide evenly. For example, if heirs divide an asset, they might decide to divide it evenly. Another fixed strategy is tit for tat. e.g., person A starts out acting nice to person B, and he continues to act nice as long as person B acts nice, but if person B acts mean, person A will act mean too. But if person B starts acting nice again, person A will act nice too.\n\n\n\n\nGrim trigger Moore Machine\n\nTit-for-tat and grim trigger are fixed strategies represented by Moore Machines. Tit for tat starts with cooperation and only switches to defection if the other player defects, returning to cooperation when the opponent cooperates again. Grim trigger, however, permanently switches to defection after a single defection by the opponent.\n\nAdaptive decision rules. These rules are adjusted based on feedback. Examples include the gradient-based methods, where actions are continuously adjusted in the direction of improvement, and trial-and-error approaches.\ne.g., suppose that you are baking cookies, and start adding one spoon full of honey, and it turns out that the cookies are very good. The next time, try adding two spoons full of honey. If the cookies taste even better, you might add another spoonful of honey. You might go on until the cookies start to taste too sweet. Another adaptive rule is random behavior or changing what you are doing until you find something better. With regard to cookies, add raisins, chocolate, or walnuts.\nAdaptive strategies. These strategies are particularly relevant in games, where adapting to others’ behavior can be advantageous. Examples include best response, where an agent chooses the optimal action given the opponent’s action, and mimicry, where successful behaviors are imitated."
  },
  {
    "objectID": "w05.html#sec-when-does-behavior-matter",
    "href": "w05.html#sec-when-does-behavior-matter",
    "title": "Lesson 5 - Thinking Electrons Modelling People",
    "section": "When does behavior matter?",
    "text": "When does behavior matter?\nThe significance of accurately modeling behavior depends on the situation. Two contrasting examples are two-sided markets and races to the bottom. In a two-sided market with buyers and sellers, the market price tends to converge towards equilibrium regardless of whether agents are rational, partially informed, or even completely uninformed. Behavior has minimal impact on the outcome.\nIn a race to the bottom game, where the winner is the player closest to two-thirds of the average guess, rational players will converge on a guess of zero. However, introducing biases can significantly alter the outcome, with guesses clustering around 50, 33, and 22, reflecting a mix of bias and attempts at rational calculation. Repeated play can lead to convergence towards zero as players adapt their strategies.\nThese examples highlight that while rational behavior serves as a valuable benchmark, incorporating biases and considering simple rules are crucial for accurate modeling. Ultimately, the importance of behavior depends on how much it affects the model’s outcome. If the impact is minimal, precise behavioral modeling might be unnecessary. However, for situations where behavior significantly influences outcomes, selecting the most appropriate model type becomes critical."
  },
  {
    "objectID": "w05.html#references",
    "href": "w05.html#references",
    "title": "Lesson 5 - Thinking Electrons Modelling People",
    "section": "References",
    "text": "References\nNote: this page is based on the following source:\n\n[@page2017modelthinking] MOOC, Course material & Transcripts.\nTA Notes by [@fisher2017modelthinking].\nStudent notes by in [@kleinikink2016naturalmoney] and [@groh2017model]."
  },
  {
    "objectID": "w07.html",
    "href": "w07.html",
    "title": "Lesson 7 - Tipping Points",
    "section": "",
    "text": "Height of the camel\n\n\nTipping points are nonlinear models where a small change is leading to a big effect.\n\n\n\n\n\n\n\nCritical Note\n\n\n\nThe straw that broke the Camel’s back is utterly ridiculous example. Here is perhaps a few better ones:\n\n\n\nAs the human body gains in muscle and loses fat it reaches a point where it can no longer float in water, at this point a swimmer must forever be treading water or swimming to keep afloat. The tipping point can also be viewed as drop survivability time for people in the sea.\nThe volume of an explosive reaches a tipping point when it is detonated.\nToday most bridges can withstand cross winds. However in the past some bridge were not and cross wind during a storm could force a bridge to shake at its resonant frequency which caused it to break apart.\n\nAlso here are some worse examples of tipping point\n\nThe drop in crime rate in New York is claimed by Malcom Gladwell to be an example of a tipping point. He suggested that “Broken Windows Theory” was what tipped crime rates in 1984 which was a policy of increasing the police force and getting them to crack down hard on misdemeanors like graffiti, jay-walking and free riding in the subway. Did this pretext of harassing minorities really motivated criminals to abandon crime, stop selling drugs, or emigrate to other cities with smaller police forces ?\n\nIn @donohue2001impact a more convicting hypothesis for the decline of crime is presented. The economists also looked at other cities in the US that experienced similar dramatic drops in crime. What they report is that drops in crime rate took place in both rich cities that boosted trier police force and in poor cities that could not afford to. The point out that crimes follow a Pareto-distribution were 95% of all violent crime in big cities is due to a very small population of career criminals. They suggest that the drop was due to Roe Vs Wade passing about 18 years with the outcome that teen-agers and single mothers were able to avoid having children they could not support. The data required lots of massaging (murders are solved long after they are reported, there was a crack epidemic, some place had easier access to abortion etc,) And while this effect seems to occur globally in different localities after an 18 year lag this analysis has been challenged by others researchers several times. See also Legalized abortion and crime effect\nA third hypothesis is that the crime levels are correlated with the use of leaded gasoline. Lead is highly toxic to the human brain.\nOften when people think about tipping points, they think of kinks in curves. Sometimes a kink in a curve reflects a tipping point, but not always. In many cases these kinks are just exponential growth, because with exponential growth you have a curve that takes off at some point. A book about tipping points [@gladwell2006tipping], written by Malcolm Gladwell.\nIt is important to see what is a tip and what is not, and also what kind of models produce tips. Two famous models are the percolation model from physics and SIS model from epidemiology. Percolation refers to the question whether or not water can make its way through a certain layer, e.g. the ground or a coffee filter. SIS stands for susceptible, infected and susceptible again, which is a simple model for diseases.\nThere is a distinction between types of tips. There are direct tips and contextual tips. With a direct tip or active tip, a variable itself changes, which causes it to tip. e.g., a battle might tip a war. With a contextual tip something changes in the environment that makes it possible for the system to move from one state to another. e.g., the density of the trees could tip the spread of a forest fire.\nA system can be stable (in equilibrium), periodic, random or complex. A system can tip from one state to another but there are also tips within classes. e.g., a system might tip from one equilibrium to another equilibrium.\n\n\n\n\nPercolation checker board"
  },
  {
    "objectID": "w07.html#sec-tipping-points",
    "href": "w07.html#sec-tipping-points",
    "title": "Lesson 7 - Tipping Points",
    "section": "",
    "text": "Height of the camel\n\n\nTipping points are nonlinear models where a small change is leading to a big effect.\n\n\n\n\n\n\n\nCritical Note\n\n\n\nThe straw that broke the Camel’s back is utterly ridiculous example. Here is perhaps a few better ones:\n\n\n\nAs the human body gains in muscle and loses fat it reaches a point where it can no longer float in water, at this point a swimmer must forever be treading water or swimming to keep afloat. The tipping point can also be viewed as drop survivability time for people in the sea.\nThe volume of an explosive reaches a tipping point when it is detonated.\nToday most bridges can withstand cross winds. However in the past some bridge were not and cross wind during a storm could force a bridge to shake at its resonant frequency which caused it to break apart.\n\nAlso here are some worse examples of tipping point\n\nThe drop in crime rate in New York is claimed by Malcom Gladwell to be an example of a tipping point. He suggested that “Broken Windows Theory” was what tipped crime rates in 1984 which was a policy of increasing the police force and getting them to crack down hard on misdemeanors like graffiti, jay-walking and free riding in the subway. Did this pretext of harassing minorities really motivated criminals to abandon crime, stop selling drugs, or emigrate to other cities with smaller police forces ?\n\nIn @donohue2001impact a more convicting hypothesis for the decline of crime is presented. The economists also looked at other cities in the US that experienced similar dramatic drops in crime. What they report is that drops in crime rate took place in both rich cities that boosted trier police force and in poor cities that could not afford to. The point out that crimes follow a Pareto-distribution were 95% of all violent crime in big cities is due to a very small population of career criminals. They suggest that the drop was due to Roe Vs Wade passing about 18 years with the outcome that teen-agers and single mothers were able to avoid having children they could not support. The data required lots of massaging (murders are solved long after they are reported, there was a crack epidemic, some place had easier access to abortion etc,) And while this effect seems to occur globally in different localities after an 18 year lag this analysis has been challenged by others researchers several times. See also Legalized abortion and crime effect\nA third hypothesis is that the crime levels are correlated with the use of leaded gasoline. Lead is highly toxic to the human brain.\nOften when people think about tipping points, they think of kinks in curves. Sometimes a kink in a curve reflects a tipping point, but not always. In many cases these kinks are just exponential growth, because with exponential growth you have a curve that takes off at some point. A book about tipping points [@gladwell2006tipping], written by Malcolm Gladwell.\nIt is important to see what is a tip and what is not, and also what kind of models produce tips. Two famous models are the percolation model from physics and SIS model from epidemiology. Percolation refers to the question whether or not water can make its way through a certain layer, e.g. the ground or a coffee filter. SIS stands for susceptible, infected and susceptible again, which is a simple model for diseases.\nThere is a distinction between types of tips. There are direct tips and contextual tips. With a direct tip or active tip, a variable itself changes, which causes it to tip. e.g., a battle might tip a war. With a contextual tip something changes in the environment that makes it possible for the system to move from one state to another. e.g., the density of the trees could tip the spread of a forest fire.\nA system can be stable (in equilibrium), periodic, random or complex. A system can tip from one state to another but there are also tips within classes. e.g., a system might tip from one equilibrium to another equilibrium.\n\n\n\n\nPercolation checker board"
  },
  {
    "objectID": "w07.html#sec-percolation-models",
    "href": "w07.html#sec-percolation-models",
    "title": "Lesson 7 - Tipping Points",
    "section": "Percolation models",
    "text": "Percolation models\nPercolation models come from physics. The idea is that water comes down in the form of rain and the question is does the water percolate through the soil or not? To make a model, you can simplify the situation by using a checkerboard. The idea is that water can only percolate from one filled box to another, including diagonal directions. In the example, the water can’t make it to the bottom.\n\n\n\n\nPercolation probability\n\nThe model is as follows. Assume that the graph is big. Let p be the probability that a box gets filled. Then we ask the question: does it percolate? As long as p is less than 59.2%, it doesn’t percolate. Above this value, the system tips, and then it suddenly becomes likely that it does percolate.\nWhat is causing the tip? For p less than 59.2%, there just aren’t enough boxes filled in. However, when the values passes 59.2%, it suddenly becomes more likely that the water percolates to the bottom.\n\n\n\n\nForest fire\n\nThis model can be applied to all kinds of things, for instance forest fires. In this case, you can use the checkerboard to represent trees, and the filled in boxes represent the density of the forest. e.g., you set the forest density to 57% and set fire to all trees on the left side in the model.\nAs you can see, the fire doesn’t make it to the other side, even after repeated attempts. If you push up forest density to 61%, nearly all attempts will show that the forest fire makes it to the other side. This is an example of the fertility of models. A model that was used to explain percolation, can also be used for forest fires.\n\n\n\n\nForest yield curve\n\nCuriously, the 59.2% threshold is also a tipping point in the yield curve of a forest. If you plant more trees, the yield of the forest in terms of wood produced rises in a linear fashion, until the tipping point, after which the yield will fall.\nYou can also apply this to banks. You can have a checkerboard of banks. If one bank fails then all banks that have loaned a lot of money to this bank may also fail. In this way bank failures can cascade.\n\n\n\n\nIMF bank model\n\nThe IMF has constructed models of banks that are more sophisticated. This model uses sophisticated accounting equations where banks have assets, capital, liabilities, and loans that are failing. You can then stress the system by having banks fail and see how far the failures spread.\nThis model is also based on the idea of percolation. In this model you can also ask the question whether there is a tipping point, where suddenly there are many bank failures? You could do the same with country failures.\n\n\n\n\nPeople network\n\nYou can also apply this on information percolating. Information spreads through networks of people. Assume that there is a probability that people will tell a piece of information to their friends, then you could ask what’s the likelihood that the information percolates as a function of this probability. The model would tell that if the piece of information, e.g. a gossip, is juicy enough, then it’s likely to spread.\nIntuitively, you might think that there might be a linear relationship between the value of information and the number of people hearing of it, but if you use a network model with a probability of people telling the information across links, then nothing happens if the information is not very valuable, but once the value gets above some critical threshold, you might see a tipping point, and almost everybody will hear about it.\n\n\n\n\nFrom A to B\n\nThis can also be applied to innovations. e.g., people may have been working on some technology for years or decades, and then suddenly a lot of people figure it out at approximately the same time. e.g., for a long time nobody made a steam engine, and suddenly many people were building steam engines. We see often bursts of scientific activity in a particular area, e.g. biotechnology where many people work on the same innovation at the same time.\nWhy does the percolation model apply here? Often finding a solution for a particular type of problem, e.g. producing a car or getting a mathematical proof, requires getting from A to B. There are many parts that have to work together, and you need a significant number of partial solutions to get through the whole problem. For instance, for a car you need wheels, brakes, an engine and a steering wheel.\nAs information accumulates, we can fill in more squares, and eventually someone can find a path from A to B. Suddenly there are multiple paths so that others can find other paths from A to B. It is therefore plausible that the percolation model explains those bursts in scientific activity."
  },
  {
    "objectID": "w07.html#sec-contagion-models-1-diffusion",
    "href": "w07.html#sec-contagion-models-1-diffusion",
    "title": "Lesson 7 - Tipping Points",
    "section": "Contagion models 1: Diffusion",
    "text": "Contagion models 1: Diffusion\n\n\n\n\nDiffusion graph\n\nIn a diffusion model everybody receives something, which could be information or a disease. The diffusion model works as follows:\n\nSuppose that there is some new disease called Wobblies,\n\\(W_t\\) - is the number of people who got the Wobblies at time \\(t\\).\n\\(N\\) is the total population.\n\\(N - W_t\\) is therfore the number of healthy.\n\\(τ\\) - is the transmission rate or the likelihood that someone sick infects someone healthy.\n\nSo, if two people meet, then what is the likelihood that one person gives it to the other? \\[\n\\tau \\times \\frac{W_t}{N} \\times \\frac{(N-W_t )}{N} \\qquad\n\\tag{1}\\]\nYou need one person who has the Wobblies and one person that doesn’t have it. So the probability is\nYou can also apply this to a new technology being adopted or a piece of information being spread, but in those cases people don’t have to meet physically.\nThe spreading also depends on the contact rate \\(c\\), which is how often people meet, then \\(cN\\) is the number of meetings. Hence,\n\\[\nW_{t+1} = W_t + c\\times N \\times τ \\times \\frac{W_t}{N}\\frac{(N-W_t )}{N} \\qquad\n\\tag{2}\\]\nThe formula says that the spread will start slow because only a few people are affected, then speed up where more people are affected, and then, when most people are affected, slows down again because there are fewer people that can be affected. The model has no tipping point."
  },
  {
    "objectID": "w07.html#sec-contagion-models-2-sis-model",
    "href": "w07.html#sec-contagion-models-2-sis-model",
    "title": "Lesson 7 - Tipping Points",
    "section": "Contagion models 2: SIS model",
    "text": "Contagion models 2: SIS model\n\\[\n\\begin{aligned}\n{\\frac {dS}{dt}}&=-{\\frac {\\beta SI}{N}}+\\gamma I\\\\[6pt]{\\frac {dI}{dt}}&={\\frac {\\beta SI}{N}}-\\gamma I\n\\end{aligned} \\qquad\n\\tag{3}\\]\nThe SIS model is much like the diffusion model but there is a difference. After people have been infected, they can recover and move back to the susceptible state. Hence,\n\\[\nW_{t+1} = W_t + c\\times N \\times τ \\times \\frac{W_t}{N}\\frac{(N-W_t )}{N} - a \\times W_t \\qquad\n\\tag{4}\\]\nwhere:\n\na is the recovery rate.\n\nThis can be simplified to \\[\nW_{t+1} = W_t (1 +(cτ\\frac{(N - W_t)}{N} - a) )\\qquad\n\\]\nThis model is interesting, because if the recovery rate is higher than the transmission rate, then the disease is not going to spread. This model has a tipping point.\nIf \\(W_t\\) is very small, then \\(\\frac{N - W_t}{N} \\approx 1\\) so \\(W_t+1 = W_t(1 + cτ - a)\\).\nThe disease is going to spread if \\(cτ - a &gt; 0\\) or \\(cτ &gt; a\\) or \\(\\frac{cτ}{a} &gt; 1\\).\n\\[\nR_0 \\def \\frac {cτ}{a}\n\\]\nis the basic reproduction number \\(R_0\\).\n\nif \\(R_0 &gt; 1\\) then the disease will spread.\nIf \\(R_0 &lt; 1\\) the disease dies off.\nThe tipping point is 0.\n\nDiseases like measles (15), mumps (5) and flu (3) have \\(R_0 &gt; 0\\).\nIf you have had the measles or the mumps, you don’t become susceptible again, so here the SIR model applies.\n\n\n\n\nStable dynamic model\n\nThis model is interesting for policy decisions on how many people need to be vaccinated. Assume V to be the proportion of people vaccinated, then the altered basic reproduction number will be \\(r0 = R0(1 - V)\\). So, to keep the disease from spreading, we need \\(r_0 &lt; 1\\) or \\(R_0(1 - V) &lt; 1\\) or \\(1 - \\frac{1}{R_0} &lt; V\\).\ne.g., if we want to keep the measles from spreading, and measles has a \\(R_0 = 15\\), then we need \\(1 - 1/15 = 14/15\\) of the people to be vaccinated.\nThere is also a tipping point with regard to vaccines. Below the tipping point only the people that are vaccinated are protected. Above the tipping point everyone is protected.\n\nHerd Immunity\n\nThe tipping point for a population vaccination when \\(1 - \\frac{1}{R_0} &lt; V\\)"
  },
  {
    "objectID": "w07.html#sec-classifying-tipping-points",
    "href": "w07.html#sec-classifying-tipping-points",
    "title": "Lesson 7 - Tipping Points",
    "section": "Classifying tipping points",
    "text": "Classifying tipping points\nThe basic idea of a dynamical system is in the graph to the left. What does the graph say? If says that x is going to change over time. If y is positive, then x is going to change in the direction of the arrows on the left. However, if y is negative, then x is going to change in the direction of the arrows on the right. This system moves to a stable equilibrium where x = y/2.\n\n\n\n\nDynamic model with tipping point\n\nMore complicated graphs are possible. In the next graph, if x &lt; 0.2 then it will move to 0, and if x &gt; 0.2 then it will move to 0.5. In this case there are two stable equilibria, while and 0.2 is an unstable equilibrium or a tipping point. This is called a direct tip, where a slight change in a variable will cause the variable to move into another direction.\n\n\n\n\nContextual change\n\nWith a direct tip, a small change in the variable itself, can have a large effect on its end state. e.g., the assassination of Archduke Franz Ferdinand tipped the whole system so that Europe went to war. Often directs tips were destined to happen, because the environment has become supportive of such an event.\nIf more and more preconditions are met, then the percolation model suggests that it is going to happen anyway. e.g., in the eve of World War I, the European powers were already forming alliances and preparing for war. So, often what causes a direct tip, is a change in the context. This is a contextual tip.\n\n\n\n\nTypes of systems\n\nYou can also apply this to the second graph. If you move down the line a little bit, then the system tips, and x will always move to 0. A contextual tip means that a small change in the environment has a large effect on the end state, often because of the number of squares filled in in the percolation model has reached the tipping point.\n\n\n\n\nState switches\n\nThis also applies to the SIS model where R0 = cτ - a and if R0 &gt; 1 then the disease spreads. If we change the virulence of the disease, or the rate at which people make contact, or the rate at which people recover, then the context changes.\nA system can be stable (in equilibrium), periodic, random or complex. A system can tip from one class to another but there are also tips within classes. e.g., a system might tip from one equilibrium to another equilibrium. This can be represented in a graph that depicts the movement of a variable x depending on some other variable r.\n\n\n\n\nActive tip"
  },
  {
    "objectID": "w07.html#sec-measuring-tips",
    "href": "w07.html#sec-measuring-tips",
    "title": "Lesson 7 - Tipping Points",
    "section": "Measuring tips",
    "text": "Measuring tips\nWhen measuring tips, we try to find out whether how likely the tip was going to happen. With an active or direct tip, the variable itself can cause the system to tip. If the system is at a tipping point, it is uncertain how it will behave. Once a tip has occurred, we know for certain how the system is going to behave. One way of measuring tippiness is by reductions in uncertainty.\nA measure of uncertainty can be changes in the likelihood of different outcomes that could occur. Initially, there may be a large number of possible outcomes. After the system tips, there might be an equilibrium or one possible outcome, or a number of other things could occur. We measure changes in the likelihood of different outcomes using the diversity index, which is used in social sciences, and entropy, which comes from physics and information theory.\nThe diversity index for i possible outcomes is\n\\[\n\\frac{1}{\\sum P_i^²}\n\\]\ne.g., if we have three possible outcomes A, B and C, with probabilities\nP(A) = 1/2, P(B) = 1/3 and P(C) = 1/6,\nthen the diversity index is\n\\(1 / (1/4 + 1/9 + 1/36) = 36/14 ≈ 2.57\\).\nFor i possible outcomes, the maximum diversity is i.\ne.g., if we have four possible outcomes A, B, C and D, which all have a probability of 1/4, so\n\\(P(A) = 1/4, P(B) = 1/4, P(C) = 1/4 and P(D) = 1/4\\), then the diversity index is\n\\(1 / ((1/4)² + (1/4)² + (1/4)² + (1/4)²) = 4.\\)\nHow can we use the diversity index to measure tips?\nIf changes in a variable cause changes in the diversity index, this is indicative of a tip.\ne.g., if initially there were three possible values, and the diversity index was 2.57, and then it flips to 1, the change in value of the diversity index is the measure of the tip.\nEntropy also measures the degree of uncertainty.\nThe formula for entropy is:\n\\[\n-\\sum_i P(X_i) log_2(P(X_i))\n\\]\nlog2 is the inverse of the power of 2, so log2(2x) = x. e.g., log2(1/4) = log2(2-2) = -2. e.g., if we have four possible outcomes A, B, C and D, which all have a probability of 1/4, then the entropy is - ((1/4)log2(1/4) + (1/4)log2(1/4) + (1/4)log2(1/4) + (1/4)log2(1/4)) = 2.\nEntropy tells us the number of bits of information we need to know to identify the outcome. If we have four possible outcomes A, B, C and D, which all have a probability of 1/4, you can split up the possiblities in half. It is either in A, B or in C, D. This is one bit of information. If it is in C, D then you need to know whether it is C or D. This is the second bit of information. You can always find the answer by asking two questions. Hence, the entropy is 2.\nThe diversity index shows the number of types. The entropy is the amount of information you need to identify the type. e.g., if you have options A and B that each have a probability of 1/2, then the diversity index is 2 and the entropy is 1. After the system tips, the diversity index goes to 1 and the entropy becomes 0, and you don’t need to ask any question to know what the state of the system is. Tips are changes in the likelihood of outcomes.\n\nReferences\nNote: this page is based on the following source:\n\n[@page2017modelthinking] MOOC, Course material & Transcripts.\nTA Notes by [@fisher2017modelthinking].\nStudent notes by in [@kleinikink2016naturalmoney] and [@groh2017model]."
  },
  {
    "objectID": "w08.html",
    "href": "w08.html",
    "title": "Lesson 8 - Economic Growth",
    "section": "",
    "text": "Exponential growth is accumulating over time like interest. It is possible to make primitive models for economic growth. These models show that without innovation, growth stops. Solow’s growth model allows for innovation and shows how innovation has a multiplier effect on our collective well-being. Extensions to these models can be used to explain why some countries are successful while others aren’t, and what enables economic growth.\n\n\n\n\nComparison Botswana versus Zimbabwe\n\nThe gross domestic product (GDP) is the market value of all goods and services produced in an economy. We want to know what causes growth. e.g., Botswana did much better than Zimbabwe. The economists Daron Acemoglu and James Robinson wrote a book named [@acemoglu2012nations] Why Nations Fail that focuses on the question why some countries like Zimbabwe do so badly.\nEconomists often talk about real changes in GDP. Real means that inflation is taken into account. The economy is measured in currency, e.g. dollars. If the economy grows by 5% in dollars but the inflation is 3%, then this increased amount of dollars is worth less.\nAn important question is whether or not super high levels of economic growth are sustainable. China has had growth rates of around 10% for 15 years. Japan had similar growth rates in the 1960s, and they remained high in the 1970s and 1980s. Japan’s growth rates fell after the country caught up with the rest of the world.\n\n\n\n\nWealth and happiness\n\nEconomic growth often doesn’t look like simple exponential growth. First economic growth can be high, and then it tends to fall. That is because investments are made in machinery and technologies, and they depreciate over time, so that prevents the economic growth curve from becoming exponential. Only innovation can shift the whole curve up.\nEconomic growth is focused on material things. Does material wealth really matter, in the sense that it makes people happier? That is a complicated question. GDP can be established accurately, but life satisfaction is a soft variable that is difficult to measure. Given these limitations it is possible to construct a graph that shows a few things.\nFor high income countries, where average incomes are above $20,000, more wealth doesn’t really matter. It does matter for low income countries. Getting from 0 to $10,000 is huge, but getting from $30,000 to $60,000 doesn’t matter a lot. Lifting people out of poverty makes them happier, but becoming richer after that isn’t that important."
  },
  {
    "objectID": "w08.html#sec-introduction-to-growth",
    "href": "w08.html#sec-introduction-to-growth",
    "title": "Lesson 8 - Economic Growth",
    "section": "",
    "text": "Exponential growth is accumulating over time like interest. It is possible to make primitive models for economic growth. These models show that without innovation, growth stops. Solow’s growth model allows for innovation and shows how innovation has a multiplier effect on our collective well-being. Extensions to these models can be used to explain why some countries are successful while others aren’t, and what enables economic growth.\n\n\n\n\nComparison Botswana versus Zimbabwe\n\nThe gross domestic product (GDP) is the market value of all goods and services produced in an economy. We want to know what causes growth. e.g., Botswana did much better than Zimbabwe. The economists Daron Acemoglu and James Robinson wrote a book named [@acemoglu2012nations] Why Nations Fail that focuses on the question why some countries like Zimbabwe do so badly.\nEconomists often talk about real changes in GDP. Real means that inflation is taken into account. The economy is measured in currency, e.g. dollars. If the economy grows by 5% in dollars but the inflation is 3%, then this increased amount of dollars is worth less.\nAn important question is whether or not super high levels of economic growth are sustainable. China has had growth rates of around 10% for 15 years. Japan had similar growth rates in the 1960s, and they remained high in the 1970s and 1980s. Japan’s growth rates fell after the country caught up with the rest of the world.\n\n\n\n\nWealth and happiness\n\nEconomic growth often doesn’t look like simple exponential growth. First economic growth can be high, and then it tends to fall. That is because investments are made in machinery and technologies, and they depreciate over time, so that prevents the economic growth curve from becoming exponential. Only innovation can shift the whole curve up.\nEconomic growth is focused on material things. Does material wealth really matter, in the sense that it makes people happier? That is a complicated question. GDP can be established accurately, but life satisfaction is a soft variable that is difficult to measure. Given these limitations it is possible to construct a graph that shows a few things.\nFor high income countries, where average incomes are above $20,000, more wealth doesn’t really matter. It does matter for low income countries. Getting from 0 to $10,000 is huge, but getting from $30,000 to $60,000 doesn’t matter a lot. Lifting people out of poverty makes them happier, but becoming richer after that isn’t that important."
  },
  {
    "objectID": "w08.html#sec-exponential-growth",
    "href": "w08.html#sec-exponential-growth",
    "title": "Lesson 8 - Economic Growth",
    "section": "Exponential growth",
    "text": "Exponential growth\nEconomic growth models are complex with variables like labour, physical capital, depreciation rates and savings rates, so it is better to start with simple exponential growth like the compounding of interest. The GDP of countries can grow in a similar fashion. This is why different growth rates are so important. The rule of 72 explains how quickly an exponential growing variable will double.\n\n\n\n\nDifferent growth rates\n\nExponential growth is simple and like the formula \\(x(1 + r)t\\). e.g., if you have $100 and 5% interest, you have $105 after one year. After two years it is $110.25. After ten years is is 100*(1 + 0.05)10. You can do the same with GDP. So, if you have a GDP of G and a growth rate of \\(r%\\) then GDP after \\(n\\) years will be \\(G * (1 + r/100)n\\).\nSo, why are growth rates so important? That is because differences in the growth rate can have dramatic consequences in the long run. This is because this growth is exponential.\nThe Rule of 72 means that dividing 72 by the growth rate approximately gives you the number of years in which the variable will double. For a growth rate of 2%, the Rule of 72 gives a number of years of 36. The exact number is 35. For 6% growth, the Rule of 72 gives a number of years of 12. So, in 36 years, 6% growth means doubling 3 times, or growing to 8 times the original GDP.\nContinuous compounding means that growth is constant and not just at intervals. The formula \\(x(1 + r)t\\) is just a simplification as if growth only happens once a year. It is possible to calculate interest every day using a formula like \\(x(1 + r/365)365\\), but this doesn’t work for continuous compounding.\nFor continuous compounding, the interval length approximates zero and the number of intervals approximates infinite. In that case the interest can be calculated using \\[\n\\lim_{n \\to\\infty} (1 + \\frac{r}{n})^{n_t}  = e^{r_t}\n\\] where\n\\(e= 2.71828\\).\nThis is a much simpler formula for growth.\n\n\n\n\nModel assumptions"
  },
  {
    "objectID": "w08.html#sec-basic-growth-model",
    "href": "w08.html#sec-basic-growth-model",
    "title": "Lesson 8 - Economic Growth",
    "section": "Basic growth model",
    "text": "Basic growth model\nLet’s make a simple growth model. Assume there is a group of workers and a field of coconut trees. When workers pick coconuts they can do two things, which are eating them or they can use the coconuts to build coconut picking machines that can pick coconuts faster but those machines wear out over time and have to be replaced with new machines. This can be used to make a model that explain the role of capital in growth and the limits to that.\nNow assume that Lt is the number of workers at time t, Mt is the number of machines at time t, Ot is the output of coconuts at time t, Et is the number of coconuts consumed at time t, It is the number of coconuts invested in machines at time t, s is the savings rate and d is the depreciation rate.\nThe models has some assumptions. First, the output is increasing and concave in labour and machines, so Ot = √Lt√Mt. Concave means that the first machine is worth more than the second, the second is worth more than the third, and so on. Economists call this diminishing returns to scale. Second, the output is consumed or invested, so Ot = Et + It, where It = sOt. Third, machines can be built, but they depreciate, so Mt+1 = Mt + It - dMt.\n\n\n\n\nBasic growth\n\nAssume that there are 100 workers so the output is Ot = 10√Mt. Assume that the depreciation rate d = 0.25 and the savings rate s = 0.3. Assume that we have 4 machines in the first year. The initial output is 10 * √4 = 20. Investment will then be 0.3 * 20 = 6 machines. Depreciation will be 0.25 * 4 = 1 machine. Hence, next year the number of machines will be 4 + 5 = 9, and the output will be 10 * √9 = 30. In that year 9 new machines will be built and 2.25 will be lost due to depreciation. If we round this number to 2, we will have 16 machines the next year and output will be 100 * √16 = 40.\nThe question is whether or not this growth can continue. That is more easy to see if we consider a big number of machines, for instance 400. Output will be 10 * √400 = 200. Investment will be 60. Depreciation will be 100. Hence, 40 machines will be lost. From this one can conclude that the economy cannot grow this big by itself. Consequently, economic growth must flat out and there must be equilibrium level of GDP.\nThe long run equilibrium occurs when investment equals depreciation. Now, O = 10√M, I = 3√M and I = M/4. Now 3√M = M/4, so 12√M = M, so 3√M = 12, so M = 144. So, when the number of machines is 144, then depreciation equals investments. In this case output will be 10√144 = 120. In this case investment as well as depreciation will be 36 machines.\nThe irony of the growth model is that it isn’t really a growth model. Eventually there is no growth. This is because depreciation is linear while output is concave. In this case innovation may bring more growth. That is why innovation is so important."
  },
  {
    "objectID": "w08.html#sec-solow-growth-model",
    "href": "w08.html#sec-solow-growth-model",
    "title": "Lesson 8 - Economic Growth",
    "section": "Solow growth model",
    "text": "Solow growth model\nWithout innovation, growth will stop assuming that the amount of labour is fixed. In reality economies continue to grow. The Solow growth model adds another variable to include innovation. In this model Lt is labour at time t, Kt is capital at time t, At is technology at time t, and Ot is output at time t. The model then states that\n\\[\nO_t  = A_tK_t^{\\beta}L_t^{1-\\beta}\n\\]\n\nThe variable \\(A\\) just states how good the technology is.\nIf β = 1/2 then this is just a square root function as in the basic groth models above. If β &gt; 1/2 then capital matters more. If β &lt; 1/2 then capital matters less.\n\nIn the previous example O = 10√M, I = 3√M, I = M/4 and in equilibrium M = 144. If we now introduce an innovation, and the variable for technology A = 2, then O = 20√M, I = 6√M, I = M/4 and in equilibrium 6√M = M/4, so M = 496 and O = 2 * 10 * √496 = 480. The model shows that when productivity doubles, long run GDP becomes 4 times as big. This is because two thing happen. First, the process is becoming more productive. Second, because the process is more productive, it makes sense to invest in more machines.\nThe innovation multiplier means that if labour and capital become more productive, it makes sense to invest in more capital. A = 3, then O = 30√M, I = 9√M, I = M/4 and in equilibrium 9√M = M/4, so M = 36² and O = 3 * 10 * √36² = 1080. If we are three times as productive then the total output went up to nine times the initial output. Hence, the effect of innovation is multiplicative.\nAccording to the Solow model, if we continue to innovate and increase our productivity, then growth can continue. This raises the question of where does growth come from? In endogenous growth models, labour can go to picking coconuts to increase capital, but also into investing in new technology in order to increase the parameter A. A can be seen as a choice variable, as a corporation or a country can choose to invest in research an development.\n\n\n\n\nJapan growth rates"
  },
  {
    "objectID": "w08.html#sec-will-china-continue-to-grow",
    "href": "w08.html#sec-will-china-continue-to-grow",
    "title": "Lesson 8 - Economic Growth",
    "section": "Will China continue to grow?",
    "text": "Will China continue to grow?\nIn the past Japan had very high growth rates like China has nowadays. The question is can China sustain these levels of economic growth? The growth models show that it is dubious that China can continue to grow this fast unless they have massive increases in technological improvement.\nAs long as China is catching up with other countries, it is relatively easy to have such high growth levels, but once China has caught up with the rest of the world, it become much harder to sustain high levels of economic growth.\n Growth without innovation\nHow can we work this out in a model. Assume L = 10,000, M = 3,600, s = 0.2 * O and d = 0.1 * M, so O = √10,000 * √3,600 = 6,000. Depreciation will be 360 and investment will be 1,200, so there will be 4,440 machines in the next year. If you calculate this trough, output will be around 6,700 in the next year, a growth of 11%. The year after that, output will be around 7,300, a growth of 9%.\nWhen there are 10,000 machines, output will be 100 * 100 = 10,000, investment 2,000, depreciation 1,000, so there will be 11,000 machines in the next period. Next year, output will be 100√11,000 ≈ 10,500, a growth of 5%. If the number of machines rises, growth falls. If the number of machines is 22,500, output will go from 15,000 to 15,250, and growth will be 1.7%.\nChina has 8%-10% growth rates for over a decade. China is in the early part of the curve where there is relatively little capital relative to labour. When China is moving further along the curve, it probably will not grow so fast any more. The growth function without innovation is concave. To sustain high growth, China can’t keep on putting more money into capital, and it must innovate. But improvements in technology probably will not be enough to sustain such high levels of growth, and the picture will look like Japan."
  },
  {
    "objectID": "w08.html#sec-why-do-some-countries-not-grow",
    "href": "w08.html#sec-why-do-some-countries-not-grow",
    "title": "Lesson 8 - Economic Growth",
    "section": "Why do some countries not grow?",
    "text": "Why do some countries not grow?\nThe Solow growth model has labour, capital and technology that are combined to produce output. In this model things like equality and culture are left out. In the book @acemoglu2012nations Daron Acemoglu and James Robinson look over hundreds of years to investigate why some countries are successful and why others are not. e.g., Botswana did very well while Zimbabwe didn’t. Why is that?\nAcemoglu and Robinson think that growth requires a strong central government to protect capital and investment but that can’t be controlled by a select few. If there is no strong central government to protect property rights, whether it is physical or intellectual property, there is less incentive for people to invest and innovate. And if you have less investment and innovation, you will have lower growth @acemoglu2012nations.\nOn the other hand, the central government shouldn’t be controlled by a select few like in Zimbabwe. If that is the case then the people in control will extract resources from the economy, often in the form of bribery and corruption. Extraction limits growth by lowering investment in innovation and capital. Extraction causes less money to be available for investment as well as less incentive to invest. This has a similar effect as reducing the variable A, and if this variable is reduced to 1/3, output is only 1/9. Syphoning off money from the economy therefore has a multiplier effect 8.\n\n\n\n\nCreative destruction in music industry\n\nWhen A increases, labour becomes more productive. As\n\\[\nO_t  = A_tK_t^βL_t^{1-β}\n\\]\n, increases in A mean that less labour is required. In a real economy, if less labour is required to produce the same output, this may result in unemployment. In the long run, this is labour is expected to go into producing more capital and innovation, so that there will be more growth and unemployment will disappear again.\nWhat we learn from the model is that growth requires creative destruction, which is a term invented by Joseph Schumpeter. When A increases, whole industries may be wiped out. e.g., when the tractor was invented, blacksmiths were not needed any more.\n\n\n\n\nAd sales\n\nAnother example of creative destruction happened in the music industry. First there were vinyl records. Then came the cassette tapes. Next were the compact discs. Finally digital singles came up. Newer technologies replaced the older ones.\nThe American newspaper industry was hurt by people placing ads online. On Craigslist you can post things for sale. This company has really hurt the newspaper industry. The revenue for Craigslist went up while the revenue for the newspaper industry went down as people moved ads from newspapers to Craigslist. Craigslist has more options and it is cheaper, so it is an innovation.\nThe number of people employed by the newspaper industry dropped from 450,000 in 1988 to 275,000 in 2009. Craigslist only has 23 employees. So 23 employees wiped out 225,000 jobs. The reason why Craigslist has so few employees is that people place the ads themselves. That is called creative destruction. However the internet also created a lot of jobs. Other internet corporations like Yahoo, Google, Time Warner, Disney and Amazon hired tens of thousands of employees.\nSuppose now that the country is controlled by a few and that the newspaper industry has a lot of influence on the government. And so it might happen that the government bans advertising on the web because this saves many jobs in the newspaper industry and because the government is captured by the newspaper industry. This might be a good thing because it saves jobs but this might also be a bad thing because it lowers productivity.\nThis model may also be applied in other fields such as our personal production and income. We can work hard, but if we don’t invest in new skills and technologies, our income may level out, or even go down when our skills become obsolete. Successful people continue to learn so that they improve their personal variable A.\n\nReferences\nNote: this page is based on the following source:\n\n[@page2017modelthinking] MOOC, Course material & Transcripts.\nTA Notes by [@fisher2017modelthinking].\nStudent notes by in [@kleinikink2016naturalmoney] and [@groh2017model]."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "w13.html",
    "href": "w13.html",
    "title": "Lesson 13 - Path dependence",
    "section": "",
    "text": "Path dependence means that what happens at time \\(T\\) depends on all time steps \\(t&lt;T\\). Or put plainly that history matters.\nThis is clearly quite different from Markov process models where only the current state matters and history does not.\nUrn models can help to understand path dependence, and to distinguish between different types of path dependence. 1. deterministic outcome such as what happens today could be path dependent, 2. stochastic path dependence which is an equilibrium distribution over all possible outcomes of what happens in the long run could also be path dependent.\n\nExample 1 One of the most famous examples of path dependence involves the standard typewriter keyboard configuration called QWERTY. Initially, there were lots of different keyboard configurations, but it turned out that in the path through which history played out, QWERTY ended up getting locked in due to a process of increasing returns. The more people had QWERTY, the more people wanted it, and the more typewriters that were built with it, and so it became locked in.\n\nPath dependence means that outcome probabilities depend on the sequence of previous outcomes, which is called the path.\nPhat dependence means that outcome probabilities depend on past outcomes but not on their order. The past impacts the outcome but it doesn’t necessarily determine it.\n\nExample 2 With choices over technology such as keyboards, alternating current or direct current, or gasoline cars versus electric cars.\n\n\nExample 3 The law which evolves through precedent depends on earlier laws and their interpretation made by judges in the past.\n\nIt also the case with institutional choices like having a single payer healthcare system or defined benefits pensions versus defined contributions. These choices can depend on previous institutional choices. Economic success can also depend on sequences of past outcomes. e.g., Ann Arbor had the largest public university while Jackson had the largest prison. Population in Ann Arbor increased from 10,000 in 1910 to 110,000 in 2000 while in Jackson it first increased and then declined and grew only from 31,000 in 1910 to 36,000 in 2000. The public university greatly helped Ann Arbor.\nPath dependence often coincide with increasing returns. If you build a university then other educational institutions like hospitals, and law schools that weren’t originally part of the university, could join in. Eventually, you grow through a virtuous cycle. Increasing returns means building success on top of success. Path dependence isn’t the same as increasing returns.\nChaos is often referred to as ESTIC, or Extreme Sensitivity To Initial Conditions. Chaos means that, even when the starting points A and B are very close to each other, subsequent paths can diverge tremendously. Chaos deals with initial points while path dependence deals with the path. Path dependence in a dynamic process that has a state in each period differs from a Markov process. These path dependent processes violate the fixed transition probabilities assumption of the Markov Process. In Urn models transition probabilities change, and that is how history can matter.\n\n\n\n\nUrn model"
  },
  {
    "objectID": "w13.html#sec-path-dependent-or-tipping-point",
    "href": "w13.html#sec-path-dependent-or-tipping-point",
    "title": "Lesson 13 - Path dependence",
    "section": "Path dependent or tipping point",
    "text": "Path dependent or tipping point\nPath dependence and tipping points seem closely related concepts. Path dependence means that outcome probabilities depend on the sequence of past outcomes. There are path dependent outcomes, meaning that what happens in a given period depends on the path, and path dependent equilibria, meaning that what happens in the long run depends on what happens along the way. Tipping points are related to path-dependent equilibria.\n\n\n\n\nActive tip\n\nRecall that there are two types of tipping points:\n\nDirect/Active tip\n\na variable itself changes, which causes it to tip. In Schelling’s model this is the agent moving house.\n\nContextual tip\n\na change in the environment triggers one or more transitions in the system. In Schelling’s model this is a preferred neighbor moving out or a unwanted neighbor moving in.\n\n\nPath dependence can be related to direct tips.\nThe difference between path-dependence and tipping points is that path-dependence means that what happened along the way has an effect. Each step may have a small effect. A tipping point is a single instance in time where, where the equilibrium suddenly changes drastically. A singular event suddenly tips the system abruptly.\nTo measure tips, you can use measures of uncertainty like the diversity index, which shows the probability of different equilibria, and entropy, which measures how much information there is in the system. For the Pólya process any probability distribution of red balls is an equilibrium and equally likely. When drawing four balls, five things could happen, which are drawing zero, one, two, three or four red balls. Each is equally likely, so the probability of each option is 1/5 so the diversity index is 5.\nSuppose that the first ball is red. Then the following could happen. The probability of having four red balls is (2/3)(3/4)(4/5) = 2/5. The probability of having three red balls and one blue ball is (2/3)(3/4)(1/5)3 = (1/10)3 = 3/10 as the blue ball could be in three locations. The probability of having two red balls and two blue balls is (2/3)(1/4)(2/5)3 = (1/15)3 = 1/5 as the additional red ball could be in three locations. The probability of having four blue balls is (1/3)(2/4)(3/5) = 1/10. Now, P(4R) = 4/10, P(4R) = 4/10, P(3R) = 3/10, and P(1R) = 1/10, so the diversity index is 1/((4/10)² + (3/10)² + (2/10)² + (1/10)²) = 30/100 ≈ 3.33.\nThis movement of the diversity index suggests that something happened along the path affecting the outcome probabilities. There is path dependence, but it is not abruptly tipped. Abruptly tipped would be moving from 5 to 1 or 1.2 so that one single event would get rid of a lot of uncertainty. The difference between path dependence and tipping points is one of degree. Path dependence is more gradual. Tipping points are more abrupt.\n\n\n\n\nSchool friendship network\n\n\nReferences\nNote: this page is based on the following source:\n\n[@page2017modelthinking] MOOC, Course material & Transcripts.\nTA Notes by [@fisher2017modelthinking].\nStudent notes by in [@kleinikink2016naturalmoney] and [@groh2017model]."
  },
  {
    "objectID": "w09.html",
    "href": "w09.html",
    "title": "Lesson 9 - Diversity and innovation",
    "section": "",
    "text": "Landscape"
  },
  {
    "objectID": "w09.html#sec-problem-solving-and-innovation",
    "href": "w09.html#sec-problem-solving-and-innovation",
    "title": "Lesson 9 - Diversity and innovation",
    "section": "Problem solving and innovation",
    "text": "Problem solving and innovation\nThere are several issues regarding to problem solving, e.g. how individuals and teams go about solving problems, the role of diversity in problem solving, how ideas can get recombined, and how a lot of innovation actually comes from somebody having an idea in one place and it being applied someplace else. The two main themes are the role of diversity and the power of recombination.\nWe can model problem solving in a formal fashion. Assume that you take some action a and there is a payout function F, that gives the value of that action F(a). e.g., a could be a health care policy, and F could be how efficient this health care policy is. So a is the solution that you propose and F(a) is how good the solution is.\nWe want to understand how people come up with better solutions, hence where innovation comes from. For this, we use the metaphor of a landscape as a lens through which to interpret our models. Each solution has a value and the altitude can be seen as the value of it. Here B is the best possible solution. Assume that I have some idea X, which is represented by the black dot. It may be a good idea.\nI might be looking for better ideas by going up and down the slope. In this way I well arrive at C and here I will get stuck. What we want to see is how people come up with these ideas, how teams of people come up with better ideas, and how we can avoid getting stuck on C, and possibly get to B. How do we model this?\nThe first part of the model is perspectives. A perspective is how you represent or encode a problem. So if someone poses some problem to you, whether it is software code, designing a bicycle, or a health care policy, you have some way of representing that problem in your head. Once you have encoded the problem, you can create a metaphorical landscape with a value for each possible solution. Different perspectives give different landscapes.\nThe second part of the model is heuristics. Heuristics define how you move on the landscape. Hill climbing is one heuristic. Random Search would be another heuristic. Different perspectives and different heuristics allow people to find a better solutions to problems. Individuals have perspectives and heuristics.\nTeams of people are better in solving problems than the individuals in it because they have more tools, and those tools tend to be diverse. They have different perspectives and different heuristics, and all that diversity makes them better at coming up with new solutions and better solutions to problems.\nRecombination means that solutions for different problems from different people can be combined to produce even better solutions for those problems. Sophisticated products like a houses, automobiles and computers consist of all sorts of solutions to sub-problems. By recombining solutions to sub-problems we get ever better solutions, and that is really a big driver of innovation, and growth depends on sustained innovation."
  },
  {
    "objectID": "w09.html#sec-perspectives-and-innovation",
    "href": "w09.html#sec-perspectives-and-innovation",
    "title": "Lesson 9 - Diversity and innovation",
    "section": "Perspectives and innovation",
    "text": "Perspectives and innovation\nWhen you think about a problem, a perspective is how you represent it. Landscape is a way to represent the solutions along the horizontal axis and the value of these solutions as the height. The landscape metaphor can be formalized into a model. In this model a perspective is a representation or encoding of the set of all possible solutions. Then we can create our landscape by assigning a value to each one of those solutions. The right perspective depends on the problem.\n\n\n\n\nPerspectives\n\ne.g., there are different ways of representing a point. The first method is Cartesian, using the horizontal x and vertical y coordinates. The second method is the polar method using the radius r and the angle Θ. The question is which is better? If you want to represent a line, Cartesian may be better. e.g., you could define the line as y = 5. If you want to describe an arc, polar is better.\nPerspectives help us find solutions to problems and to be innovative. In the history of science a lot of great breakthroughs, such as Newton’s theory of gravity, are new perspectives on old problems.\ne.g., Mendeleev came up with the periodic table, where he represented the elements by atomic weight. In doing so, he found all sorts of structures, e.g. all the metals being lined up in certain columns. He could have organized them alphabetically, but that wouldn’t have made much sense. Atomic weight representation gives a lot of structure. When Mendeleev wrote down all the elements that were around at the time according to atomic weight, there were gaps in his representation. They were eventually found years later. The perspective of atomic weight was useful because it made people look for missing elements.\nWe use different perspectives all the time. e.g., when evaluating applicants for a job, you might look at competence or achievement in the form of grade point average (GPA), work ethic in the form of thickness of the resume, or creativity as indicated by the colorfulness of the personality. Depending on what you’re hiring for, any one of these might be fine. All these ways of organizing applicants are perspectives.\n\n\n\n\nCaloric landscape\n\nPerspectives will determine how hard the problem will be for you using the landscape metaphor. If the landscape is rugged, there are lots of peaks. To formalize this notion of peaks, we define local optima. A local optima is an action a where all neighboring actions have lower values, so that it is a peak on the landscape. So, if you look in all directions, there seems not to be a solution that is better. Better perspectives therefore have fewer local optima.\ne.g., I’m tasked with inventing a new candy bar. There are many different options and I want to find the very best one. One way to represent those candy bars might be by the number of calories. In this case there may be three local optima. Alternatively, I might represent those candy bars by masticity, which is chew time. Chew time probably isn’t the best way to look at candy bars so, as a result this produces a landscape with many more peaks.\n\n\n\n\nMt. Fuji landscape\n\nThe best perspective would be a Mount Fuji landscape, the ideal landscape that just has one peak. An example of the Mount Fuji landscape, comes from scientific management. Frederick Taylor solved for the optimal size of a shovel.\nSuppose we’re shovelling coal and I want to figure out how many pounds of coal can one shovel in a day as a function of the size of the pan. The larger the shovel gets, workers can shovel more coal, until the shovel gets too big and too heavy to lift. The shovel landscape is therefore single peaked and easy to solve. You are only certain to find a solution if the landscape is single peaked. If there are many peaks, you can easily get stuck on some local peak.\n\n\n\n\nSum to 15\n\nHerb Simon developed Sum to Fifteen to show people why different ways of representing a problem can make them more easy and make them like Mount Fuji, or can make them really difficult. Sum to fifteen works as follows. There are cards numbered from one to nine face up on a table. There are nine cards numbered from 1 to 9. There are two players. Each person takes turns, taking a card, until all the cards are gone. Possibly it could end sooner. If a player ever holds three cards that add up to exactly 15, he or she wins.\n\n\n\n\nMagic square\n\nAssume Peter and David play this game. Peter goes first and takes the four. David goes next so he takes the five. Peter then takes the six. David then takes the eight. Eight plus five equals thirteen so that Peter has to take the two. Four plus two is six. So if David doesn’t take the nine, he’s going to lose. But six plus two is eight. So if David doesn’t take the seven, he’s going to lose. In this case Peter has won.\nThis game can be seen in a different perspective using the magic square where every row, column and diagonal adds up to fifteen. Peter goes first, and takes the four. David goes next, and takes the five. Peter takes the six, which is an odd choice, because now he can’t win. David then takes the eight. Peter blocks him with the two. But now it turns out, either the nine or seven will let Peter win. What game is this? This is tic-tac-toe.\nSum to fifteen is just tic-tac-toe, but on a different perspective. If you move the cards into the magic square, you create a Mount Fuji landscape. You make the problem really simple. So a lot of great breakthroughs, like the periodic table, Newton’s Theory of Gravity, are perspectives on problems that turned something really difficult to figure out into something that suddenly made a lot of sense.\nIn his book The Difference, Professor Page discusses the Savant Existence Theorem, which states that for any problem, there exists some way to represent it, so that it can be turned into a Mount Fuji problem. All you have to do is, is to put the very best solution in the middle, put the worst ones at the end, and line up the solutions in such a way so that you turn it into a Mount Fuji. In order to make the Mount Fuji, you would have to know the solution already. This isn’t a good way to solve problems. But there is such a perspective, and if you change your perspective, you might find it, and in this way you might find the solution 9.\nThere are a large amount of bad perspectives. With \\(N\\) alternatives, you have \\(N!\\) ways to create one dimensional landscapes. Suppose I have just ten alternatives and I want to order them. here’s ten things I could put first, nine things I could put second, eight things I could put third and so on. So there are \\(10 × 9 × 8 × 7 × 6 × 5 × 4 × 3 × 2 × 1\\) perspectives. Most of those are not very good because they are not going to organise this set of solutions in any useful way. Only a few of them are going to create Mount Fujis. If we just think in random ways, we’re likely to get a landscape that’s so rugged that we’re going to get stuck just about everywhere."
  },
  {
    "objectID": "w09.html#sec-heuristics",
    "href": "w09.html#sec-heuristics",
    "title": "Lesson 9 - Diversity and innovation",
    "section": "Heuristics",
    "text": "Heuristics\nHeuristics is about finding solutions to problems once they have been represented in a perspective. A heuristic is a technique in which you look for new solutions. An example is hillclimb, which is moving locally to better points. It is one of many possible heuristics. Heuristics are defined relative to the problem to be solved. e.g., do the opposite is one famous heuristic that’s in a lot of books on how to innovate. It means to do the exact opposite of the existing solution.\ne.g., when you go to buy something, the seller tells you the price. Do the opposite would be that the buyer tells the price. A lot of companies have been starting to do exactly this. So Priceline, lets buyers go to hotels and tell them how much they would like to pay to stay at the hotel or to use the airline. Alternatively, companies can go for lower costs or do the opposite, and charge a high price to signal quality. Doing the opposite can sometimes lead to interesting innovations.\nAnother heuristic is big rocks first. If you have a bucket and a bunch of rocks of various sizes, it is better to put in the big rocks first, because then you have a bigger chance of putting all the rocks in the bucket. The little rocks can fill in the gaps. In his book The 7 Habits of Highly Effective People, Stephen Covey argues that this is one heuristic successful people use. The big rocks represent the important things. It means that you solve the important issues first, and then arrange the solutions of the less important things around them, you will find better solutions 10.\nBut there is a drawback. There’s a famous theorem in computer science called the no free lunch theory, proved by Wolpert and McCready. All algorithms that search the same number of points with the goal of locating a maximum value of a function defined on a finite set perform exactly the same when averaged over all possible functions. This means that some of these problems are incredibly hard and some are really easy so that no heuristic is any better than any other.\nThis doesn’t mean that Covey is wrong. The free lunch theory states that if you look across all problems, no heuristic is better than the other. Covery spent a lot of time in management and he thinks that management problems lend themselves to the big rock search first heuristic. Another way of looking at the free lunch theory is that, unless you know something about the problem being solved, no algorithm or heuristic performs better than any other. Once you know something about the problem, you might decide that big rocks first does a good job in solving it. When you’re digging a hole in the ground then little rocks first may be a better solution.\n\n\n\n\nCombining heuristics\n\nDiverse heuristics or combining heuristics can be useful in finding solutions to problems. Suppose I have a choice based on two dimensions, e.g. ice creams that have a size and a number of chocolate chips. The heuristic to find the best taste is to look for a better option in horizontal and vertical directions. Another heuristic is to look in diagonal directions. If you combine those heuristics, you check more options and have a better chance of succeeding. Diverse perspectives plus diverse heuristics enable groups of people to find better solutions to problems."
  },
  {
    "objectID": "w09.html#sec-teams-and-problem-solving",
    "href": "w09.html#sec-teams-and-problem-solving",
    "title": "Lesson 9 - Diversity and innovation",
    "section": "Teams and problem solving",
    "text": "Teams and problem solving\nPerspectives and heuristics can be used to show why teams of people often can find solutions to problems that individuals can’t. That’s why teams are better. The term teams is used in a very loose sense. e.g., some person invented the toaster. Then somebody else improved it. Then somebody come up with the crumb tray. Then somebody else came up with the automatic shut off. Others came up with further improvements\nWhy are groups of people better than individuals? Think about the candy bar example. One perspective was based on calories. It had three local peaks. Let’s call them A, B and C. Another landscape based on masticity had five peaks. Let’s call these A, B, D, E and F. These peaks are different than the peaks for the caloric landscape, with the exception of A and B. A is the best possible point. The best possible point has to be a point in every landscape. Mr. Page doesn’t explain why this is so.\nThe caloric landscape is better than the masticity landscape because it has fewer local optima. The heuristic is just hill climbing. The peaks where people get stuck are A, B, C, D, E, and F. We can assign a value to each of those peaks. Suppose A is the global optimum, and some of these other peaks aren’t so good. We can ask what’s the average value of a peak for the caloric problem solver? It is the average of A, B and C. \\(A = 10, B = 8 and C = 6\\), so the average is 8, which is the ability of the caloric problem solver. You can do the same for the masticity problem solver. If \\(A = 10, B = 8, D = 6, E = 4, F = 2\\) then this ability is 6.\nThe caloric problem solver had fewer local optima, but also a higher average, so this is another reason why the caloric problem solver is better. The caloric problem solver may get stuck at B, and then pass the problem on to the masticity problem solver. He will then say that B looks good. If the caloric problem solver gets stuck at C, this point doesn’t look good for the masticity problem solver. This person can get from C to some other local optima. If that is D, E, or F, then it doesn’t look good to the caloric problem solver. Consequently, the team will get stuck at A or B, which is a better outcome. The average is 9, so the ability of the team is 9.\nThe ability of the team is higher than the ability of either person. This is because the team’s local optima is the intersection of the local optima for the individuals. This is why over time products get better, and why teams are innovative. The reason why a lot of science is done by teams is because the only place a team can get stuck is where everybody on the team can get stuck. This simple model of perspectives and heuristics can explain why teams are better than individuals and why, over time, we keep finding better and better solutions to problems.\nThe big claim is that the team can only get stuck at a local optima for everyone on the team. That means the team is better than the people in it. Therefore it is better to have people with different local optima, diverse perspectives and diverse heuristics. And that diversity produces different local optima, and those different local optima will mean that the intersections are taken, so that we end up with better points.\nWhat’s missing? This model is highly stylised. Two things are left out. First, there is communication. The model assumed that team members communicate their solutions to one another right away. That is not always the case. There are a lot of misunderstandings and people might not listen. If you make a better product, for instance a better toaster, this could be the way of communicating. Second, There might be an error in interpreting the value of a solution. If a good proposal is made, others can think that it is a bad idea. The model assumes that there is no error in assessing the value of a solution.\nIn a more advanced model, there could be room for communication error and errant evaluation. That is going to hurt the case for using teams. Even so, this model has shown us something fairly powerful, which is that diverse representations of problems in diverse ways of coming up with solutions can make teams of people better able at coming up with solutions than individuals. And it gave an indication where innovation is coming from. Innovation is coming from different ways of seeing problems and different ways of finding solutions."
  },
  {
    "objectID": "w09.html#sec-recombination",
    "href": "w09.html#sec-recombination",
    "title": "Lesson 9 - Diversity and innovation",
    "section": "Recombination",
    "text": "Recombination\nUntil now we focused on individual problems and individual solutions. Recombination is combing a solution or a heuristic to come up with even more solutions or more heuristics. Recombination is an incredibly powerful tool for science, innovation and economic growth. If we have a few solutions or a few heuristics, then we can combine those to create more. The real driving force behind innovation in the economy is that when we come up with a solution and then recombine it with all sorts of other solutions.\ne.g., fill in the missing number. 1 2 3 5 _ 13, the missing number is 8, because you add those numbers up, or substract them if you go back. 1 4 _ 16 25 36, the missing number is 9 because it is the square of the next number, which is 3. 1 2 6 _ 1806, the missing number here is 42. The solution is harder to find because you have to combine the first two techniques. \\(2 - 1 = 1 = 1², 6 - 2 = 4 = 2², 42 - 6 = 36 = 6², 1806 - 42 = 1764 = 42²\\).\nRecombining is a driver of economic growth and also of science because when a new solution can be combined with other solutions. This produces a geometric explosion in the number of possibilities. This may be the reason why it was possible to sustain economic growth by increasing the technology parameter A.\nTo show how this works, let’s start with finding out how many ways there are to pick three objects from ten. There are 10 I can pick first, 9 I can pick second and 8 I can pick third. 10 * 9 * 8 is too much because picking A, B and C is the same as picking B, A and C. There are three things I can pick first, 2 I can pick second, and 1 I can pick last. Hence, the answer is (10 * 9 * 8) / (3 * 2 * 1) = 120. With far more than 10 solutions, the number is very big. e.g., if you have 52 cards, and you want to combine 20 of them, then the number of combinations becomes (52 * 51 * … * 33) / (20 * 19 * … * 1) ≈ 125,000,000,000,000.\nThis idea of ideas building on ideas is the foundation of the theory of recombinant growth of Martin Weitzman. This theory states that ideas get generated all the time. e.g., the steam engine gets invented and developed, the gasoline engine gets developed, the microprocessor gets developed. And all these things get recombined into interesting combinations. And those combinations, in turn, get recombined to create ever more growth. e.g., many parts in the steam engine were solutions to previous problems. The same applies to the desk top computer.\nAll those parts of the steam engine weren’t developed the steam engine in mind. They were developed for other purposes. This is an idea from biology called exaptation. The classic example of exaptation, is the feather. Birds developed feathers primarily to keep them warm, but eventually those same feathers allowed them to fly. Expectation means that some innovation for one reason, gets used in another context. Another example is the laser. The laser was not invented with the idea of laser printers in mind. So once something is developed, it gets used for all sorts of unexpected things through the power of recombination.\nThis also applies to perspectives, e.g. the masticity perspective of a candy bar. Masticity can be a useful perspective for other problems, e.g. pasta or breakfast cereal. So even failed solutions for one problem may work well as a solution to other problems. e.g., the glue in the post it note was originally a failure because the glue didn’t stick very well. But it turned out to be useful for other sorts of problems, mainly making sticky notes.\nThere is more to it than this. It’s not just the recombination of ideas, because for hundreds and thousands of years people had ideas. There had to be some way to communicate those ideas. In @mokyr2011gifts the economist Joel Mokyr, argues that the rise of modern universities, the printing press, and scientific communication allowed ideas to be transferred from one location and one person to another. The technological revolution was driven by the fact that people could share ideas and then recombine them.\n\nReferences\nNote: this page is based on the following source:\n\n[@page2017modelthinking] MOOC, Course material & Transcripts.\nTA Notes by [@fisher2017modelthinking].\nStudent notes by in [@kleinikink2016naturalmoney] and [@groh2017model]."
  },
  {
    "objectID": "w06.html",
    "href": "w06.html",
    "title": "Lesson 6 - Categorical and Linear Models",
    "section": "",
    "text": "Categorical models allocates the data into different boxes. For example, trying to understand why some people live longer than others, we could divide them into people who exercise and people who do not exercise. By looking at the variation and the mean values in those boxes, you might find out whether or not this categorization says something about how long people live.\n\n\n\n\nConstructing linear model from data\n\nLinear models assume that some variable \\(Y\\) depends on variable \\(X\\), so \\(Y\\) is a function of \\(X\\) or \\(Y = f(X)\\).\nFor instance, if \\(Y\\) is life expectancy, \\(X\\) might measure physical exercise. Thus, life expectancy is a function of how much a person exercises.\nAn important question is how we produce the linear model from the data. The line in the graph must represent the data as well as possible.\nNonlinear models could be represented by functions that do not produce a straight line, e.g., exponential growth functions. The diversity of nonlinear functions are enormous.\nThe big coefficient refers to using the coefficient corresponding to the main effect size to make decisions. e.g., a function of school quality might be\n\\[\nY = a_1X_1 + a_2X_2\n\\]\nWhere:\n\n\\(X_1\\) might be the class size and\n\\(X_2\\) might be teacher quality.\n\nHere \\(a_1\\) and \\(a_2\\) are the coefficients. These coefficients tell how important the variable is, so the bigger the coefficient, the more important the variable.\nLinear models can lead to big coefficient thinking, and if these are valid, then that will serve decision-makers better than rules of thumb, intuition, or experience. However, as time passes, reusing the main effect of an old model may be inadequate. There could be data drift, or alternatively, any improvement due to the main effect may have petered out due to diminishing returns. Over time, overhalling dysfunctional policies can require new and radical thinking. Approaches like revisiting old assumptions and rethinking out of the box can lead to something new, which we call new reality thinking. A problem with big coefficient thinking is that it only works in areas with specific data available. Over time, new data may even switch the size of the coefficients, leading to a loss of confidence in the model. Significant breakthroughs are often made by shifting to areas without data. A “new reality” will arise when the projections of big coefficients or even the assumptions behind them can be seen as inadequate, and decision-makers decide to rebuild a new model. ::: callout-warning ### Big Coefficient Warning:\nRegression and Linear models can have negative coefficients, interaction terms and so on, which, with one negative term, can render the “big coefficient.” approximation or decision wrong. :::\n\\[\nY = 6 X_1 + 6 X_2 - 8 X_3 \\\\ \\text{subject to} (X_3 = max({X_1,X_2}))\n\\]\nWe want to increase \\(Y\\). The big coefficient is \\(X_3\\), but it reduces \\(Y\\). Reducing \\(X_3\\) will reduce \\(Y\\) since it also reduces \\(X_1, X_2\\) to increase. Increasing \\(Y\\), we need to increase \\(X_1\\) and \\(X_2\\) together as increasing just one will increase \\(X_3\\) and thus decrease \\(Y\\). The bottom line is that under certain conditions not listed by Dr. Page, the big coefficient is a useful approximation, but this assumption is not generally true in regression models. Dr. Page introduces the concept of a “new reality” to counter the shortcomings of big coefficient thinking. Unfortunately, the new reality that was introduced seems to have arisen from a failure. Later, he makes a case that “new reality” is a policy based on some other effect. However, these new reality initiatives seem to have no evidence to back them up. So this seems like muddy thinking rather than Clear thinking"
  },
  {
    "objectID": "w06.html#sec-introduction-to-categorical-linear-and-non-linear-models",
    "href": "w06.html#sec-introduction-to-categorical-linear-and-non-linear-models",
    "title": "Lesson 6 - Categorical and Linear Models",
    "section": "",
    "text": "Categorical models allocates the data into different boxes. For example, trying to understand why some people live longer than others, we could divide them into people who exercise and people who do not exercise. By looking at the variation and the mean values in those boxes, you might find out whether or not this categorization says something about how long people live.\n\n\n\n\nConstructing linear model from data\n\nLinear models assume that some variable \\(Y\\) depends on variable \\(X\\), so \\(Y\\) is a function of \\(X\\) or \\(Y = f(X)\\).\nFor instance, if \\(Y\\) is life expectancy, \\(X\\) might measure physical exercise. Thus, life expectancy is a function of how much a person exercises.\nAn important question is how we produce the linear model from the data. The line in the graph must represent the data as well as possible.\nNonlinear models could be represented by functions that do not produce a straight line, e.g., exponential growth functions. The diversity of nonlinear functions are enormous.\nThe big coefficient refers to using the coefficient corresponding to the main effect size to make decisions. e.g., a function of school quality might be\n\\[\nY = a_1X_1 + a_2X_2\n\\]\nWhere:\n\n\\(X_1\\) might be the class size and\n\\(X_2\\) might be teacher quality.\n\nHere \\(a_1\\) and \\(a_2\\) are the coefficients. These coefficients tell how important the variable is, so the bigger the coefficient, the more important the variable.\nLinear models can lead to big coefficient thinking, and if these are valid, then that will serve decision-makers better than rules of thumb, intuition, or experience. However, as time passes, reusing the main effect of an old model may be inadequate. There could be data drift, or alternatively, any improvement due to the main effect may have petered out due to diminishing returns. Over time, overhalling dysfunctional policies can require new and radical thinking. Approaches like revisiting old assumptions and rethinking out of the box can lead to something new, which we call new reality thinking. A problem with big coefficient thinking is that it only works in areas with specific data available. Over time, new data may even switch the size of the coefficients, leading to a loss of confidence in the model. Significant breakthroughs are often made by shifting to areas without data. A “new reality” will arise when the projections of big coefficients or even the assumptions behind them can be seen as inadequate, and decision-makers decide to rebuild a new model. ::: callout-warning ### Big Coefficient Warning:\nRegression and Linear models can have negative coefficients, interaction terms and so on, which, with one negative term, can render the “big coefficient.” approximation or decision wrong. :::\n\\[\nY = 6 X_1 + 6 X_2 - 8 X_3 \\\\ \\text{subject to} (X_3 = max({X_1,X_2}))\n\\]\nWe want to increase \\(Y\\). The big coefficient is \\(X_3\\), but it reduces \\(Y\\). Reducing \\(X_3\\) will reduce \\(Y\\) since it also reduces \\(X_1, X_2\\) to increase. Increasing \\(Y\\), we need to increase \\(X_1\\) and \\(X_2\\) together as increasing just one will increase \\(X_3\\) and thus decrease \\(Y\\). The bottom line is that under certain conditions not listed by Dr. Page, the big coefficient is a useful approximation, but this assumption is not generally true in regression models. Dr. Page introduces the concept of a “new reality” to counter the shortcomings of big coefficient thinking. Unfortunately, the new reality that was introduced seems to have arisen from a failure. Later, he makes a case that “new reality” is a policy based on some other effect. However, these new reality initiatives seem to have no evidence to back them up. So this seems like muddy thinking rather than Clear thinking"
  },
  {
    "objectID": "w06.html#sec-categorical-models",
    "href": "w06.html#sec-categorical-models",
    "title": "Lesson 6 - Categorical and Linear Models",
    "section": "Categorical models",
    "text": "Categorical models\nCategorical models allocate the data into different groups. Variation in and between groups can be capture much of the overall variation in variations in the data.\n\nExample 1 e.g., at the time of the IPO of Amazon, most analysts catagorised it as a delivery service like DHL or FedEx, and because the delivery buiness was such a cut-throat bussiness companies in it have low-profit margins so they thought Amazon would be a bad investment. A few analysts thought Amazon would be a good investment because they saw Amazon as part of the new information economy. Differnt analysts put Amazon in a different boxes. Amazon did badly for over a decade then they started letting computers a totatly differnt business model and then they did very well, so which box we choose matters for our investment portfollio and also which story you choose to tell. :::: {callout-info} ### dot.com crash & the information economy. 💩 💩 💩\nAny time I hear the concept of “the information economy” I am sad to recall how this “rethoric for a new economy” died with out when most of the dot.com companies burt through their VC/IPO money through incompetency or corrupt management caused a global recession. So I think Dr. Page is being disingenious by picking this example.\nI also recall Amazon had zero or negative profits for over a decade after the doc.com crash. It overhauled its bussines concept servral times before it became a “good” investment and is facing stiff competition in its primery market from chinese stores. Also over times there are many new stories coming out how Amazon is making profits by squeezing it workforce to work harder and for less that DHL and FedEx which are both partners and competitors.\nFinaly the question this example raises is: How can we find build a model that knows to place novel items accurately into new boxes without any other examples. Because model are supposed to better than analysts. In reality categorical model are not as resourceful as an analyst, they can’t invent new catergories. Event if is add some extra lables, without examples the model wont assign new items to these category.\n\n::: Lump to live means that we create categories to make sense of the world. For example, if we see a car, we do not say there goes a 2003 Volkswagen 1.6 GTI, but we say car. We model to decide, plan, and design. One reason we lump is to make faster decisions. For example, a child may have a rule not to eat green items. This helps the child avoid eating grasshoppers, which it does not like to eat. However, the rule is not optimal. The child might forego a juicy pear in this way.\n\n\n\n\nCalories per food item\n\nFor example, you may have a list of calories per food item. We want to make sense of why some of these items have many calories and why others have not. The first thing we want to figure out is the average value . How much variation is there in the data? The mean is \\((100 + 250\n+90 + 110 + 350) / 5 = 180\\) calories.\nOne way of determining the variation is the difference between every value and the mean. We take absolute values because these differences cancel each other out. Hence, the total difference from the mean is \\(80 + 70 +90 + 70 + 170 = 480.\\)\n\n\n\n\nSquare differences\n\nIn statistics, the differences are squared to make every value positive and to amplify more significant deviations. e.g., for the pear we get \\((100 - 180)^2 = 6,400\\), and for the pie we get 28,900. It shows that substantial deviations from the mean greatly impact the total variation.\nThe obvious categorization is that pears, apples, and bananas are fruit, while cakes and pies are desserts. The fruit values are 90, 100, and 110, with a mean of 100 and a total variation of 200, while the dessert values are 250 and 350, with a mean of 300 and a total variation of 5,000.\nTotal variation went down from 53,200 to 5,200. Hence, the categories substantially reduce the amount of variation. Variation can be seen as unexplained, so the categorization explained a lot.\nThe total variation explained is\n\\[\n(1 - 5,200/53,200)*100% = 90.2%\n\\tag{1}\\]\nThis is called the R-squared.\n\nIf the R-squared is near 1, the model explains a lot of the variance in the data\nif it is near 0, it explains very little of the variance in the data\n\nSometimes, there is so much variation in the data that great models only have an R-squared of 5% to 10%. Sometimes, the situation is clear-cut, and the R-squared is nearly 90%. There is no standard measure for the R-squared of a good model, but in a class of models, a higher R-squared means that the model is better.\nExperts tend to have many useful boxes. The previous example of fruits and desserts could be enhanced with categories like vegetables and grains. If you want to be good at understanding how the world works, you need to have a lot of categories, which must also be helpful and explain much variation.\nEven if the model explains a lot of variation, it does not mean that the model is good. If we want to figure out what determines good school performance, and we make a distinction between schools with an equestrian team and those without one, we might find out that schools with equestrian teams do better. However, that does not mean that the equestrian team made the school good. Correlation is not causation.\n\n\n\n\nLinear function"
  },
  {
    "objectID": "w06.html#sec-linear-models",
    "href": "w06.html#sec-linear-models",
    "title": "Lesson 6 - Categorical and Linear Models",
    "section": "Linear models",
    "text": "Linear models\nLinear models assume that there is some independent variable X and a dependent variable Y that is a function of X where \\(Y = mX + b\\). This can be plotted in a graph. The value of m determines the slope. There are two questions: 1. sign of the coefficient: does Y increase or decrease with X? 2. magnitude of the coefficient: how much does Y increase for each unit increase in X?\nModels are used to predict and to understand data. Assume we have a linear model for the price of a television: \\(cost = \\$15 \\times length + \\$100\\). So suppose we want to buy a 30-inch TV, then we can predict that it will cost $550. In this fashion, we can also try to predict the prices of counterfactual products, like a 100-inch TV.\n\n\n\n\nLinear data\n\nOne way to understand data is to make a line that fits the data. In this way, you can make a linear model that makes sense of the data and makes predictions. Even simple linear models are better than people. Professor Robyn Dawes wrote a paper comparing very primitive linear models.\nOne of his examples was 43 bank loan officers predicting which 30 of 60 companies would go bankrupt based on their financial statements. The bankers were 75% accurate, but a simple linear model based on the ratio between assets and liabilities was 80% accurate. In similar studies, experts did not do better than simple linear models."
  },
  {
    "objectID": "w06.html#sec-fitting-lines-to-data",
    "href": "w06.html#sec-fitting-lines-to-data",
    "title": "Lesson 6 - Categorical and Linear Models",
    "section": "Fitting lines to data",
    "text": "Fitting lines to data\nAn important question is how much variation can be explained using the model. If we take just the mean and calculate the variation, then there would be much variation. If we draw the right line, it is possible to explain 87.2% of the variation. The question is how to draw the best line.\n\n\n\n\nLinear model\n\nLet us do a simple example. Suppose we have three kids, and we want to figure out how much their grade explains their shoe size. We can see shoe size as a function of grade. The values for grade and shoe size for the three kids are (1,1), (2,5) and (4,9). The total variation from the mean 5 is: \\((5-1)^2 + 0 + (1-5)^2 = 32\\).\nThe goal is to make a linear model that explains as much of the variation as possible. Try $Y = 2X \\(. The variation would be\\)(1-2)^2 + (4-5)^3 + (9-8)^2 = 3$. In this way, \\((1 - 3/32) = 29/32\\) of the variation would be explained, which is over 90%. Nevertheless, this is just a guess.\n\n\n\n\nLinear model\n\nSo, how do we fit the best line?\nAssume the line to be \\(Y = mX + b\\). Here \\(b\\) is called the intercept, which is the value for \\(Y\\) when \\(X = 0\\). How big is the total variation? How far would that line be from the data? That is \\((m + b - 1)^2 + (2m + b - 5)^2 + (4m + b - 9)^2 = 21m^2 + 14mb + 3b^2 - 94m - 30b + 81\\). It is now possible to find the values for \\(b\\) and \\(m\\) to make the total variation the smallest.\nThese values are: \\(b = -1\\) and \\(m = 8/3\\) so that \\(Y = -1 + (8/3)m\\). In this case, the total variation would be \\((2/3)^2 + (2/3)^2 + (2/3)^2 = (4/9)\\times3 = 4/3\\). This would explain \\((1 - (4/3)/32)\\) of the variation, which is over 95%. The line is now even closer to the data.\nSuppose there are multiple variables. e.g., the test score \\(Y\\) of kids might depend on their IQ \\(Q\\), teacher quality \\(T\\) and class size \\(Z\\) in the following way: \\(Y = a + bQ + cT + dZ\\). We expect the coefficient \\(d\\) on class size to be negative and \\(b\\) as well as \\(c\\) to be positive.\nOf the 78 studies on class size, 4 showed a positive coefficient, 13 had a negative coefficient, and 61 showed no effect. Even though we think that class size matters, it appears not to matter much, at least within the range that has been studied. Research has shown that teacher quality matters far more.\nWe can fit data in linear models. These models can often explain some percentage of the variation. The models also show us the sign and the magnitude of the coefficients. This shows whether the variable has a positive or a negative effect and how big that effect is, allowing us to make policy choices.\n\n\n\n\nRegression output"
  },
  {
    "objectID": "w06.html#sec-reading-regression-output",
    "href": "w06.html#sec-reading-regression-output",
    "title": "Lesson 6 - Categorical and Linear Models",
    "section": "Reading regression output",
    "text": "Reading regression output\nSomebody may give we some regression output, and we may have to make sense of this. When we see regression output, then we deal with a linear model based on multiple variables and functions like\n\\[\nY = m_1X_1 + m_2X_2 + \\ldots + m_nX_n + b.\n\\]\nWith multiple regression output, there is more than one \\(X\\) variable.\ne.g., \\(Y\\) could be a test score depending on teacher quality \\(T\\) and class size \\(Z\\) so that the model function could be \\(Y = cT + dZ + b\\). We could expect that better teachers lead to better results so that \\(c &gt; 0\\) and bigger classes lead to poorer results so that \\(d &lt; 0\\).\nIn the regression output, the standard error is 24.21, which tells us, on average, how much the values differ from the mean. The value of \\(R^2 = 0.72\\), i.e., 72% of the variation, is explained by the linear model. There were 50 data points. The coefficient column tells us that the model is \\(Y = 20X_1 + 10X_2 + 25\\).\nIf \\(X_1\\) is teacher quality and \\(X_2\\) is class size, then the positive value for the class size coefficient raises some questions. Maybe the data is wrong, or maybe our intuition is wrong. So, we have to dig a little deeper. The first issue is that we have only 50 observations, so the coefficient may not be correct. The standard error (SE) column gives the error in the coefficients.\nFor the intercept, the coefficient is 25, and the standard error is 2, so 68% of the values are expected to be between 23 and 27. Hence, we could be really sure that the coefficient is between 19 and 31. Similarly, we could be really sure that \\(X_1\\) is between 17 and 23, and \\(X_2\\) is between -2 and 22. The p-value gives the probability that the sign of the coefficient is wrong. We should not be so sure about the positive impact of class size on school performance.\nThe important issues to deal with are the following:\n\nHow good is the model? How much of the variation does it explain?\nsign of the coefficient: does Y increase or decrease with X?\nmagnitude of the coefficient: how much does Y increase for each unit increase in X?\nwhat is the probability that the coefficient is wrong?\n\n\n\n\n\nApproximate by breaking up in quadrants"
  },
  {
    "objectID": "w06.html#from-linear-to-nonlinear",
    "href": "w06.html#from-linear-to-nonlinear",
    "title": "Lesson 6 - Categorical and Linear Models",
    "section": "From Linear to Nonlinear",
    "text": "From Linear to Nonlinear\nThere is a problem with linear regression. Phenomena in the real world are often nonlinear. The amount of nonlinear functions is enormous compared to linear functions. All kinds of graphs could be based on functions that are exponential, logarithmic, something else, or mixed.\nHow can we best-fit lines in messy situations if we only have techniques for using linear functions?\nThere are three approaches to get around this, which:\n\napproximate nonlinear models with linear functions: we could use multiple linear functions to approximate a nonlinear model. Hence, we get different linear models for different ranges.\nbreak up the data into different quadrants: we can draw lines in each quadrant that match the data as closely as possible, and to make the line continuous, we may have to forego drawing an optimal line for each quadrant separately.\ninclude onlinear terms, e.g.\n\n\\[\nY = m\\sqrt{X} + b. \\qquad\n\\]"
  },
  {
    "objectID": "w06.html#the-big-coefficient-versus-the-new-reality",
    "href": "w06.html#the-big-coefficient-versus-the-new-reality",
    "title": "Lesson 6 - Categorical and Linear Models",
    "section": "The Big Coefficient versus the New Reality",
    "text": "The Big Coefficient versus the New Reality\nIf we have some model like \\[\nY = m_1X_1 + m_2X_2 + b\n\\]\nWhere:\n\n\\(Y\\) might be sales,\n\\(X_1\\) might be web advertising, and\n\\(X_2\\) might be print advertising.\n\nIf the coefficient is \\(m_1 &gt; m_2\\), then we should advertise on the web rather than print. The coefficient is an evidence-based rank, and it is applied, for instance, in medicine, philanthropy, education, and management. Linear models based on evidence are better than guessing.\nEvidence-based thinking with models works as follows:\n\nconstruct a model;\ngather data;\nidentify important variables;\nchange those variables;\n\nData science has seen a movement from the statistics era, in which data acquisition was expensive. However, the analysis was cheap, to an age where data is cheap, but the analysis is expensive. This has been called a movement towards big data. The big data approach is that we can begin with lots of data and fit the model later. Instead, we could do the following:\n\ngather data;\nfind patterns;\nidentify important variables;\nchange those variables.\n\nBig data does not make models obsolete. Models help to understand how the world works. Identifying the patterns is not the same as understanding where they came from, because:\n\nCorrelation is not causation.\nLinear models tell the sign and magnitude of changes in dependent variables within the data range. However, we need some understanding to tell whether the model will hold outside the data range.\n\n\n\n\n\n\n\nCritical Note:\n\n\n\nI am afraid I have to disagree with this mischaracterization of big data.\nWho are these people who can analyze big data without a model?\nAlso, almost all the models in his book and in the course are not constructed using Causal methods, which is how we can discriminate correlation from causation and model them appropriately.\nRegression and other methods shown here are not except for this problem. If the effects are linear, a linear model should extrapolate. If the effects are nonlinear, then who can say it, and why use a linear approximation? Also, I only know mathematical methods for extrapolating how a model (linear or not) will behave outside the observed range. Probabilistically, by looking at higher moments of a distribution analytically by considering more terms in a Taylor approximation. Statistically, cross-validation and Structural modeling are used to determine the impact of various causes on each effect. Finally, if we have more data, lots of patterns we initially see can be due to random chance, and we may be able to capture much more subtle effects happening in the real world.\n\n\nOne crucial issue is feedback. e.g., We may think that anti-lock brakes will reduce the number of accidents. That may be true initially, but after a while, people may start to drive closer to the car in front of them, so the benefits of anti-lock brakes might fall off.\n\n\n\n\n\n\nBonkers 💩 💩 💩\n\n\n\nSo the good old boys who tech Price theory at Chicago have many examples like this about the unintended consequence of some dentist’s intervention by adding flouride to the water makes people brush thier teeth less and having even more cavaties…. But they make a good case in explain this by saying the dentist missed the point – people’s prefernces wen’r to have perfect teeth, so they were happier to not have to brush thier teeth as much. That is a more honest explanation. Now milton friedman went on TV and discussed things like seatbelt in cars, better tires and explained that if the market wants safer cars people will pay for them and that regulation is usally the least economically efficent way to go about it.\nIn the final anaysis if people paid for anti-lock brakes then these reduuced some accidents but not all accidents are caused by brakes locking and a car skidding. Head on collisions are a differnt story, burst tiers, falling asleep and many other reasons. I’m pretty sure that getting used to thier fancy brakes didn’t go to thier heads.\nDr. Page gave many trivial models, but this one is brought with purely hand waving.\nA feedback mechanism can be positive or negative, but it is a nonlinear differential or difference equation.\nThe example is ridiculous - a person is taught how to drive, what distance to keep, etc. For a person to adjust to the new minimal distance of better brakes, they would have to have crashed the car non-fatally, at different speeds before and after the new brakes are introduced, and then make an adjustment based on that feedback - this cannot be an example of feedback.\n\n\n\n\n\n\nMultiple peaks\n\nA bigger problem is multiple peaks. For example, we might have some data in a small range that suggests a peak. Thus, based on this data, the peak seems to be the optimal point, and you miss a better point.\n\n\n\n\n\n\nCritical Note 💩\n\n\n\nWhile this is true,\nThis, again, is a nonlinear phenomenon. The presence of a single peak indicate the effect is not linear - linear means approximating a straight lines and lines do not have peaks.\nI am unsure if any local method can tell you if you are in a local or global maximum. What is certain is that the better you fit a single peak, the worse you are at fitting the other peak.\n\n\nThe new reality means that we may want to try something big and new, so we consider non-marginal changes. If we have only data in a small range, this may not help we very much. The multiple peak problem is just one instance of this issue.\nSometimes, big co-efficient thinking may be helpful, like taxing cigarettes to reduce healthcare costs and raise money to finance healthcare. A new reality might be implementing universal healthcare in the US. 1\n1 it might be but I prefer to view Universal healthcare and taxing cigatettes say to reduce lung cancer as two differnt issues and htus both likely to be perhaps run in parallel as happened during Barak Obama’s presidency and leter reversed by Donald TrumpThe American Jobs Act is also an example of big coefficient thinking. A new reality was the US interstate highway system. In 1956, the US government allocated $25 billion, which is about $410 billion at today’s prices, for 41,000 miles of roads. This was creating something completely new, so it was difficult to calculate the employment effects. Big coefficient thinking can be good for minor changes but ignores new realities.\n\nSad to say that: The American Jobs Act is not an example of big coefficient thinking. There are just so many difernt tax cuts and clauses to make us think this is all based out of a big coeffinct in somebody’s regression model 💩 The construction of public works like roads is nothing new - following the great depression the government intiated a massive number of public works project. 💩 It pretty simple for the bean counters in the treasury deprtment to calculate how many jobs building 41,000 miles of road would create. 💩 Both are independent efforts tp solve differnt problems.\n\n\n\n\n\nAmerican Jobs Act"
  },
  {
    "objectID": "w06.html#references",
    "href": "w06.html#references",
    "title": "Lesson 6 - Categorical and Linear Models",
    "section": "References",
    "text": "References\nNote: this page is based on the following source:\n\n[@page2017modelthinking] MOOC, Course material & Transcripts.\nTA Notes by [@fisher2017modelthinking].\nStudent notes by in [@kleinikink2016naturalmoney] and [@groh2017model]."
  },
  {
    "objectID": "w10.html",
    "href": "w10.html",
    "title": "Lesson 10 - Markov Processes",
    "section": "",
    "text": "Markov models consist of entities that can be in a set of states and there are transition probabilities between those states. e.g., there is a set of students. Those students could be in either one of two states, alert or bored. There is some probability P that they move from alert to bored and some probability Q that they move from bored to alert. Over time these students are moving back and forth from the alert state to the bored state. The Markov process gives us a framework to understand how those dynamics take place.\n\n\n\n\nHistorical trend of freedom in countries\n\nThis can also be applied to countries being free, being partially free or not free. Historically there are different trends. The free states seem to be increasing. The not-free tend to be decreasing. The Markov process can be used to figure out where this is going to end up.\nThe Markov convergence theorem state that, as long as a couple of assumptions hold, namely that there is a finite number of states, and the transition probabilities stay fixed, and you can get from any state to any other state, then the system goes to equilibrium. This is a really powerful finding that has all sorts of implications.\nTo understand Markov processes, matrices can be used. Matrices are grids of numbers. Those numbers will be the transition probabilities. Multiplication by matrices is used to understand Markov processes and the Markov convergence theorem. These matrices can be used to explain why these systems go to equilibria.\nMarkov processes are interesting for two reasons. First, Markov processes are a useful way to think about how the world works and this gives powerful results The Markov convergence theorem says that these systems are going to equilibria. Second, is the idea of exaptation. That the Markov model is incredibly fertile and can be applied in a whole range of different settings. Transition probabilities and matrices can also be used in a lot of settings as well.\n\n\n\n\nMarkov transition matrix"
  },
  {
    "objectID": "w10.html#sec-markov-chain-models",
    "href": "w10.html#sec-markov-chain-models",
    "title": "Lesson 10 - Markov Processes",
    "section": "",
    "text": "Markov models consist of entities that can be in a set of states and there are transition probabilities between those states. e.g., there is a set of students. Those students could be in either one of two states, alert or bored. There is some probability P that they move from alert to bored and some probability Q that they move from bored to alert. Over time these students are moving back and forth from the alert state to the bored state. The Markov process gives us a framework to understand how those dynamics take place.\n\n\n\n\nHistorical trend of freedom in countries\n\nThis can also be applied to countries being free, being partially free or not free. Historically there are different trends. The free states seem to be increasing. The not-free tend to be decreasing. The Markov process can be used to figure out where this is going to end up.\nThe Markov convergence theorem state that, as long as a couple of assumptions hold, namely that there is a finite number of states, and the transition probabilities stay fixed, and you can get from any state to any other state, then the system goes to equilibrium. This is a really powerful finding that has all sorts of implications.\nTo understand Markov processes, matrices can be used. Matrices are grids of numbers. Those numbers will be the transition probabilities. Multiplication by matrices is used to understand Markov processes and the Markov convergence theorem. These matrices can be used to explain why these systems go to equilibria.\nMarkov processes are interesting for two reasons. First, Markov processes are a useful way to think about how the world works and this gives powerful results The Markov convergence theorem says that these systems are going to equilibria. Second, is the idea of exaptation. That the Markov model is incredibly fertile and can be applied in a whole range of different settings. Transition probabilities and matrices can also be used in a lot of settings as well.\n\n\n\n\nMarkov transition matrix"
  },
  {
    "objectID": "w10.html#sec-a-simple-markov-model",
    "href": "w10.html#sec-a-simple-markov-model",
    "title": "Lesson 10 - Markov Processes",
    "section": "A simple Markov model",
    "text": "A simple Markov model\nLet’s use the simple example of alert and bored students. Some percentage of the students are alert and some percentage of the students are bored. An alert student can switch and become bored and a bored student can switch and become alert. We need to assume something about the transition probabilities. Assume that in any given period, 20% of the alert students become bored, but 25% of the bored students become alert. Here the matrices will be useful.\nWe can calculate this by hand. Assume we start with 100 alert and 0 bored, so (A,B)-&gt;(100,0). After one period, you have 80 alert and 20 bored, so (A,B) -&gt; (100-20+0=80, 0-0+20=20). If you go on then (A,B) -&gt; (80-16+5=69, 20+16-5=31). This is rather complicated so there must be a better way to keep track of this.\n\n\n\n\nMarkov transition matrix\n\nThe Markov transition matrix gives the probabilities of moving from state to state. The columns tell what is true at time t and the rows tells what is true at time t+1. So if you’re alert at time t, there’s an 80% chance you stay alert, and a 20% chance you become bored. If you’re bored at time t, there’s a 25% chance you become alert and a 75% chance that you stay bored.\n\n\n\n\nMarkov transition matrix\n\n\n\n\nMarkov transition matrix\n\n\nThe calculation process works as follows. For each row you multiply the elements with the value at t to get the value at t+1. So A(t+1) = 0.8A(t) + 0.25B(t) and B(t+1) = 0.2A(t) + 0.75B(t). It is possible to repeat these steps. The result after six periods is that only 58% of the students are alert.\nWhere does this process stop? It is possible to do the same calculation starting with all students being bored. After 6 turns 53% is alert and 47% is bored. It looks like there is an equilibrium.\n\n\n\n\nMarkov equilibrium matrix\n\nSo how do we calculate the equilibrium? The equilibrium would mean that at t+1 the same percentage of people is alert as at t. So 0.8p + 0.25(1-p) = p =&gt; p/5 = (1-p)/4 =&gt; (4/5)p = 1 - p =&gt; (9/5)p = 1 =&gt; p = 5/9. So in equilibrium 5/9 will be alert and 4/9 will be bored. In that case 20% of the alert will get bored, which is 1/9, and 25% of the bored will become alert, which is also 1/9.\nThis is a stochastic equilibrium. The thing that doesn’t change is the probability. The population is still moving from alert to bored and from bored to alert."
  },
  {
    "objectID": "w10.html#sec-markov-model-of-democratisation",
    "href": "w10.html#sec-markov-model-of-democratisation",
    "title": "Lesson 10 - Markov Processes",
    "section": "Markov model of democratisation",
    "text": "Markov model of democratisation\n\n\n\n\nMarkov transition matrix\n\nA slightly more complicated model involves countries that can either be free, partly free or not free at all. This can be used to learn how Markov processes work, how they can be extended to more dimensions. Let’s start simply with just a two-state model with democracies and dictatorships. Assume that 5% of democracies become dictatorships every decade, and that 20% of dictatorships become democracies.\nLet’s start off by assuming 30% are democracies and 70% are dictatorships. After a decade, (0.95 * 0.3 + 0.2 * 0.7) * 100% = 42.5% are going to be democracies while 57.5% will be dictatorships. One decade later, 52% will be democracies and 48% will be dictatorships. The equilibrium can also be calculated. 0.95p + 0.2(1-p) = p =&gt; 1/5 - p/5 = p/20 =&gt; p/4 = 1/5 =&gt; p = 4/5. The surprising finding is that we only end up with 80% democracies even though 95% of democracies stay democracies and 20% of dictatorships become democracies in each decade.\n\n\n\n\nDemocracy Markov model\n\nA sophisticated model is having three categories, free, partly free, and not free. This data comes from Freedom House and shows an increase in free countries and a decrease in not free countries, and a slight decrease in partly free countries. What is likely to happen?\nYou can use five year increments, use transition probabilities, and do some crude estimates. This results in the following. Each decade, 5% of free, and 15% of not free become partly free. And 5% of not free and 10% of partly free become free. And 10% of partly free become not free.\nThis is a three by three matrix. With computers it is possible to make a huge matrices and solve them for equilibrium. So what does that equilibrium look like? All we do is take each one of these rows, and multiply by the columns. The algebra results in 62.5% of countries being free, 25% being partly free, and 12.5% being not free, presuming that the transition probabilities stay fixed.\nThe model shows some general trends. The graph generated from the model doesn’t look exactly the same as the real picture, but it doesn’t look bad either. The model comes up at the end of the 40 year period with values that are close to those in the real world but that is because the estimated transition probabilities were based on the actual data. What’s more interesting is that the patterns look fairly similar as well."
  },
  {
    "objectID": "w10.html#sec-markov-convergence-theorem",
    "href": "w10.html#sec-markov-convergence-theorem",
    "title": "Lesson 10 - Markov Processes",
    "section": "Markov convergence theorem",
    "text": "Markov convergence theorem\nThe Markov convergence theorem tells that, provided a few fairly mild assumptions are met, Markov processes converge to a stochastic equilibrium. There are movements, but the probability of being in each state stays fixed. The conditions that must hold for that to be true are the following:\n\na finite number of states\nfixed transition probabilities\neventually you can get from any state to any other state\n\nnot a simple cycle\n\nIf these conditions hold then a Markov process converges to an equilibrium distribution that is unique, which means that there is only one equilibrium distribution that is independent of the initial state. It is determined entirely by the transition probabilities. In Markov processes, the initial state, history, and interventions to change the state have no effect on the long run on the system.\nInterventions can have effect. First, it could take a long time before the system is back to equilibrium so that an intervention may have a significant benefit. Second, some of the conditions of the theorem may not hold in the real world, most notably the fixed transition probabilities. The transition probabilities may change over time as the function of the state of the system. Changing the state has a temporary effect, but changing the transition probabilities has permanent consequences. Useful interventions change the transition probabilities. This may happen to tipping points."
  },
  {
    "objectID": "w10.html#sec-expectation-of-the-markov-model",
    "href": "w10.html#sec-expectation-of-the-markov-model",
    "title": "Lesson 10 - Markov Processes",
    "section": "Expectation of the Markov model",
    "text": "Expectation of the Markov model\nThe Markov model can be used in contexts and problems we never would have thought of. The first way is taking the entire process and modelling to other things. The second way is using a part of the Markov model, the transition probability matrix, to understand some things that are surprising and interesting. A Markov process is. Fixed set of states with fixed transition probabilities between those states. If it is possible to get from any state to any other through a sequence of transitions, then the Markov convergence theorem states that the process is going to a unique equilibrium.\nThis can be applied to voter turnout. Assume that there is a set of voters at time t, and there is a set a of non-voters at time t. We can make a transition matrix of that to find out how many are going to vote at time t+1, and how many are not going to vote at time t+1. If the transition probabilities stay fixed then there is a unique equilibrium that tells the number of people that is expected to vote in any election. This can also be applied to school attendance as each day there are children that go to school and children that don’t go to school. These two applications are very standard.\n\n\n\n\nIdentify writer\n\nIt is also possible to use only a part of the Markov model, the Markov transition matrix, to identify writers. The transition matrix can be used to figure out who wrote a book. So suppose some anonymous person wrote a book and you are trying to figure out whether Bob or Elisa wrote it.\nYou can figure out transition probabilities by taking some key words, and then create transition matrices. e.g., you can calculate at what percentage of the time does “the record”, “example” or “the sake of” of follow the word “for”. You can compare this with books Bob and Elisa wrote with the use of computers.\nThis can be applied to medical diagnosis and treatment. Typically, there’s a sequence of reactions to that treatment. You can write down transition probabilities that can be multi-stage. e.g., if the treatment is going to be successful, the patient goes through the following transitions: first pain, then feeling slightly depressed, then some more pain, but then the patient gets better. Alternatively, if the treatment is not successful, it could be that, initially the patient is depressed, then there is mild pain, then there’s no pain, and then the treatment fails. This can be used to figure out early on whether or not a treatment will be successful.\nAnother example is the road to war. Suppose there are two countries and there is some tension. The political process goes through the following transitions. First there are some political statements on each side, then that leads to trade embargoes, followed by military buildup. Based on this sequence of three events, you estimate the likelihood of war based on historical data of previous times when those three transitions happened.\nIn these cases we are not using the full power of the Markov model and we do not assume that the transition probabilities necessarily stay fixed. We are not interested in solving for the equilibrium. All we’re trying to do is just use this probability matrix to organize the data in such a way that we can think more clearly about what’s likely to happen."
  },
  {
    "objectID": "w10.html#references",
    "href": "w10.html#references",
    "title": "Lesson 10 - Markov Processes",
    "section": "References",
    "text": "References\nNote: this page is based on the following source:\n\n[@page2017modelthinking] MOOC, Course material & Transcripts.\nTA Notes by [@fisher2017modelthinking].\nStudent notes by in [@kleinikink2016naturalmoney] and [@groh2017model]."
  },
  {
    "objectID": "w03.html",
    "href": "w03.html",
    "title": "Lesson 3 - Aggregation",
    "section": "",
    "text": "More is different. — Physics Nobel Laureate Phillip Anderson, Nobel Memorial Prize Laureate\nAggregation can capture the emergent properties of a system, which gets interesting when an emergent phenomenon diverges from the behaviors of its constituent parts. The key question is “How does more become different?”\nThe most prominent model for Aggregation is the Central Limit Theorem that arises from the Law of Large Numbers with the requirements that we aggregate over many similar non-interacting units with bounded effects. This leads us to easy mean and variance estimates using the empirical rule.\nNext, I would use more general statistical summary functions that we call population parameters if we can track the entire system or model statistics if we can only sample from the population. There may be different levels of aggregation possible for our model. Preferences and ordered relations can be challenging to aggregate in a way that maintains transitivity. However, it is relatively easy to aggregate demand patterns."
  },
  {
    "objectID": "w03.html#sec-agg-introduction",
    "href": "w03.html#sec-agg-introduction",
    "title": "Lesson 3 - Aggregation",
    "section": "Aggregation",
    "text": "Aggregation\n\n\n\n\n\n\nTip 1: 🥜 Aggregation in a nutshell\n\n\n\n\nmore is different\n🔔 When small changes stack up, you are in the Central Limit Zone\n🐉 Otherwise, you are in a danger zone 2\n\n\n\n2 be very careful.Aggregation can lead to unexpected results, and we refer to this phenomenon using “more is different” in deference to [@Anderson72MoreIsDifferent].\n\nExample 1 Can we consider a single water molecule to be wet? Not if wetness is an emergent property of multiple water molecules.3 But, the three phases of water also require aggregation. So we are dealing with problematic definitions.\n3 How does wetness arise? 💭 Isn’t two water molecules sharing an H bond? Aren’t other liquids like mercury also wet? 🤔 Once we figure that we are aggregate over h-bonds, we see this isn’t a case of more is different but an ecological fallacy ❌\n\n\n\n3D model hydrogen bonds in water source\n\n\n\nExample 2 Can a single neuron explain consciousness, cognition, or individual characteristics? 4 No more than a single cogwheel could explain the working of a clock.\n4 what are these phenomena :thought:, how are they measured 🤔 ? An ant or a rat has brains but doesn’t exhibit consciousness, etc - so this is not just a consequence of neuron aggregation; this is another ❌.\n\nExample 3 Schelling’s model exhibits segregation for fairly tolerant individuals and mixing for highly intolerant ones. 💰💰\n\n\n\n\nGas at high temperature source\n\n\n\nExample 4 Atoms of gas have variable kinetic energy, but temperature and pressure are statistical aggregates for these, and one molecule has little bearing on the aggregate. 💰💰💰\n\nCellular automata show how complexity can arise from very simple rules. 5\n5 This is surprising when you first learn about it 🤔, not so much afterwards. After all this is a rehash of any number of ideas - like the halting problem, inverse problems, and the axiom of choice - proving existence of arbitrary functions - without giving access to the rule. e.g., the function for all winning lottery tickets on a given day 💩 So complexity exists, but how is the game of life teaching us about aggregation?\n\n\n\n\n\nFigure 1: Cellular Automata: A period 16 oscillator source\n\n\n\nWe considered Aggregation via the following mechanism:\n\nStacking small changes with the Central Limit theorem.\nUsing a single to aggregate behavior in the game of life.\nUsing a family of rules: how to aggregate using one-dimensional cellular automata models\nPreferences: how to aggregate preferences, e.g., to make collective choices.\n\nWe model for the following reasons:\n\npredict points\nunderstand data, e.g., using a bell-shaped curve\nunderstand patterns, e.g., the glider pattern in the game of life\nunderstand the class of outcome, e.g., using one-dimensional cellular automata models\nwork through logic, e.g., difficulties arise with the aggregation of preferences."
  },
  {
    "objectID": "w03.html#sec-agg-central-limit-theorem",
    "href": "w03.html#sec-agg-central-limit-theorem",
    "title": "Lesson 3 - Aggregation",
    "section": "Central limit theorem",
    "text": "Central limit theorem\nA Probability Distribution lists a variable’s different outcomes, each with its likelihood of occurring.\nThe Central Limit Theorem states that if we add up a large enough series of independent variables, the distribution will follow a normal distribution characterized by a bell–shaped curve. The most likely outcome, the mean \\(μ\\), is in the middle of the curve.\nWe can simulate this process by flipping coins a large number of times.\n\nWhen we toss a fair coin two times, how many heads will we see, and at what likelihood?\n\nBinomial distribution for two coin flips\n\n\nCol1\nCol2\n\n\n\n\n2H\n\\(1/4\\)\n\n\n1H\n\\(1/2\\)\n\n\n0H\n\\(1/4\\)\n\n\n\nWhen you flip a coin 4 times, the outcomes\n\n\n\nCol1\nCol2\n\n\n\n\n4H\n\\(1/16\\)\n\n\n3H\n\\(4/16\\)\n\n\n2H\n\\(6/16\\)\n\n\n1H\n\\(4/16\\)\n\n\n0H\n\\(1/16\\)\n\n\n\nAnd when we flip a coin N times:\nFor N coin flips:\n\\[\nP(X=k)={n \\choose k} p^k(1-p)^{n-k}\\qquad\n\\tag{1}\\]\nSuccess in N coin flips follows a Binomial distribution.\n\\[\n\\mu=pN \\qquad\n\\tag{2}\\]\n\\[\n\\sigma = p \\times (1-p) \\times n \\qquad\n\\tag{3}\\]\n\nFor \\(N&gt;&gt;20\\), the Binomial distribution becomes more like the Normal distribution."
  },
  {
    "objectID": "w03.html#sec-agg-empirical-rule",
    "href": "w03.html#sec-agg-empirical-rule",
    "title": "Lesson 3 - Aggregation",
    "section": "The Empirical Rule",
    "text": "The Empirical Rule\n\n\n\n\n\n\nFigure 2: Bell curve and empirical rule\n\n\n\nThe standard deviation \\(\\sigma\\) parameter determines how the normal distribution curve is spread. We can use these values to build confidence intervals:\n\nThe Empirical Rule\n\n\nSD of Standard Normal\n% of Values\n\n\n\n\n\\(\\sigma\\)\n68\n\n\n\\(2\\sigma\\)\n95\n\n\n\\(3\\sigma\\)\n99.7\n\n\n\ne.g., set the mean \\(\\mu\\) to 100, and the standard deviation \\(\\sigma\\) to 3, then for 95% of all cases, the outcome will fall in [94, 106].\nIn a Binomial distribution where \\(p = 1/2\\) like flipping coins, \\(\\sigma = (\\sqrt{N})/2\\), thus when \\(N = 100\\), the mean is \\(100/2 = 50\\), and the standard deviation \\(\\sigma=√100/2 = 5\\).\nIn general, Binomial distributions have a standard deviation of success for N trials:\n\\[\n\\sigma = \\sqrt{(p(1-p)N)}\n\\] {eq-binom-sd}\nWhen \\(p = \\frac{1}{2}\\) then this simplifies to: \\[\n\\sigma =  \\frac{\\sqrt{N}}{2}\n\\] {eq-binom-sd-std}\nWe can now use this standard deviation to make statistical inferences. Airlines overbook to increase their profits, so they need to know the probability of someone not getting a seat. This allows them to estimate the expected cost of offering people a second ticket to take the next flight.\n\nExample 5 (Flight Overbooking)  \n\nimport math\np=0.9           # show up rate\nseats = 380     # seat on plane\ntickets = 400   # tickets sold\nmean = 360      # mean tickets sold\nsigma = round(math.sqrt(p*(1-p)*tickets))\nlb99 = mean - 3*sigma\nub99 = mean + 3*sigma\nprint(f'mu={mean}, sigma={sigma}, 97.5 CI = [{lb99},{ub99}]')\n\nmu=360, sigma=6, 97.5 CI = [342,378]\n\n\n\nA Boeing 747 has 380 seats\nThere is a 0.9 show-up rate,\nlet’s assume people show up independently of each other.\n400 tickets were sold. i.e. \\(N = 400\\),\nThe mean \\(μ = 360\\)\nThe standard deviation is \\(\\sigma = \\sqrt{(0.9\\times(1-0.9)\\times 400)} = \\sqrt{36} = 6\\). we can now use the Empircal rule to estimate a 99.7 confidence interval:\nin 99.75% of cases, the number of passengers will be in [342, 378].\n\n\nThe Central Limit theorem states that stacking IID random variables drawn from any distribution possessing finite mean and variance will aggregate to normal distribution. Much of the world’s predictability stems from exceptional events never being observed. Rare events called black swans are usually products of heavy-tailed distributions that cannot be aggregated using the central limit theorem.\nIf the events are not independent the central limit may be inaccurate or even fail completely to model the aggregation"
  },
  {
    "objectID": "w03.html#sec-agg-six-sigma",
    "href": "w03.html#sec-agg-six-sigma",
    "title": "Lesson 3 - Aggregation",
    "section": "Six sigma",
    "text": "Six sigma\n\n\n\n\n\n\n\n\nSix Sigma logo source\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\nDMAIC source\n\n\n\n\n\n\n\n\n\nDMADV source\n\n\n\n\n\n\n\n\n\n\n\n\nNormal Distribution source\n\n\n\n\n\n\nFigure 3: Six Sigma\n\n\n\nThe idea of managing variance beyond three times the variance to work with sigmas has been embraced in the Six Sigma project management and planning system.\n\n\n\n\n\n\nCaution 2: Six sigma In a nutshell\n\n\n\nSix Sigma is a project management methodology, not an aggregation method.\nWe will revisit it later when it is time to consider the tradeoffs between reducing variation as practiced in Six Sigma and increasing variation to speed up convergence in Replicator Dynamics.\n\n\nWhen I asked friends who worked at Motorola, they said that one department used it to win prizes in the company. Still, it never really caught on to become the ubiquitous management technique its proponents have touted.\nEven with extensive training, statistics can be challenging in most business scenarios. Recall that Ronald Fischer’s influential works were prescriptive how-to manuals that did little to impart their authors’ depth of knowledge. That said, even a little statistics can go a long way.\nSix Sigma is a quality control or process improvement methodology developed by engineer Bill Smith at Motorola. \\(6\\sigma\\) is applied within each of DMAIC five steps for improving existing projects, and for planning new projects, \\(6\\sigma\\) DMADV is used.\n\n\n\n\n\n\nImportant 2: DMAIC v.s. Minto Pyramid with SCQA & MECE tools\n\n\n\nWhile the DMAIC framework is practical for organizing research, it has many drawbacks for reporting the results, particularly to executives.\nThe eponymous Pyramid Method introduced in [@minto1996minto] by Barbara Minto while working at McKinsey, the Minto pyramid, and the following two methods can be used with it to structure storytelling for data analysis and to explain the use of Models.\n\nSCQA & MECE #{tbl-MINO-method}\n\n\nSCQA\nMECE\n\n\n\n\nSituation,\nMutably\n\n\nComplication\nExclusive\n\n\nQuestion\nCollectively\n\n\nAnswer\nExhaustive\n\n\n\n\n\n\\(6\\sigma\\) assigns roles to practitioners:\n\nMaster Black Belts, are identified by the champions to act as Six Sigma mentors. They are expected to devote all their full time to Six Sigma, assisting the champions and guiding Black Belts and Green Belts. In addition to statistical tasks, they should ensure that Six Sigma is applied consistently across departments. Black Belts work under the Master Black Belt to apply \\(6\\sigma\\) to specific projects. Their responsibility is running \\(6sigma\\) projects. Champions and Master Black Belts are tasked with identifying projects for \\(6\\sigma\\). Green Belts are employees tasked with implementing Six Sigma alongside their other responsibilities. They operate under the supervision of Black Belts..\n\nThe main idea is to control the process to reduce the defects to below 3.4 per million opportunities or six standard deviations.\n\nExample 6 (the 6 sigma banana store)  \n\nLet the mean banana sales be \\(\\mu= 500 kg\\),\nand the sd of \\(\\sigma= 10 kg\\),\nHow much stock of bananas is required to cope with a Six Sigma event?\n\nSix Sigma is 60 kg, so 560 kg of bananas are required.\n\nRemember, we are talking about a 2-in-a-billion frequency for these defects. They will only manifest in huge-scale situations, like big factories.\n\nExample 7 (washers) The required metal thickness is between 500 and 560 mm in production. Assuming the outcome is normally distributed, then:\n\\[\n\\mu=530\n\\]\nThen \\(\\sigma\\) should be 5 or less. \\[\n\\sigma = 560-500/2=30 \\implies tolerance = 30/6 = 5 mm.\n\\]\n\n\nExample 8 (tires) A production process creates tires with:\n\nan average diameter of \\(\\mu=20\\) inches and\na standard deviation of \\(sd=0.1\\) inch.\nWhat is the Six Sigma range?\n\n\\(20 \\pm 6\\times 0.1= [19.4,20.6]\\)"
  },
  {
    "objectID": "w03.html#sigma-models-and-methods",
    "href": "w03.html#sigma-models-and-methods",
    "title": "Lesson 3 - Aggregation",
    "section": "\\(6\\sigma\\) Models and Methods :",
    "text": "\\(6\\sigma\\) Models and Methods :\n\n\n\n\n\n\nNote 1: Extra Content\n\n\n\nI was curious about the statistics used in this framework, so I researched Six Sigma, drawing on [@cano2012sixsigma, @cano2015qcr] and using Wikipedia. This is my summary.\n\nRoot cause analysis\n\n5 Whys - a root cause analysis\nCause & effects diagram (also known as fishbone or Ishikawa diagram) - a root cause analysis\n\nStatistical and fitting tools\n\nRegression analysis\nGeneral linear model - Extends regression analysis.\nANOVA - Analysis of variance is a regression that explains variation within and between groups.\n\nANOVA Gauge R&R ANOVA Gauge repeatability and reproducibility is a measurement systems analysis technique that uses an ANOVA random effects model to assess a measurement system.\nCorrelation\nChi-squared test - hypothesis testing on contingency tables.\nDesign of experiments\nStratification\n\nVisualizations\n\nScatter diagram\nHistograms/Pareto analysis/Pareto chart\nPick chart/Process capability/Rolled throughput yield\nControl chart/Control plan (also known as a swimlane map)/Run charts\n\nTQM Methodologies\n\nAxiomatic design - a systems design methodology that uses matrix-based methods to analyze the transformation of customer needs into functional requirements.6\nBusiness Process Mapping/Check sheet\nCost–benefit analysis\nCTQ tree Critical-to-quality trees are the key measurable characteristics of a product whose performance standards must be met to satisfy the customer.\nQuality Function Deployment (QFD)\nQuantitative marketing research through use of Enterprise Feedback Management (EFM) systems\nSIPOC analysis (Suppliers, Inputs, Process, Outputs, Customers)\nTaguchi methods/Taguchi Loss Function\nValue stream mapping\n\n\nIf most of these are familiar, perhaps we should complete the black belt training on LinkedIn!\n\n\n6 Axiomatic design - Wikipedia. https://en.wikipedia.org/wiki/Axiomatic_design"
  },
  {
    "objectID": "w03.html#sec-game-of-life",
    "href": "w03.html#sec-game-of-life",
    "title": "Lesson 3 - Aggregation",
    "section": "Game of life",
    "text": "Game of life\n\n\n\n\n\n\n\n\n\n\n\n(a) John Conway source\n\n\n\n\n\n\n\n\nGlider pattern\n\n\n\n\n\n\nFigure 4: Conway’s Game of Life\n\n\n\nJohn Conway’s game of life is a model for artifical life that shows. In essense it aggregate activity stemming from a single rule at differnt location that can leads to surprisingly complex system.\nIn the system thinking paradigm, the worldview is that all parts of the system under study are interconnected. In this paradigm, inferring how macro-behavior emerges from micro-motives is one of the main challenges for researchers. Viewed through this system-thinking prism, the game of life is a microcosm whose emergent complexity is an analog of the challenges of aggregation in the real world. ,\nThe glider pattern shown above reconstructs shifted diagonally.\n\n\n\n\n\n\nBoosting and Bagging\n\n\n\nAggregating simple rules in series or parallel called bagging and boosting, results in complex rules c.f. random forest and feed forward neural networks.\n\n\n\n\n\n\n\n\nTuring Complete\n\n\n\nIt is no surprise that the game of life is complex once we factor in that it is a complete system, that the grid is part of the initial grid, and that its evolution is all part of the system.\n\n\nThe game of life is simulated on a grid whose cells can be either alive or dead.\n\nif a cell is off, it will only turn on if precisely 3 of the 8 cells in the Moore neighborhood are on.\nif a cell is on, it will only stay on if 2 or 3 of the 8 neighbors in the Moore Neighborhood are on.\n\n\n\n\n\n\n\n\n\ncounting neighbors for x\n\n\n\n\n\n\n\ncounting neighbors full\n\n\n\n\n\n\nFigure 5: counting neighbors\n\n\n\nIn this case, x has two neighbors, so it will turn on in the next time step. Estimating the next step requires counting neighbors and applying the rules.\n\n\n\n\n\n\n\n\n\nBliker horizontal state\n\n\n\n\n\n\n\nBlinker vertical state\n\n\n\n\n\n\na period 2 blinker (oscillator)\n\n\n\nThe following rules use the 8 cells of the Moore neighborhood\n\nA living cell with less than two neighbors dies.\nA living cell with two or three neighbors will stay alive.\nA living cell with more than three neighbors dies.\nA dead cell with exactly three live neighbors will spawn a live cell.\n\nnote: this is called the B3/S23 rule c.f. rules and many other rules exist.\n\n\n\n\n\n\n\n\nConfiguration 3a\n\n\n\n\n\n\n\nConfiguration 3b\n\n\n\n\n\n\nFigure 6\n\n\n\nThis allows us to consider different cell configurations. #1 with 2 cells on dies #2a with 3 cells on will flip into #2b then flip back into #2a. This is a blinker that oscillates between the two states. The game can also produce units that keep growing. c.f. #3a and #3b.\nWe can use the NetLogo program to simulate the evolution of different patterns, such as the Beacon, Figure 8, and the F-Pimento.\nThe beacon behaves like a blinker and moves back and forth between two states.\n\n\n\n\n\n\n\n\nBeacon\n\n\n\n\n\n\n\nFigure 8\n\n\n\n\n\n\n\nF-Pimento\n\n\n\n\n\n\nFigure 7: Additional Patterns\n\n\n\nThe Figure 8 pattern is periodic, with a cycle length of 8 steps, generating a sequence of complex patterns. The F-Pimento pattern moves in space generating gliders.\nWe could say that even simple brain cells, following basic rules, can give rise to complexity of thinking, remembering, and understanding.\nThe Game of Life doesn’t spell out how our minds work, but it intimates that our thoughts may just be an aggregate of these simpler parts working together. A glider pattern is a cool example of this: it’s when a shape moves across the board, popping up again in a new spot after a few moves\nLike other dynamical systems, the game of life exhibits four classes of behavior.\n These are:\nstable cyclic random complex\nThe game of life demonstrates that:\nEmergence - complex behaviors like guns and gliders arise from a few simple rules and initial conditions. Self Organization: Patterns appear without a designer. The logic of complex systems can be simple."
  },
  {
    "objectID": "w03.html#sec-cellular-automata",
    "href": "w03.html#sec-cellular-automata",
    "title": "Lesson 3 - Aggregation",
    "section": "Cellular automata",
    "text": "Cellular automata\n\n\n\n\n256 rules\n\n\n\n\na new kind of science\n\n\nJohn von Neumann a Hungarian-American mathematician and physicist introduced Cellular automata. The game of life is a type of cellular automata. We focused on one-dimensional cellular automata to see that the notions of complex systems we have just explored can be seen even when we restrict these rule-based systems to a single dimension. The restriction reduces the number of neighbors available in the rules. These automata are more straightforward to enumerate and study in detail. Some of this work is collected in A New Kind of Science by the author Stephen Wolfram.\nThe cellular automata models we considered were one-dimensional, which made their rules simpler than the ones used in the game of life.\n\nCells have two neighbors, except those on the borders.\nThis lets us write out all possible rules\nWe can also track the model’s evolution along the vertical axis.\n\nSo, think of it like a game where you have a cell that can be either on or off, and its fate is decided by its own current state along with the state of its neighbors on each side. Picture three cells in a row - together, they can end up in eight different combinations. Now, for each of these combinations, our middle cell has two choices: it can either light up (on) or stay dark (off).\n\n\n\n\nRule 30 in Netlogo\n\nThis setup gives us 256 different ways (or rules) to determine what happens to our middle cell based on those three-cell setups. It’s kind of like flipping switches on or off depending on the pattern you see.\nWe can keep track of these rules by assigning them numbers, kind of like naming each unique switch-flipping strategy. For the cell to turn on or off when all three cells are off, we decide if this situation is a “1” (turn on) or a “0” (stay off), and we do this for all eight combinations. In one of the scenarios we’re talking about, the pattern is called “00001110” in binary code, which translates to 30.\nThe image shows what happens with this rule number 30 in action for the first few steps. We can simulate this rule using Netlogo, and it will show a complex pattern and behavior where the middle cell is tough to predict in advance.\n\n\n\n\nRule numbering\n\nRule 110. creates an increasing triangle. If we simulate it in Netlogo, you will see a complex pattern of triangles nested in the main triangle, while cells to the right side are empty blank.\n\n\n\n\nRule 110 with random start\n\nThose models have four different types of outcomes:\n\nfixed state\nalternation\nrandomness\ncomplexity\n\nThe question is why do rules go to a specific state? Or more specifically what creates chaos, what creates complexity and what creates order?\nThe fact that such simple models lead to such complexity has led physicists like John Wheeler to suggest in [@Wheeler1999InformationPQ] that reality itself may arise out of binary processes. This idea is called it from bit"
  },
  {
    "objectID": "w03.html#sec-agg-langton-lambda",
    "href": "w03.html#sec-agg-langton-lambda",
    "title": "Lesson 3 - Aggregation",
    "section": "Langton’s Lambda",
    "text": "Langton’s Lambda\n{column-margin}\n\n\n\n\nBehavior classes versus Lambda\n\nLangton’s Lambda \\(\\lambda\\) can tell what the outcomes look like. Langton asked a much simpler question: How many things go on? e.g., in rule 30, four cells out of eight switch on. Lambda is the percentage of cells that go on; in this case, \\(\\lambda = 4/8 = 1/2\\). For rule 110, \\(\\lambda = 5/8\\).\nBased on the Lambdas values, we will see differnt behavior:\n\nif \\(\\lambda = 0\\) then all cells die off;\nif \\(\\lambda = 1/8\\) then the system blinks;\nif \\(\\lambda = 1\\) then all cells switch on;\nif \\(\\lambda\\) is in [3/8, 5/8], most of the complex or random patterns will appear.\n\nThus intermediate levels of codependence tend to create complexity and randomness. Pattern trading is an idea of tracking such intermediate levels of interdependence in markets, which present to complex patterns."
  },
  {
    "objectID": "w03.html#sec-preference-aggregation",
    "href": "w03.html#sec-preference-aggregation",
    "title": "Lesson 3 - Aggregation",
    "section": "Preference Aggregation",
    "text": "Preference Aggregation\nPreference aggregation follows a different mathematical structure. It differs from what we learned on aggregating numbers with the central limit theorem or even aggregating rules for cellular automata.\nWe enumerate preferences through revealed actions [w03-5]. You can give people some money and ask them to buy something with it according to their preferences. One person may prefer apples to bananas and bananas to kiwi fruit. Preference orderings are the rankings of alternatives, such as fruit.\n\n\n\n\nTable 1: irrational fruit preferences\n\n\n\n\n\n🍎\n\\(&gt;\\)\n🍌\n\n\n🍌\n\\(&gt;\\)\n🥝\n\n\n🍎\n\\(&lt;\\)\n🥝\n\n\n\n\n\nHow many preference orderings are there? If you have three options, e.g., apples, bananas, and kiwi fruit, there are eight options $$, but not all are rational. If you prefer A to B, and B to C, then you also prefer A to C. In this case preferring C to A is not rational. We consider a preference ordering rational if it is transitive.\nIf you have three options, there are \\(3 \\times 2 \\times 1 = 6\\) rational preference orderings, as follows :\n\n\\(A &gt; B &gt; C\\)\n\\(A &gt; C &gt; B\\)\n\\(B &gt; A &gt; C\\)\n\\(B &gt; C &gt; A\\)\n\\(C &gt; A &gt; B\\)\n\\(C &gt; B &gt; A\\).\n\n\n\n\n\nTable 2: Preference ordering\n\n\n\n\n\n🍎\n\\(&gt;\\)\n🍌\n\\(&gt;\\)\n🥝\n\n\n\n🍌\n\\(&gt;\\)\n🥝\n\\(&gt;\\)\n🍎\n\n\n\n🥝\n\\(&gt;\\)\n🍎\n\\(&gt;\\)\n🍌\n\n\n\n\n\n\nThe Condorcet Paradox of aggregation states that if you aggregate individual preferences, you might get irrational preference orderings, even when individual preferences are rational.\ne.g. \\(C &gt; B &gt; A &gt; C\\),\neven when individual preferences are rational. This can happen when people have different preferences.\ne.g., person 1 has preference\n\n\\(A &gt; B &gt; C\\), person 2 has a preference\n\\(B &gt; C &gt; A\\) and person 3 has a preference ordering\n\\(C &gt; A &gt; B\\).\n\nIn such a situation, the aggregate preference ordering is not apparent. A possible solution is to order the preferences pairwise. Persons 2 and 3 prefer kiwi fruit to apples, so the aggregate preference is \\(C &gt; A\\). Other aggregate preference are\n\n\\(B &gt; C\\) and\n\\(A &gt; B\\). So we get\n\\(C &gt; A &gt; B &gt; C\\)\n\nThis may have consequences, when we want to aggregate votes. Even when individuals vote rationally, there is no guarantee that the collective outcome is rational. This surprising outcome introduced in [@arrow1950figgiculty] by Economics Nobel Laureate Kenneth Arrow is called the Arrow Impossibility theorem in Game theory and Economics of social welfare.\nSince collective outcomes may not be rational, people may vote strategically, or there may be all kinds of political games where voters try to manipulate the outcome of an election into the desired result by misrepresenting their preferences.\n\nReferences\nNote: this page is based on the following source:\n\n[@page2017modelthinking] MOOC, Course material & Transcripts.\nTA Notes by [@fisher2017modelthinking].\nStudent notes by in [@kleinikink2016naturalmoney] and [@groh2017model]."
  }
]